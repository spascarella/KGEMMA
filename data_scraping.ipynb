{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries import, logging and HF API setup\n",
    "\n",
    "\n",
    "#### **NOTEBOOK OUTPUT**: full-dataset.csv and paper_info.csv - they will have to be preprocessed by *preprocessing.ipynb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import arxiv as ax\n",
    "from arxiv import Client, Search\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    models_generator = api.list_models(full=True, cardData=True, fetch_config=True)\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error fetching models from Hugging Face API: {e}\")\n",
    "    models_generator = iter([])\n",
    "\n",
    "data_list = []\n",
    "\n",
    "model_iter = iter(models_generator)\n",
    "while True:\n",
    "    try:\n",
    "        model = next(model_iter)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Skipping model due to load error: {e}\")\n",
    "        continue\n",
    "    try:\n",
    "        arxiv_ids = [x for x in model.tags if x.startswith(\"arxiv\")] # Extract arXiv IDs\n",
    "        arxiv_ids_str = \", \".join(arxiv_ids) if arxiv_ids else None # Convert to a list of strings in case of multiple arXiv IDs\n",
    "\n",
    "        base_model = model.card_data.get(\"base_model\", None) if model.card_data else None #Extract base model\n",
    "        if isinstance(base_model, list):\n",
    "            base_model = \", \".join(base_model)\n",
    "        elif not isinstance(base_model, str):\n",
    "            base_model = None\n",
    "\n",
    "        language = model.card_data.get(\"language\", None) if model.card_data else None # Extract language and convert to a string if it's a list\n",
    "        if isinstance(language, list):\n",
    "            language = \", \".join(language)\n",
    "        elif not isinstance(language, str):\n",
    "            language = None\n",
    "\n",
    "        # Extract evaluation results\n",
    "        eval_list = []\n",
    "        if model.card_data and model.card_data.eval_results:\n",
    "            # Check if model_name exists in card_data\n",
    "            if not model.card_data.model_name:\n",
    "                logging.debug(f\"Model {model.id} has eval_results but no model_name, skipping evaluation results.\")\n",
    "            else:\n",
    "                for result in model.card_data.eval_results:\n",
    "                    eval_list.append({\n",
    "                        \"task_type\": result.task_type,\n",
    "                        \"dataset_name\": result.dataset_name,\n",
    "                        \"metric_type\": result.metric_type,\n",
    "                        \"metric_value\": result.metric_value\n",
    "                    })\n",
    "\n",
    "        data_list.append({\n",
    "            'id': model.id,\n",
    "            'author': model.author,\n",
    "            'created_at': model.created_at,\n",
    "            'downloads': model.downloads,\n",
    "            'pipeline_tag': model.pipeline_tag,\n",
    "            'arxiv_ids': arxiv_ids_str,\n",
    "            'base_model': base_model,\n",
    "            'language': language,\n",
    "            'evaluation_metrics': json.dumps(eval_list) if eval_list else None\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing model {model.id}: {e}\")\n",
    "\n",
    "logging.info(f\"Processed {len(data_list)} models\") # Processed 1404500 models\n",
    "df_final = pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning -- we get rid of duplicated models (same name and same evaluation metrics) and also those with zero downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "if not df_final.empty:\n",
    "    df_final = df_final.drop_duplicates(subset=['evaluation_metrics'], keep=False) # Drop duplicates based on evaluation metrics\n",
    "    df_final[\"id_second_part\"] = df_final[\"id\"].apply(lambda x: x.split(\"/\")[1] if \"/\" in x else x) \n",
    "    df_final = df_final.drop_duplicates(subset=[\"id_second_part\"], keep=\"first\") # Drop model duplicates\n",
    "    df_final = df_final.drop(columns=[\"id_second_part\"])\n",
    "    df_final = df_final[df_final[\"downloads\"] != 0]\n",
    "\n",
    "df_final.to_csv(\"./output/full-dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: (25254, 9)\n",
      "After filtering: (18741, 9)\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"./output/full-dataset.csv\") # 1404500 -> 25254\n",
    "print(\"Before filtering:\", dataframe.shape)\n",
    "dataframe = dataframe[dataframe[\"downloads\"] != 0] # 25254 -> 18741\n",
    "#dataframe = dataframe[dataframe[\"downloads\"] < 50] # Filter models with less than 50 downloads - Probably to apply. \n",
    "print(\"After filtering:\", dataframe.shape)\n",
    "\n",
    "dataframe.to_csv(\"./output/full-dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving authors, title and summary for each paper and put them into a dataframe -- we also drop duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of arvix ids (3125, 9)\n",
      "3125\n",
      "['arxiv:2002.00293', 'arxiv:2202.03555', 'arxiv:2405.05374', 'arxiv:2104.08678', 'arxiv:2004.10964', 'arxiv:2402.16107', 'arxiv:2310.16609', 'arxiv:2311.09613', 'arxiv:2310.04921', 'arxiv:2405.00675', 'arxiv:2204.08387', 'arxiv:2403.03206', 'arxiv:2012.03411', 'arxiv:2106.13731', 'arxiv:2204.10757', 'arxiv:2401.02415', 'arxiv:2106.13687', 'arxiv:2203.00585', 'arxiv:2310.00752', 'arxiv:2101.11718', 'arxiv:2203.09509', 'arxiv:2311.11691', 'arxiv:2311.07052', 'arxiv:1603.08983', 'arxiv:2205.13147', 'arxiv:1907.12412', 'arxiv:1908.07490', 'arxiv:2306.01708', 'arxiv:2412.03187', 'arxiv:2405.07703', 'arxiv:1911.11641', 'arxiv:0000.00000', 'arxiv:2410.02525', 'arxiv:2305.10853', 'arxiv:2308.07124', 'arxiv:2311.13534', 'arxiv:2312.12450', 'arxiv:2304.12244', 'arxiv:2408.07990', 'arxiv:2310.16049', 'arxiv:2110.08207', 'arxiv:2405.19495', 'arxiv:2411.15734', 'arxiv:2402.14830', 'arxiv:2308.11878', 'arxiv:2312.06795', 'arxiv:2310.19923', 'arxiv:2401.10491', 'arxiv:2309.07597', 'arxiv:2501.01028', 'arxiv:2412.08347', 'arxiv:2305.09690', 'arxiv:2310.12371', 'arxiv:2206.12693', 'arxiv:2404.15217', 'arxiv:2005.08100', 'arxiv:2407.18887', 'arxiv:2502.18101', 'arxiv:2302.14220', 'arxiv:1312.5602', 'arxiv:2205.12446', 'arxiv:2208.01006', 'arxiv:2007.00224', 'arxiv:2410.03051', 'arxiv:2312.06550', 'arxiv:2402.18153', 'arxiv:2410.18634', 'arxiv:2208.02816', 'arxiv:2210.13569', 'arxiv:1906.01749', 'arxiv:1804.06876', 'arxiv:2205.03026', 'arxiv:2309.12871', 'arxiv:1802.09130', 'arxiv:2404.00495', 'arxiv:2104.09617', 'arxiv:2501.09749', 'arxiv:2104.07179', 'arxiv:1612.00796', 'arxiv:2109.10282', 'arxiv:2305.01210', 'arxiv:2107.03374', 'arxiv:2502.13898', 'arxiv:2412.09957', 'arxiv:1911.03894', 'arxiv:2211.04054', 'arxiv:2001.11314', 'arxiv:2104.01778', 'arxiv:2010.05171', 'arxiv:2007.01282', 'arxiv:2410.12391', 'arxiv:2303.10008', 'arxiv:1706.01905', 'arxiv:2101.11075', 'arxiv:2110.08193', 'arxiv:2310.05914', 'arxiv:2502.14673', 'arxiv:2502.18009', 'arxiv:2212.04356', 'arxiv:2203.01215', 'arxiv:2402.05672', 'arxiv:2310.16944', 'arxiv:2302.09611', 'arxiv:2110.05781', 'arxiv:1904.09728', 'arxiv:2208.04347', 'arxiv:2002.05202', 'arxiv:2310.08232', 'arxiv:2409.05677', 'arxiv:2210.13952', 'arxiv:2305.12567', 'arxiv:1703.07737', 'arxiv:2411.08868', 'arxiv:2410.14687', 'arxiv:1911.01547', 'arxiv:2406.18266', 'arxiv:2402.01613', 'arxiv:2312.17279', 'arxiv:2401.03462', 'arxiv:2112.08547', 'arxiv: 2310.16609', 'arxiv:2403.06018', 'arxiv:2403.04652', 'arxiv:2203.01023', 'arxiv:2403.13257', 'arxiv:2306.05685', 'arxiv:2502.18934', 'arxiv:2412.03565', 'arxiv:2308.09583', 'arxiv:2112.07916', 'arxiv:1912.08777', 'arxiv:2111.09296', 'arxiv:2004.09813', 'arxiv:2107.11414', 'arxiv:2305.05084', 'arxiv:2406.06612', 'arxiv:1705.03551', 'arxiv:2405.04324', 'arxiv:2210.07316', 'arxiv:2203.05482', 'arxiv:2010.02405', 'arxiv:2201.03545', 'arxiv:1306.4606', 'arxiv:2303.08774', 'arxiv:2308.09662', 'arxiv:2306.02231', 'arxiv:2307.11760', 'arxiv:2007.14062', 'arxiv:2406.12194', 'arxiv:2404.05961', 'arxiv:2210.09261', 'arxiv:2101.00434', 'arxiv:1910.03771', 'arxiv:2011.09468', 'arxiv:2101.06983', 'arxiv:2408.03326', 'arxiv:2411.07238', 'arxiv:2108.07732', 'arxiv:2403.04706', 'arxiv:2408.06930', 'arxiv:2408.13359', 'arxiv:1911.12559', 'arxiv:2010.13154', 'arxiv:2202.08904', 'arxiv:2401.03590', 'arxiv:2210.03629', 'arxiv:2009.11462', 'arxiv:2209.11055', 'arxiv:2003.13016', 'arxiv:2310.11441', 'arxiv:1811.00937', 'arxiv:2307.09288', 'arxiv:2312.15166', 'arxiv:2102.05095', 'arxiv:2306.02707', 'arxiv:2107.07253', 'arxiv:2207.00220', 'arxiv:2104.06979', 'arxiv:2407.00463', 'arxiv:2402.09906', 'arxiv:2005.11723', 'arxiv:1803.05457', 'arxiv:2010.11934', 'arxiv:2403.20266', 'arxiv:2402.11746', 'arxiv:1905.07830', 'arxiv:1809.02789', 'arxiv:2311.12022', 'arxiv:2411.07854', 'arxiv:2309.12284', 'arxiv:1908.10084', 'arxiv:2212.09741', 'arxiv:2412.16855', 'arxiv:2206.03065', 'arxiv:2311.03099', 'arxiv:2006.13979', 'arxiv:2109.06870', 'arxiv:2401.00368', 'arxiv:2205.14135', 'arxiv:2312.15503', 'arxiv:2310.07554', 'arxiv:2104.08663', 'arxiv:2110.15731', 'arxiv:2203.16822', 'arxiv:2407.08275', 'arxiv:2109.04127', 'arxiv:1804.09301', 'arxiv:2103.15691', 'arxiv:2207.05188', 'arxiv:2006.03677', 'arxiv:2109.07958', 'arxiv:1905.10044', 'arxiv:2209.04280', 'arxiv:2407.21139', 'arxiv:2407.19669', 'arxiv:2002.10857', 'arxiv:2104.02821', 'arxiv:2402.03300', 'arxiv:2205.12644', 'arxiv:2004.05150', 'arxiv:2210.11416', 'arxiv:2410.14745', 'arxiv:2106.04624', 'arxiv:2311.07171', 'arxiv:2410.02197', 'arxiv:2311.07911', 'arxiv:2407.15831', 'arxiv:2105.07464', 'arxiv:2409.10173', 'arxiv:2408.12503', 'arxiv:2311.05845', 'arxiv:2105.09680', 'arxiv:2305.14314', 'arxiv:1904.08779', 'arxiv:2310.06825', 'arxiv:2406.14712', 'arxiv:2305.13245', 'arxiv:2501.10979', 'arxiv:2101.03961', 'arxiv:2310.05209', 'arxiv:2301.05948', 'arxiv:2403.15246', 'arxiv:2110.14566', 'arxiv:2310.00274', 'arxiv:2103.13413', 'arxiv:2406.11617', 'arxiv:2106.07447', 'arxiv:2005.01107', 'arxiv:2111.09543', 'arxiv:2403.07691', 'arxiv:1706.03762', 'arxiv:2402.14776', 'arxiv:2006.11477', 'arxiv:2401.16640', 'arxiv:2411.16662', 'arxiv:2305.18290', 'arxiv:2005.14165', 'arxiv:2112.10752', 'arxiv:2207.14255', 'arxiv:2211.01786', 'arxiv:2407.05700', 'arxiv:2411.14402', 'arxiv:2212.10440', 'arxiv:2105.08209', 'arxiv:2110.14168', 'arxiv:2310.17389', 'arxiv:2309.10931', 'arxiv:2112.10668', 'arxiv:1910.13461', 'arxiv:2210.15226', 'arxiv:2009.03300', 'arxiv:2109.10686', 'arxiv:2308.03281', 'arxiv:1909.08053', 'arxiv:1704.02853', 'arxiv:2404.16710', 'arxiv:2407.20267', 'arxiv:2307.13989', 'arxiv:2308.16884', 'arxiv:2401.03003', 'arxiv:2104.01721', 'arxiv:2312.06281', 'arxiv:2401.04088', 'arxiv:2010.08240', 'arxiv:2105.03824', 'arxiv:2212.04089', 'arxiv:1910.01108', 'arxiv:2501.15383', 'arxiv:2404.18873', 'arxiv.org/abs/2106.13687', 'arxiv:1705.00652', 'arxiv:2406.01574', 'arxiv:2412.19048', 'arxiv:2311.00430', 'arxiv:2401.02909', 'arxiv:2302.08468', 'arxiv:2010.02666', 'arxiv:2410.02713', 'arxiv:1809.05053', 'arxiv:2407.10671', 'arxiv:2107.07402', 'arxiv:2206.04615', 'arxiv:2201.02419', 'arxiv:2309.00071', 'arxiv:2408.15710', 'arxiv:2205.11111', 'arxiv:2410.17437', 'arxiv:2405.06932', 'arxiv:2402.03216', 'arxiv:2204.05862', 'arxiv:2306.08568', 'arxiv:2405.17428', 'arxiv:2108.12409', 'arxiv:2312.11805', 'arxiv:1907.10641', 'arxiv:2308.12950', 'arxiv:1810.04805', 'arxiv:2305.14201', 'arxiv:2309.11235', 'arxiv:2106.09685', 'arxiv:2304.06795', 'arxiv:2501.12386', 'arxiv:2412.13663', 'arxiv:2311.07767', 'arxiv:2402.15449', 'arxiv:2402.09844', 'arxiv:2406.07188', 'arxiv:2305.11206', 'arxiv:2407.13690', 'arxiv:1911.02116', 'arxiv:2402.14905', 'arxiv:1911.02671', 'arxiv:2311.03226', 'arxiv:2205.01068', 'arxiv:2210.02592', 'arxiv:2407.21489', 'arxiv:2405.06640', 'arxiv:2212.09535', 'arxiv:2103.07762', 'arxiv:2011.13205', 'arxiv:2104.02112', 'arxiv:2407.11828', 'arxiv:1804.05685', 'arxiv:2306.17492', 'arxiv:1907.11692', 'arxiv:2405.03548', 'arxiv:2006.03654', 'arxiv:2307.01672', 'arxiv:2412.04506', 'arxiv:2402.17016', 'arxiv:2103.16801', 'arxiv:2210.03992', 'arxiv:2402.15343', 'arxiv:2306.02928', 'arxiv:2407.03941', 'arxiv:1910.09700', 'arxiv:2307.11224', 'arxiv:2310.14282', 'arxiv:2307.01952', 'arxiv:2304.00869', 'arxiv:2311.08526', 'arxiv:2302.13971', 'arxiv:2210.17323', 'arxiv:2402.16829', 'arxiv:2203.03759', 'arxiv:2304.06364', 'arxiv:2207.12345', 'arxiv:2501.00574', 'arxiv:2012.15828', 'arxiv:1808.10583', 'arxiv:2402.13350', 'arxiv:2411.15124', 'arxiv:2308.07074', 'arxiv:2409.06656', 'arxiv:2403.02884', 'arxiv:1907.10529', 'arxiv:2403.19522', 'arxiv:2104.09864', 'arxiv:2307.03170', 'arxiv:2408.13106']\n",
      "380\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of arvix ids {dataframe[dataframe[\"arxiv_ids\"].notnull()].shape}\")\n",
    "\n",
    "paper_list = dataframe[\"arxiv_ids\"].dropna().tolist() # Retrieve arXiv IDs\n",
    "print(len(paper_list)) # 3125\n",
    "split_data = [entry.strip() for item in paper_list for entry in item.split(',')] # Split and create a list of arXiv IDs\n",
    "split_data = list(set(split_data)) # Remove duplicates\n",
    "print(split_data)\n",
    "print(len(split_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_info_list = []\n",
    "\n",
    "for paper in split_data:\n",
    "    try:\n",
    "        print(f\"Getting info for paper {paper}\")\n",
    "        paper_id = paper.split(\":\")[1].strip()\n",
    "        print(f\"Paper ID: {paper_id}\")\n",
    "        paper_info = api.paper_info(paper_id)\n",
    "        if isinstance(paper_info.authors, list):\n",
    "            paper_authors = \", \".join(paper_info.authors)\n",
    "        else:\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting paper info for {paper}: {e}\")\n",
    "        continue\n",
    "\n",
    "    paper_info_list.append([paper_info.id, paper_authors, paper_info.title, paper_info.summary])\n",
    "    \n",
    "paper_info_df = pd.DataFrame(paper_info_list, columns=[\"id\", \"authors\", \"title\", \"summary\"])\n",
    "paper_info_df.to_csv(\"./output/paper_info.csv\", index=False)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
