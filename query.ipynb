{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: oxrdflib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.4.0)\n",
      "Requirement already satisfied: Pygments in /Users/saasyp/Library/Python/3.12/lib/python/site-packages (2.19.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/saasyp/Library/Python/3.12/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: pyoxigraph~=0.4.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from oxrdflib) (0.4.7)\n",
      "Requirement already satisfied: rdflib<8.0,>=6.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from oxrdflib) (7.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/saasyp/Library/Python/3.12/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rdflib<8.0,>=6.3->oxrdflib) (3.2.1)\n",
      "4149107\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas oxrdflib Pygments\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "from pygments import highlight\n",
    "from pygments.lexers import SparqlLexer\n",
    "from pygments.formatters import HtmlFormatter\n",
    "from rdflib import Graph\n",
    "from rdflib.plugins.sparql import prepareQuery\n",
    "\n",
    "def run_query(graph, query_path):\n",
    "    try:\n",
    "        with open(query_path, 'r') as file:\n",
    "            query = file.read()\n",
    "    except Exception as _e:\n",
    "        print(f\"No file for {query_path}\")\n",
    "        return\n",
    "    results = graph.query(query)\n",
    "    # Display the SPARQL query\n",
    "    formatted_query = highlight(query, SparqlLexer(), HtmlFormatter(style='solarized-dark', full=True, nobackground=True))\n",
    "    display(HTML(formatted_query))\n",
    "    # Convert results to a Pandas DataFrame\n",
    "    res_list = []\n",
    "    for row in results:\n",
    "        res_list.append([str(item) for item in row])\n",
    "    df = pd.DataFrame(res_list, columns=[str(var) for var in results.vars]) if len(res_list) > 0 else pd.DataFrame()\n",
    "    # Display the DataFrame as a table in Jupyter Notebook\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "g = Graph(store=\"Oxigraph\")\n",
    "\n",
    "g.parse(\"output/graph/models-metrics-papers.ttl\", format=\"ttl\")\n",
    "print(len(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **QUERIES TO ASSESS THE QUALITY OF THE GRAPH**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Number of instances for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">SELECT</span> <span class=\"nv\">?class</span> <span class=\"p\">(</span><span class=\"nf\">COUNT</span><span class=\"p\">(</span><span class=\"nv\">?instance</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"nv\">?count</span><span class=\"p\">)</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "  <span class=\"p\">{</span>\n",
       "    <span class=\"nv\">?instance</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">.</span>\n",
       "    <span class=\"k\">BIND</span><span class=\"p\">(</span><span class=\"s\">&quot;Model&quot;</span> <span class=\"k\">AS</span> <span class=\"nv\">?class</span><span class=\"p\">)</span>\n",
       "  <span class=\"p\">}</span>\n",
       "  <span class=\"k\">UNION</span>\n",
       "  <span class=\"p\">{</span>\n",
       "    <span class=\"nv\">?instance</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">HyperParameter</span> <span class=\"p\">.</span>\n",
       "    <span class=\"k\">BIND</span><span class=\"p\">(</span><span class=\"s\">&quot;HyperParameter&quot;</span> <span class=\"k\">AS</span> <span class=\"nv\">?class</span><span class=\"p\">)</span>\n",
       "  <span class=\"p\">}</span>\n",
       "  <span class=\"k\">UNION</span>\n",
       "  <span class=\"p\">{</span>\n",
       "    <span class=\"nv\">?instance</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">EvaluationMetric</span> <span class=\"p\">.</span>\n",
       "    <span class=\"k\">BIND</span><span class=\"p\">(</span><span class=\"s\">&quot;EvaluationMetric&quot;</span> <span class=\"k\">AS</span> <span class=\"nv\">?class</span><span class=\"p\">)</span>\n",
       "  <span class=\"p\">}</span>\n",
       "  <span class=\"k\">UNION</span>\n",
       "  <span class=\"p\">{</span>\n",
       "    <span class=\"nv\">?instance</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Paper</span> <span class=\"p\">.</span>\n",
       "    <span class=\"k\">BIND</span><span class=\"p\">(</span><span class=\"s\">&quot;Paper&quot;</span> <span class=\"k\">AS</span> <span class=\"nv\">?class</span><span class=\"p\">)</span>\n",
       "  <span class=\"p\">}</span>\n",
       "<span class=\"p\">}</span>\n",
       "<span class=\"k\">GROUP BY</span> <span class=\"nv\">?class</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paper</td>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HyperParameter</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EvaluationMetric</td>\n",
       "      <td>666427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model</td>\n",
       "      <td>18683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_query(g, \"queries/quality/q1.rq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identify models missing names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">SELECT</span> <span class=\"nv\">?model</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "  <span class=\"nv\">?model</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">.</span>\n",
       "  <span class=\"k\">FILTER</span> <span class=\"nf\">NOT EXISTS</span> <span class=\"p\">{</span> <span class=\"nv\">?model</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">name</span> <span class=\"nv\">?name</span> <span class=\"p\">}</span>\n",
       "<span class=\"p\">}</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_query(g, \"queries/quality/q2.rq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Models missing hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">SELECT</span> <span class=\"nv\">?class</span> <span class=\"p\">(</span><span class=\"nf\">COUNT</span><span class=\"p\">(</span><span class=\"nv\">?instance</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"nv\">?count</span><span class=\"p\">)</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "  <span class=\"nv\">?instance</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">.</span>\n",
       "  <span class=\"k\">BIND</span><span class=\"p\">(</span><span class=\"s\">&quot;Model&quot;</span> <span class=\"k\">AS</span> <span class=\"nv\">?class</span><span class=\"p\">)</span> <span class=\"p\">.</span>\n",
       "  <span class=\"k\">FILTER</span> <span class=\"nf\">NOT EXISTS</span> <span class=\"p\">{</span> <span class=\"nv\">?instance</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">hasHyperParameter</span> <span class=\"nv\">?hp</span> <span class=\"p\">}</span>\n",
       "<span class=\"p\">}</span>\n",
       "<span class=\"k\">GROUP BY</span> <span class=\"nv\">?class</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model</td>\n",
       "      <td>18669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_query(g, \"queries/quality/q3.rq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Domain inconsistencies: this query will return all triples where there is a potential domain inconsistency - e.g. the property declares a domain but the subject is not typed as an instance of that domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">PREFIX</span> <span class=\"nn\">rdfs</span><span class=\"p\">:</span> <span class=\"nl\">&lt;http://www.w3.org/2000/01/rdf-schema#&gt;</span>\n",
       "\n",
       "<span class=\"k\">SELECT</span> <span class=\"nv\">?s</span> <span class=\"nv\">?p</span> <span class=\"nv\">?o</span> <span class=\"nv\">?expectedDomain</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "  <span class=\"nv\">?s</span> <span class=\"nv\">?p</span> <span class=\"nv\">?o</span> <span class=\"p\">.</span>\n",
       "  <span class=\"nv\">?p</span> <span class=\"nn\">rdfs</span><span class=\"p\">:</span><span class=\"nt\">domain</span> <span class=\"nv\">?expectedDomain</span> <span class=\"p\">.</span>\n",
       "  <span class=\"k\">FILTER</span> <span class=\"nf\">NOT EXISTS</span> <span class=\"p\">{</span> <span class=\"nv\">?s</span> <span class=\"k\">a</span> <span class=\"nv\">?expectedDomain</span> <span class=\"p\">}</span>\n",
       "  <span class=\"k\">FILTER</span><span class=\"p\">(</span><span class=\"nf\">isIRI</span><span class=\"p\">(</span><span class=\"nv\">?s</span><span class=\"p\">))</span>\n",
       "<span class=\"p\">}</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_query(g, \"queries/quality/q4.rq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">SELECT</span> <span class=\"nv\">?entity</span> <span class=\"nv\">?property</span> <span class=\"nv\">?value</span> <span class=\"nv\">?expectedRange</span> <span class=\"nv\">?datatypeValue</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span> \n",
       "  <span class=\"nv\">?entity</span> <span class=\"nv\">?property</span> <span class=\"nv\">?value</span> <span class=\"p\">.</span>\n",
       "  <span class=\"nv\">?property</span> <span class=\"nn\">rdfs</span><span class=\"p\">:</span><span class=\"nt\">range</span> <span class=\"nv\">?expectedRange</span> <span class=\"p\">.</span>\n",
       "  <span class=\"k\">FILTER</span><span class=\"p\">(</span><span class=\"nf\">isLiteral</span><span class=\"p\">(</span><span class=\"nv\">?value</span><span class=\"p\">))</span> <span class=\"p\">.</span>\n",
       "  <span class=\"k\">BIND</span><span class=\"p\">(</span><span class=\"nf\">DATATYPE</span><span class=\"p\">(</span><span class=\"nv\">?value</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"nv\">?datatypeValue</span><span class=\"p\">)</span> <span class=\"p\">.</span>\n",
       "  <span class=\"k\">FILTER</span><span class=\"p\">(</span>\n",
       "      <span class=\"nv\">?expectedRange</span> <span class=\"o\">!=</span> <span class=\"nn\">rdfs</span><span class=\"p\">:</span><span class=\"nt\">Literal</span> <span class=\"o\">&amp;&amp;</span> \n",
       "      <span class=\"nv\">?datatypeValue</span> <span class=\"o\">!=</span> <span class=\"nv\">?expectedRange</span> <span class=\"o\">&amp;&amp;</span> \n",
       "      <span class=\"nv\">?datatypeValue</span> <span class=\"o\">!=</span> <span class=\"nn\">rdf</span><span class=\"p\">:</span><span class=\"nt\">langString</span>\n",
       "  <span class=\"p\">)</span>\n",
       "<span class=\"p\">}</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This helped me to understand that I used a wrong range for the \"creator\" property. I had used a Literal instead of a URI.\n",
    "# I fixed this in the data generation script and re-generated the data.\n",
    "\n",
    "run_query(g, \"queries/quality/q5.rq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple models with the same name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">SELECT</span> <span class=\"nv\">?name</span> <span class=\"p\">(</span><span class=\"nf\">COUNT</span><span class=\"p\">(</span><span class=\"k\">DISTINCT</span> <span class=\"nv\">?model</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"nv\">?duplicateCount</span><span class=\"p\">)</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span> <span class=\"nv\">?model</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">;</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">name</span> <span class=\"nv\">?name</span> <span class=\"p\">.}</span>\n",
       "<span class=\"k\">GROUP BY</span> <span class=\"nv\">?name</span>\n",
       "<span class=\"k\">HAVING</span> <span class=\"p\">(</span><span class=\"nf\">COUNT</span><span class=\"p\">(</span><span class=\"k\">DISTINCT</span> <span class=\"nv\">?model</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_query(g, \"queries/quality/q6.rq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Untyped entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">SELECT</span> <span class=\"k\">DISTINCT</span> <span class=\"nv\">?instance</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       " <span class=\"nv\">?instance</span> <span class=\"nv\">?property</span> <span class=\"nv\">?value</span> <span class=\"p\">.</span>\n",
       " <span class=\"k\">FILTER</span> <span class=\"nf\">NOT EXISTS</span> <span class=\"p\">{</span><span class=\"nv\">?instance</span> <span class=\"nn\">rdf</span><span class=\"p\">:</span><span class=\"nt\">type</span> <span class=\"nv\">?class</span> <span class=\"p\">.}}</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://schema.org/Person</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_query(g, \"queries/quality/q7.rq\")\n",
    "\n",
    "# Sincere this returned http://schema.org/Person because I didn't assert that Person is a class in my graph, in order to ensure consistency\n",
    "# but also because I am not importing the full schema ontology I decided to create the Person class in my graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicated triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">SELECT</span> <span class=\"nv\">?s</span> <span class=\"nv\">?p</span> <span class=\"nv\">?o</span> <span class=\"p\">(</span><span class=\"nf\">COUNT</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"nv\">?dupCount</span><span class=\"p\">)</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "  <span class=\"nv\">?s</span> <span class=\"nv\">?p</span> <span class=\"nv\">?o</span> <span class=\"p\">.</span>\n",
       "<span class=\"p\">}</span>\n",
       "<span class=\"k\">GROUP BY</span> <span class=\"nv\">?s</span> <span class=\"nv\">?p</span> <span class=\"nv\">?o</span>\n",
       "<span class=\"k\">HAVING</span> <span class=\"p\">(</span><span class=\"nf\">COUNT</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_query(g, \"queries/quality/q8.rq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **QUERIES TO GAIN INSIGHTS FROM THE GRAPH**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 5 most downloaded models for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">SELECT</span> <span class=\"nv\">?name</span> <span class=\"nv\">?task</span> <span class=\"nv\">?downloads</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "  <span class=\"nv\">?model</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">name</span> <span class=\"nv\">?name</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">task</span> <span class=\"nv\">?task</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">downloads</span> <span class=\"nv\">?downloads</span> <span class=\"p\">.</span>\n",
       "  <span class=\"p\">{</span>\n",
       "    <span class=\"k\">SELECT</span> <span class=\"nv\">?model</span> <span class=\"nv\">?task</span> <span class=\"nv\">?downloads</span> <span class=\"p\">(</span><span class=\"nf\">COUNT</span><span class=\"p\">(</span><span class=\"nv\">?otherModel</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"nv\">?rank</span><span class=\"p\">)</span>\n",
       "    <span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "      <span class=\"nv\">?model</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">;</span>\n",
       "             <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">task</span> <span class=\"nv\">?task</span> <span class=\"p\">;</span>\n",
       "             <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">downloads</span> <span class=\"nv\">?downloads</span> <span class=\"p\">.</span>\n",
       "      <span class=\"k\">OPTIONAL</span> <span class=\"p\">{</span>\n",
       "         <span class=\"nv\">?otherModel</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">;</span>\n",
       "                     <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">task</span> <span class=\"nv\">?task</span> <span class=\"p\">;</span>\n",
       "                     <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">downloads</span> <span class=\"nv\">?otherDownloads</span> <span class=\"p\">.</span>\n",
       "         <span class=\"k\">FILTER</span><span class=\"p\">(</span><span class=\"nn\">xsd</span><span class=\"p\">:</span><span class=\"nt\">integer</span><span class=\"p\">(</span><span class=\"nv\">?otherDownloads</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"nn\">xsd</span><span class=\"p\">:</span><span class=\"nt\">integer</span><span class=\"p\">(</span><span class=\"nv\">?downloads</span><span class=\"p\">))</span>\n",
       "      <span class=\"p\">}</span>\n",
       "    <span class=\"p\">}</span>\n",
       "    <span class=\"k\">GROUP BY</span> <span class=\"nv\">?model</span> <span class=\"nv\">?task</span> <span class=\"nv\">?downloads</span>\n",
       "  <span class=\"p\">}</span>\n",
       "  <span class=\"k\">FILTER</span><span class=\"p\">(</span><span class=\"nv\">?rank</span> <span class=\"o\">&lt;</span> <span class=\"mi\">5</span><span class=\"p\">)</span>\n",
       "<span class=\"p\">}</span>\n",
       "<span class=\"k\">ORDER BY</span> <span class=\"nv\">?task</span> <span class=\"k\">DESC</span><span class=\"p\">(</span><span class=\"nn\">xsd</span><span class=\"p\">:</span><span class=\"nt\">integer</span><span class=\"p\">(</span><span class=\"nv\">?downloads</span><span class=\"p\">))</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>task</th>\n",
       "      <th>downloads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ast-finetuned-speech-commands-v2</td>\n",
       "      <td>audio-classification</td>\n",
       "      <td>54486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>diar_sortformer_4spk-v1</td>\n",
       "      <td>audio-classification</td>\n",
       "      <td>5350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>distil-ast-audioset-finetuned-cry</td>\n",
       "      <td>audio-classification</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wav2vec2_turkish_gender_classification</td>\n",
       "      <td>audio-classification</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>genre-recognizer-finetuned-gtzan_dset</td>\n",
       "      <td>audio-classification</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sepformer-dns4-16k-enhancement</td>\n",
       "      <td>audio-to-audio</td>\n",
       "      <td>26265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>open-universe</td>\n",
       "      <td>audio-to-audio</td>\n",
       "      <td>17603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EBEN_temple_vibration_pickup</td>\n",
       "      <td>audio-to-audio</td>\n",
       "      <td>13375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>EBEN_throat_microphone</td>\n",
       "      <td>audio-to-audio</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EBEN_rigid_in_ear_microphone</td>\n",
       "      <td>audio-to-audio</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>wav2vec2-large-xlsr-53-chinese-zh-cn</td>\n",
       "      <td>automatic-speech-recognition</td>\n",
       "      <td>5485180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>wav2vec2-large-xlsr-53-russian</td>\n",
       "      <td>automatic-speech-recognition</td>\n",
       "      <td>4551206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>wav2vec2-large-xlsr-53-portuguese</td>\n",
       "      <td>automatic-speech-recognition</td>\n",
       "      <td>4180771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>whisper-base.en</td>\n",
       "      <td>automatic-speech-recognition</td>\n",
       "      <td>2126604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>wav2vec2-large-xlsr-korean</td>\n",
       "      <td>automatic-speech-recognition</td>\n",
       "      <td>920174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dpt-hybrid-midas</td>\n",
       "      <td>depth-estimation</td>\n",
       "      <td>170499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>jina-embeddings-v3</td>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>2198589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>jina-embeddings-v2-small-en</td>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>1429342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bge-multilingual-gemma2</td>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>155319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bge-large-en-v1.5-quant</td>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>76850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>jina-embeddings-v2-base-de</td>\n",
       "      <td>feature-extraction</td>\n",
       "      <td>59504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>BERTu</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>3954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RoBERTaCrawlPT-base</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ClinicalMosaic</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bert-base-cased-pt-lenerbr</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ZINC-deberta</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>rorshark-vit-base</td>\n",
       "      <td>image-classification</td>\n",
       "      <td>677118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>vit-base-patch16-224-in21k-finetuned-moderation</td>\n",
       "      <td>image-classification</td>\n",
       "      <td>58957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>nsfw-image-detector</td>\n",
       "      <td>image-classification</td>\n",
       "      <td>48528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>pedestrian_age_recognition</td>\n",
       "      <td>image-classification</td>\n",
       "      <td>28291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>vit-base-beans</td>\n",
       "      <td>image-classification</td>\n",
       "      <td>18335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>aimv2-large-patch14-224</td>\n",
       "      <td>image-feature-extraction</td>\n",
       "      <td>3749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>aimv2-huge-patch14-448</td>\n",
       "      <td>image-feature-extraction</td>\n",
       "      <td>2838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>aimv2-3B-patch14-448</td>\n",
       "      <td>image-feature-extraction</td>\n",
       "      <td>2351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>aimv2-large-patch14-448</td>\n",
       "      <td>image-feature-extraction</td>\n",
       "      <td>2268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>aimv2-1B-patch14-448</td>\n",
       "      <td>image-feature-extraction</td>\n",
       "      <td>1873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>yolov8m-building-segmentation</td>\n",
       "      <td>image-segmentation</td>\n",
       "      <td>1836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>yolov8m-pcb-defect-segmentation</td>\n",
       "      <td>image-segmentation</td>\n",
       "      <td>1289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>yolov8n-pcb-defect-segmentation</td>\n",
       "      <td>image-segmentation</td>\n",
       "      <td>913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>yolov8s-building-segmentation</td>\n",
       "      <td>image-segmentation</td>\n",
       "      <td>858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>yolov8n-building-segmentation</td>\n",
       "      <td>image-segmentation</td>\n",
       "      <td>856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>PixelParse_AI</td>\n",
       "      <td>image-text-to-text</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>DocumentCogito</td>\n",
       "      <td>image-text-to-text</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>AuroraCap-7B-IMG-xtuner</td>\n",
       "      <td>image-text-to-text</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>qwen2-vl-7b-rslora-offensive-meme-singapore</td>\n",
       "      <td>image-text-to-text</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>saved_model_git-base</td>\n",
       "      <td>image-text-to-text</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>swin-distilbertimbau</td>\n",
       "      <td>image-to-text</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>test-push</td>\n",
       "      <td>image-to-text</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>swin-gportuguese-2</td>\n",
       "      <td>image-to-text</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>trocr-base-printed-synthetic_dataset_ocr</td>\n",
       "      <td>image-to-text</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>llm-mdeberta-v3-swag</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>chinese_paragraph_bert-ext</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>ft-bert-with-swag</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>llm-deberta-v3-swag</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>roberta-base-uncased-finetuned-swag</td>\n",
       "      <td>multiple-choice</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>yolov8m-table-extraction</td>\n",
       "      <td>object-detection</td>\n",
       "      <td>9332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>yolov8m-hard-hat-detection</td>\n",
       "      <td>object-detection</td>\n",
       "      <td>5508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>yolov8n-table-extraction</td>\n",
       "      <td>object-detection</td>\n",
       "      <td>2364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>rt-detr-finetuned-for-satellite-image-roofs-detection</td>\n",
       "      <td>object-detection</td>\n",
       "      <td>2241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>yolov8s-table-extraction</td>\n",
       "      <td>object-detection</td>\n",
       "      <td>2164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>flan-t5-large-squad2</td>\n",
       "      <td>question-answering</td>\n",
       "      <td>579573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>minilm-uncased-squad2</td>\n",
       "      <td>question-answering</td>\n",
       "      <td>153510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>roberta-large-squad2</td>\n",
       "      <td>question-answering</td>\n",
       "      <td>44772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>roberta-base-squad2-distilled</td>\n",
       "      <td>question-answering</td>\n",
       "      <td>14311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>bert-base-uncased-squad2</td>\n",
       "      <td>question-answering</td>\n",
       "      <td>12929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>ppo-Acrobot-v1</td>\n",
       "      <td>reinforcement-learning</td>\n",
       "      <td>10026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>ppo-LunarLanderContinuous-v2</td>\n",
       "      <td>reinforcement-learning</td>\n",
       "      <td>9424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>ppo-Pendulum-v1</td>\n",
       "      <td>reinforcement-learning</td>\n",
       "      <td>9355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>ppo-MountainCarContinuous-v0</td>\n",
       "      <td>reinforcement-learning</td>\n",
       "      <td>9353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>dqn-Acrobot-v1</td>\n",
       "      <td>reinforcement-learning</td>\n",
       "      <td>7406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>ppo-LiftCube-v0</td>\n",
       "      <td>robotics</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>GIST-Embedding-v0</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>573882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>snowflake-arctic-embed-m</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>548224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>snowflake-arctic-embed-xs</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>274778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>mmlw-roberta-base</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>170712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>rubert-tiny-turbo</td>\n",
       "      <td>sentence-similarity</td>\n",
       "      <td>148568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>pegasus-xsum</td>\n",
       "      <td>summarization</td>\n",
       "      <td>152043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>MEETING_SUMMARY</td>\n",
       "      <td>summarization</td>\n",
       "      <td>23298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>bart-large-xsum</td>\n",
       "      <td>summarization</td>\n",
       "      <td>14202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>bigbird-pegasus-large-arxiv</td>\n",
       "      <td>summarization</td>\n",
       "      <td>9493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>bart-german</td>\n",
       "      <td>summarization</td>\n",
       "      <td>5801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>human-disease-prediction</td>\n",
       "      <td>tabular-classification</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>TF_Decision_Trees</td>\n",
       "      <td>tabular-classification</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>diabetes-readmission</td>\n",
       "      <td>tabular-classification</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>distilroberta-finetuned-financial-news-sentiment-analysis</td>\n",
       "      <td>text-classification</td>\n",
       "      <td>1528107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>twitter-xlm-roberta-base-sentiment-multilingual</td>\n",
       "      <td>text-classification</td>\n",
       "      <td>430788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>distilbert-base-uncased-emotion</td>\n",
       "      <td>text-classification</td>\n",
       "      <td>211755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>twitter-roberta-base-dec2021-tweet-topic-multi-all</td>\n",
       "      <td>text-classification</td>\n",
       "      <td>173642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>IndoBERT-Sentiment-Analysis</td>\n",
       "      <td>text-classification</td>\n",
       "      <td>37949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>llava-onevision-qwen2-7b-ov</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>97504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>calme-2.3-qwen2-7b</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>44151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>llava-onevision-qwen2-0.5b-ov</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>40100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>gpt2-chatbot</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>16643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>megatron-gpt2-345m</td>\n",
       "      <td>text-generation</td>\n",
       "      <td>16020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>ldm3d-4c</td>\n",
       "      <td>text-to-3d</td>\n",
       "      <td>2630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ldm3d-pano</td>\n",
       "      <td>text-to-3d</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ldm3d</td>\n",
       "      <td>text-to-3d</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ldm3d-sr</td>\n",
       "      <td>text-to-3d</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>FFusionXL-BASE</td>\n",
       "      <td>text-to-image</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>rpg</td>\n",
       "      <td>text-to-image</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Xena</td>\n",
       "      <td>text-to-image</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>tts-ckb</td>\n",
       "      <td>text-to-speech</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>speecht5_finetuned_voxpopuli_es</td>\n",
       "      <td>text-to-speech</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>speecht5-finetuned-voxpopuli-sl</td>\n",
       "      <td>text-to-speech</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>speecht5_finetuned_voxpopuli_nl</td>\n",
       "      <td>text-to-speech</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>speecht5_finetuned_voxpopuli_it</td>\n",
       "      <td>text-to-speech</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>mt0-small</td>\n",
       "      <td>text2text-generation</td>\n",
       "      <td>50901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>sage-fredt5-distilled-95m</td>\n",
       "      <td>text2text-generation</td>\n",
       "      <td>25955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>ZINC-t5-v2</td>\n",
       "      <td>text2text-generation</td>\n",
       "      <td>8818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>mt0-large</td>\n",
       "      <td>text2text-generation</td>\n",
       "      <td>4046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>mt0-base</td>\n",
       "      <td>text2text-generation</td>\n",
       "      <td>3663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>indonesian-roberta-base-posp-tagger</td>\n",
       "      <td>token-classification</td>\n",
       "      <td>3336050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>ner-bert-base-cased-pt-lenerbr</td>\n",
       "      <td>token-classification</td>\n",
       "      <td>68003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>roberta-large-ontonotes5</td>\n",
       "      <td>token-classification</td>\n",
       "      <td>61157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>keyphrase-extraction-distilbert-inspec</td>\n",
       "      <td>token-classification</td>\n",
       "      <td>22036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>distilbert-base-cased-finetuned-conll03-english</td>\n",
       "      <td>token-classification</td>\n",
       "      <td>18937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>opus-mt-tc-big-en-tr</td>\n",
       "      <td>translation</td>\n",
       "      <td>69414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>opus-mt-tc-big-tr-en</td>\n",
       "      <td>translation</td>\n",
       "      <td>52983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>opus-mt-tc-big-sh-en</td>\n",
       "      <td>translation</td>\n",
       "      <td>42445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>opus-mt-tc-big-en-pt</td>\n",
       "      <td>translation</td>\n",
       "      <td>38495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>opus-mt-tc-base-en-sh</td>\n",
       "      <td>translation</td>\n",
       "      <td>13059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>xclip-base-patch16-kinetics-600-16-frames</td>\n",
       "      <td>video-classification</td>\n",
       "      <td>6623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>xclip-large-patch14</td>\n",
       "      <td>video-classification</td>\n",
       "      <td>1830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>xclip-base-patch16-16-frames</td>\n",
       "      <td>video-classification</td>\n",
       "      <td>1360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>xclip-base-patch16</td>\n",
       "      <td>video-classification</td>\n",
       "      <td>1017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>xclip-large-patch14-kinetics-600</td>\n",
       "      <td>video-classification</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>LLaVA-Video-7B-Qwen2</td>\n",
       "      <td>video-text-to-text</td>\n",
       "      <td>78699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>InternVideo2_5_Chat_8B</td>\n",
       "      <td>video-text-to-text</td>\n",
       "      <td>14014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>VideoChat-Flash-Qwen2-7B_res448</td>\n",
       "      <td>video-text-to-text</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>SpaceTimeGPT</td>\n",
       "      <td>video-text-to-text</td>\n",
       "      <td>1367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>VideoChat-Flash-Qwen2_5-2B_res448</td>\n",
       "      <td>video-text-to-text</td>\n",
       "      <td>1219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>DeBERTa-v3-base-mnli-fever-anli</td>\n",
       "      <td>zero-shot-classification</td>\n",
       "      <td>908353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>mDeBERTa-v3-base-xnli-multilingual-nli-2mil7</td>\n",
       "      <td>zero-shot-classification</td>\n",
       "      <td>70167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>deberta-v3-base-tasksource-nli</td>\n",
       "      <td>zero-shot-classification</td>\n",
       "      <td>22265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>xlm-roberta-large-it-mnli</td>\n",
       "      <td>zero-shot-classification</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>flan-t5-base-mnli</td>\n",
       "      <td>zero-shot-classification</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_query(g, \"queries/results/q1.rq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given a task, look for the most used datasets. The query shows image-classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">SELECT</span> <span class=\"nv\">?dataset</span> <span class=\"p\">(</span><span class=\"nf\">COUNT</span><span class=\"p\">(</span><span class=\"nv\">?metric</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"nv\">?metricCount</span><span class=\"p\">)</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "  <span class=\"nv\">?metric</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">datasetName</span> <span class=\"nv\">?dataset</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">taskType</span> <span class=\"s\">&quot;text-classification&quot;</span> <span class=\"p\">.</span>\n",
       "<span class=\"p\">}</span>\n",
       "<span class=\"k\">GROUP BY</span> <span class=\"nv\">?dataset</span>\n",
       "<span class=\"k\">ORDER BY</span> <span class=\"k\">DESC</span><span class=\"p\">(</span><span class=\"nv\">?metricCount</span><span class=\"p\">)</span>\n",
       "<span class=\"k\">LIMIT</span> <span class=\"mi\">10</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>metricCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glue</td>\n",
       "      <td>546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>emotion</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>imdb</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GLUE QQP</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GLUE MRPC</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tweet_eval</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GLUE STSB</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GLUE MNLI</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GLUE QNLI</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_query(g, \"queries/results/q2.rq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By finetuning a specific model, (e.g., BERT, gpt2, xlm-roberta-large etc..) for which tasks can I use it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">SELECT</span> <span class=\"k\">DISTINCT</span> <span class=\"nv\">?task</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "  <span class=\"nv\">?model</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">base_model</span> <span class=\"nv\">?baseModel</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">task</span> <span class=\"nv\">?task</span> <span class=\"p\">.</span>\n",
       "  <span class=\"k\">FILTER</span><span class=\"p\">(</span><span class=\"nf\">LCASE</span><span class=\"p\">(</span><span class=\"nf\">STR</span><span class=\"p\">(</span><span class=\"nv\">?baseModel</span><span class=\"p\">))</span> <span class=\"o\">=</span> <span class=\"s\">&quot;xlm-roberta-large&quot;</span><span class=\"p\">)</span>\n",
       "<span class=\"p\">}</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text-classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>token-classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence-similarity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_query(g, \"queries/results/q3.rq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the text-classification models (or any other task) that supports more than one language (or a specific one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">SELECT</span> <span class=\"nv\">?name</span> <span class=\"p\">(</span><span class=\"nf\">COUNT</span><span class=\"p\">(</span><span class=\"k\">DISTINCT</span> <span class=\"nv\">?lang</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"nv\">?languageCount</span><span class=\"p\">)</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "  <span class=\"nv\">?model</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">task</span> <span class=\"s\">&quot;text-classification&quot;</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">language</span> <span class=\"nv\">?lang</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">name</span> <span class=\"nv\">?name</span> <span class=\"p\">.</span>\n",
       "<span class=\"p\">}</span>\n",
       "<span class=\"k\">GROUP BY</span> <span class=\"nv\">?model</span> <span class=\"nv\">?name</span>\n",
       "<span class=\"k\">HAVING</span> <span class=\"p\">(</span><span class=\"nf\">COUNT</span><span class=\"p\">(</span><span class=\"k\">DISTINCT</span> <span class=\"nv\">?lang</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>languageCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ModernBERT-base-long-context-qe-v1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ModernBERT-large-qe-v1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>multilingual_minilm-amazon_massive-intent_eu7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zephyr-dpo-v2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BAAI-bge-reranker-large</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pixel-base-finetuned-xnli-translate-train-all</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xlm-roberta-base-sentiment-multilingual-finetuned</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ModernBERT-large-qe-maxlen512-v1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ModernBERT-base-qe-v1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ModernBERT-base-qe-maxlen512-lr3e-04-v1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>satoken</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LdIR-Qwen2-reranker-1.5B</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Emotion_RoBERTa_pooled_V4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>xlm-r-icils-ilo</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>XLMR-ENIS-finetuned-cola</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>IndoBERT-Sentiment-Analysis</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bge-reranker-large-1k</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>xlmr-chatgptdetect-noisy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Multimodal_Hate_Speech_Detection_in_Dravidian_languages</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>multilingual_minilm-amazon_massive-intent_eu6_noen</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>xlm-roberta-large-qe-v1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">SELECT</span> <span class=\"nv\">?name</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "  <span class=\"nv\">?model</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">task</span> <span class=\"s\">&quot;text-generation&quot;</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">language</span> <span class=\"s\">&quot;it&quot;</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">name</span> <span class=\"nv\">?name</span> <span class=\"p\">.</span>\n",
       "<span class=\"p\">}</span>\n",
       "<span class=\"k\">GROUP BY</span> <span class=\"nv\">?model</span> <span class=\"nv\">?name</span>\n",
       "<span class=\"k\">LIMIT</span> <span class=\"mi\">5</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jajuka-3b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Llama-3.1-8B-AlpaCare-MedInstruct-GGUF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ultiima-72B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>openbuddy-nemotron-70b-v23.1-131k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>openbuddy-deepseek-67b-v18.1-4k</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query for more than one language\n",
    "run_query(g, \"queries/results/q4.rq\")\n",
    "\n",
    "# Query for a specific language (e.g., en, it, fr ...)\n",
    "run_query(g, \"queries/results/q5.rq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the most common metrics when evaluating a model on a given task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">SELECT</span> <span class=\"nv\">?task</span> <span class=\"nv\">?metricType</span> <span class=\"p\">(</span><span class=\"nf\">COUNT</span><span class=\"p\">(</span><span class=\"nv\">?metric</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"nv\">?metricCount</span><span class=\"p\">)</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "  <span class=\"nv\">?model</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">task</span> <span class=\"nv\">?task</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">hasEvaluationMetric</span> <span class=\"nv\">?metric</span> <span class=\"p\">.</span>\n",
       "  <span class=\"nv\">?metric</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">metricType</span> <span class=\"nv\">?metricType</span> <span class=\"p\">.</span>\n",
       "  <span class=\"k\">FILTER</span><span class=\"p\">(</span><span class=\"nf\">LCASE</span><span class=\"p\">(</span><span class=\"nf\">STR</span><span class=\"p\">(</span><span class=\"nv\">?task</span><span class=\"p\">))</span> <span class=\"o\">=</span> <span class=\"s\">&quot;token-classification&quot;</span><span class=\"p\">)</span>  \n",
       "<span class=\"p\">}</span>\n",
       "<span class=\"k\">GROUP BY</span> <span class=\"nv\">?task</span> <span class=\"nv\">?metricType</span>\n",
       "<span class=\"k\">ORDER BY</span> <span class=\"k\">DESC</span><span class=\"p\">(</span><span class=\"nv\">?metricCount</span><span class=\"p\">)</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>metricType</th>\n",
       "      <th>metricCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>7793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>f1</td>\n",
       "      <td>1319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>recall</td>\n",
       "      <td>1212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>precision</td>\n",
       "      <td>1198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>f1_macro</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>precision_entity_span</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>recall_macro</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>precision_macro</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>recall_entity_span</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>f1_entity_span</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>loss</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>f_score</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>F1 (Seqeval)</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>F1@M</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>seqeval</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>F1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>Jaccard Error Rate</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>Recall</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>cher</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>cver</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>cer</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>Precision</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>auc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>token-classification</td>\n",
       "      <td>wer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_query(g, \"queries/results/q6.rq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a dataset and a task, which model achieves the highest accuracy, f1, recall, and precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">SELECT</span> <span class=\"nv\">?name</span> <span class=\"nv\">?dataset</span> <span class=\"nv\">?metricValue</span> <span class=\"nv\">?metricType</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "  <span class=\"p\">{</span>\n",
       "    <span class=\"k\">SELECT</span> <span class=\"nv\">?model</span> <span class=\"nv\">?name</span> <span class=\"nv\">?dataset</span> <span class=\"nv\">?metricValue</span> <span class=\"nv\">?metricType</span>\n",
       "    <span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "      <span class=\"nv\">?model</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">;</span>\n",
       "             <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">task</span> <span class=\"s\">&quot;text-classification&quot;</span> <span class=\"p\">;</span>\n",
       "             <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">name</span> <span class=\"nv\">?name</span> <span class=\"p\">;</span>\n",
       "             <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">hasEvaluationMetric</span> <span class=\"nv\">?metric</span> <span class=\"p\">.</span>\n",
       "      <span class=\"nv\">?metric</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">datasetName</span> <span class=\"nv\">?dataset</span> <span class=\"p\">;</span>\n",
       "              <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">metricType</span> <span class=\"nv\">?metricType</span> <span class=\"p\">;</span>\n",
       "              <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">metricValue</span> <span class=\"nv\">?metricValue</span> <span class=\"p\">.</span>\n",
       "      <span class=\"k\">FILTER</span><span class=\"p\">(</span><span class=\"nf\">lcase</span><span class=\"p\">(</span><span class=\"nf\">str</span><span class=\"p\">(</span><span class=\"nv\">?dataset</span><span class=\"p\">))</span> <span class=\"o\">=</span> <span class=\"s\">&quot;glue&quot;</span><span class=\"p\">)</span>\n",
       "      <span class=\"k\">FILTER</span><span class=\"p\">(</span><span class=\"nf\">regex</span><span class=\"p\">(</span><span class=\"nf\">lcase</span><span class=\"p\">(</span><span class=\"nf\">str</span><span class=\"p\">(</span><span class=\"nv\">?metricType</span><span class=\"p\">)),</span> <span class=\"s\">&quot;^(accuracy|acc)$&quot;</span><span class=\"p\">))</span>\n",
       "    <span class=\"p\">}</span>\n",
       "    <span class=\"k\">ORDER BY</span> <span class=\"k\">DESC</span><span class=\"p\">(</span><span class=\"nn\">xsd</span><span class=\"p\">:</span><span class=\"nt\">float</span><span class=\"p\">(</span><span class=\"nv\">?metricValue</span><span class=\"p\">))</span>\n",
       "    <span class=\"k\">LIMIT</span> <span class=\"mi\">1</span>\n",
       "  <span class=\"p\">}</span>\n",
       "  <span class=\"k\">UNION</span>\n",
       "  <span class=\"p\">{</span>\n",
       "    <span class=\"k\">SELECT</span> <span class=\"nv\">?model</span> <span class=\"nv\">?name</span> <span class=\"nv\">?dataset</span> <span class=\"nv\">?metricValue</span> <span class=\"nv\">?metricType</span>\n",
       "    <span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "      <span class=\"nv\">?model</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">;</span>\n",
       "             <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">task</span> <span class=\"s\">&quot;text-classification&quot;</span> <span class=\"p\">;</span>\n",
       "             <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">name</span> <span class=\"nv\">?name</span> <span class=\"p\">;</span>\n",
       "             <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">hasEvaluationMetric</span> <span class=\"nv\">?metric</span> <span class=\"p\">.</span>\n",
       "      <span class=\"nv\">?metric</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">datasetName</span> <span class=\"nv\">?dataset</span> <span class=\"p\">;</span>\n",
       "              <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">metricType</span> <span class=\"nv\">?metricType</span> <span class=\"p\">;</span>\n",
       "              <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">metricValue</span> <span class=\"nv\">?metricValue</span> <span class=\"p\">.</span>\n",
       "      <span class=\"k\">FILTER</span><span class=\"p\">(</span><span class=\"nf\">lcase</span><span class=\"p\">(</span><span class=\"nf\">str</span><span class=\"p\">(</span><span class=\"nv\">?dataset</span><span class=\"p\">))</span> <span class=\"o\">=</span> <span class=\"s\">&quot;glue&quot;</span><span class=\"p\">)</span>\n",
       "      <span class=\"k\">FILTER</span><span class=\"p\">(</span><span class=\"nf\">regex</span><span class=\"p\">(</span><span class=\"nf\">lcase</span><span class=\"p\">(</span><span class=\"nf\">str</span><span class=\"p\">(</span><span class=\"nv\">?metricType</span><span class=\"p\">)),</span> <span class=\"s\">&quot;^f1$&quot;</span><span class=\"p\">))</span>\n",
       "    <span class=\"p\">}</span>\n",
       "    <span class=\"k\">ORDER BY</span> <span class=\"k\">DESC</span><span class=\"p\">(</span><span class=\"nn\">xsd</span><span class=\"p\">:</span><span class=\"nt\">float</span><span class=\"p\">(</span><span class=\"nv\">?metricValue</span><span class=\"p\">))</span>\n",
       "    <span class=\"k\">LIMIT</span> <span class=\"mi\">1</span>\n",
       "  <span class=\"p\">}</span>\n",
       "  <span class=\"k\">UNION</span>\n",
       "  <span class=\"p\">{</span>\n",
       "    <span class=\"k\">SELECT</span> <span class=\"nv\">?model</span> <span class=\"nv\">?name</span> <span class=\"nv\">?dataset</span> <span class=\"nv\">?metricValue</span> <span class=\"nv\">?metricType</span>\n",
       "    <span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "      <span class=\"nv\">?model</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">;</span>\n",
       "             <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">task</span> <span class=\"s\">&quot;text-classification&quot;</span> <span class=\"p\">;</span>\n",
       "             <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">name</span> <span class=\"nv\">?name</span> <span class=\"p\">;</span>\n",
       "             <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">hasEvaluationMetric</span> <span class=\"nv\">?metric</span> <span class=\"p\">.</span>\n",
       "      <span class=\"nv\">?metric</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">datasetName</span> <span class=\"nv\">?dataset</span> <span class=\"p\">;</span>\n",
       "              <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">metricType</span> <span class=\"nv\">?metricType</span> <span class=\"p\">;</span>\n",
       "              <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">metricValue</span> <span class=\"nv\">?metricValue</span> <span class=\"p\">.</span>\n",
       "      <span class=\"k\">FILTER</span><span class=\"p\">(</span><span class=\"nf\">lcase</span><span class=\"p\">(</span><span class=\"nf\">str</span><span class=\"p\">(</span><span class=\"nv\">?dataset</span><span class=\"p\">))</span> <span class=\"o\">=</span> <span class=\"s\">&quot;glue&quot;</span><span class=\"p\">)</span>\n",
       "      <span class=\"k\">FILTER</span><span class=\"p\">(</span><span class=\"nf\">regex</span><span class=\"p\">(</span><span class=\"nf\">lcase</span><span class=\"p\">(</span><span class=\"nf\">str</span><span class=\"p\">(</span><span class=\"nv\">?metricType</span><span class=\"p\">)),</span> <span class=\"s\">&quot;^recall$&quot;</span><span class=\"p\">))</span>\n",
       "    <span class=\"p\">}</span>\n",
       "    <span class=\"k\">ORDER BY</span> <span class=\"k\">DESC</span><span class=\"p\">(</span><span class=\"nn\">xsd</span><span class=\"p\">:</span><span class=\"nt\">float</span><span class=\"p\">(</span><span class=\"nv\">?metricValue</span><span class=\"p\">))</span>\n",
       "    <span class=\"k\">LIMIT</span> <span class=\"mi\">1</span>\n",
       "  <span class=\"p\">}</span>\n",
       "  <span class=\"k\">UNION</span>\n",
       "  <span class=\"p\">{</span>\n",
       "    <span class=\"k\">SELECT</span> <span class=\"nv\">?model</span> <span class=\"nv\">?name</span> <span class=\"nv\">?dataset</span> <span class=\"nv\">?metricValue</span> <span class=\"nv\">?metricType</span>\n",
       "    <span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "      <span class=\"nv\">?model</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">;</span>\n",
       "             <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">task</span> <span class=\"s\">&quot;text-classification&quot;</span> <span class=\"p\">;</span>\n",
       "             <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">name</span> <span class=\"nv\">?name</span> <span class=\"p\">;</span>\n",
       "             <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">hasEvaluationMetric</span> <span class=\"nv\">?metric</span> <span class=\"p\">.</span>\n",
       "      <span class=\"nv\">?metric</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">datasetName</span> <span class=\"nv\">?dataset</span> <span class=\"p\">;</span>\n",
       "              <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">metricType</span> <span class=\"nv\">?metricType</span> <span class=\"p\">;</span>\n",
       "              <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">metricValue</span> <span class=\"nv\">?metricValue</span> <span class=\"p\">.</span>\n",
       "      <span class=\"k\">FILTER</span><span class=\"p\">(</span><span class=\"nf\">lcase</span><span class=\"p\">(</span><span class=\"nf\">str</span><span class=\"p\">(</span><span class=\"nv\">?dataset</span><span class=\"p\">))</span> <span class=\"o\">=</span> <span class=\"s\">&quot;glue&quot;</span><span class=\"p\">)</span>\n",
       "      <span class=\"k\">FILTER</span><span class=\"p\">(</span><span class=\"nf\">regex</span><span class=\"p\">(</span><span class=\"nf\">lcase</span><span class=\"p\">(</span><span class=\"nf\">str</span><span class=\"p\">(</span><span class=\"nv\">?metricType</span><span class=\"p\">)),</span> <span class=\"s\">&quot;^precision$&quot;</span><span class=\"p\">))</span>\n",
       "    <span class=\"p\">}</span>\n",
       "    <span class=\"k\">ORDER BY</span> <span class=\"k\">DESC</span><span class=\"p\">(</span><span class=\"nn\">xsd</span><span class=\"p\">:</span><span class=\"nt\">float</span><span class=\"p\">(</span><span class=\"nv\">?metricValue</span><span class=\"p\">))</span>\n",
       "    <span class=\"k\">LIMIT</span> <span class=\"mi\">1</span>\n",
       "  <span class=\"p\">}</span>\n",
       "<span class=\"p\">}</span>\n",
       "<span class=\"k\">ORDER BY</span> <span class=\"nv\">?metricType</span> <span class=\"k\">DESC</span><span class=\"p\">(</span><span class=\"nn\">xsd</span><span class=\"p\">:</span><span class=\"nt\">float</span><span class=\"p\">(</span><span class=\"nv\">?metricValue</span><span class=\"p\">))</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>dataset</th>\n",
       "      <th>metricValue</th>\n",
       "      <th>metricType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bert-base-uncased-sst2-epochs-2-lr-0.0001</td>\n",
       "      <td>glue</td>\n",
       "      <td>0.99</td>\n",
       "      <td>accuracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deberta-v3-small-finetuned-sst2</td>\n",
       "      <td>glue</td>\n",
       "      <td>0.94170403</td>\n",
       "      <td>f1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deberta-v3-small-finetuned-sst2</td>\n",
       "      <td>glue</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>precision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bert-base-uncased-mrpc</td>\n",
       "      <td>glue</td>\n",
       "      <td>0.9641577</td>\n",
       "      <td>recall</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_query(g, \"queries/results/q7.rq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve models with the hyperparameters used at training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">SELECT</span> <span class=\"nv\">?modelName</span> <span class=\"nv\">?hpName</span> <span class=\"nv\">?hpValue</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "  <span class=\"nv\">?model</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">name</span> <span class=\"nv\">?modelName</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">hasHyperParameter</span> <span class=\"nv\">?hp</span> <span class=\"p\">.</span>\n",
       "  <span class=\"nv\">?hp</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">parameterName</span> <span class=\"nv\">?hpName</span> <span class=\"p\">;</span>\n",
       "      <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">parameterValue</span> <span class=\"nv\">?hpValue</span> <span class=\"p\">.</span>\n",
       "<span class=\"p\">}</span>\n",
       "<span class=\"k\">ORDER BY</span> <span class=\"nv\">?modelName</span> <span class=\"nv\">?hpName</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelName</th>\n",
       "      <th>hpName</th>\n",
       "      <th>hpValue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DeBERTa-v3-base-mnli-fever-anli</td>\n",
       "      <td>fp16</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DeBERTa-v3-base-mnli-fever-anli</td>\n",
       "      <td>learning_rate</td>\n",
       "      <td>2e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeBERTa-v3-base-mnli-fever-anli</td>\n",
       "      <td>num_train_epochs</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeBERTa-v3-base-mnli-fever-anli</td>\n",
       "      <td>per_device_eval_batch_size</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DeBERTa-v3-base-mnli-fever-anli</td>\n",
       "      <td>per_device_train_batch_size</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DeBERTa-v3-base-mnli-fever-anli</td>\n",
       "      <td>warmup_ratio</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DeBERTa-v3-base-mnli-fever-anli</td>\n",
       "      <td>weight_decay</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>amd-partial-v1</td>\n",
       "      <td>batch_size</td>\n",
       "      <td>[64, 64]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>amd-partial-v1</td>\n",
       "      <td>body_learning_rate</td>\n",
       "      <td>[2e-05, 1e-05]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>amd-partial-v1</td>\n",
       "      <td>distance_metric</td>\n",
       "      <td>cosine_distance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>amd-partial-v1</td>\n",
       "      <td>end_to_end</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>amd-partial-v1</td>\n",
       "      <td>eval_max_steps</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>amd-partial-v1</td>\n",
       "      <td>head_learning_rate</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>amd-partial-v1</td>\n",
       "      <td>load_best_model_at_end</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>amd-partial-v1</td>\n",
       "      <td>loss</td>\n",
       "      <td>CosineSimilarityLoss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>amd-partial-v1</td>\n",
       "      <td>margin</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>amd-partial-v1</td>\n",
       "      <td>max_steps</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>amd-partial-v1</td>\n",
       "      <td>num_epochs</td>\n",
       "      <td>[3, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>amd-partial-v1</td>\n",
       "      <td>num_iterations</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>amd-partial-v1</td>\n",
       "      <td>sampling_strategy</td>\n",
       "      <td>oversampling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>amd-partial-v1</td>\n",
       "      <td>seed</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>amd-partial-v1</td>\n",
       "      <td>use_amp</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>amd-partial-v1</td>\n",
       "      <td>warmup_proportion</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bert-base-cased-finetuned-qnli</td>\n",
       "      <td>eval_batch_size</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bert-base-cased-finetuned-qnli</td>\n",
       "      <td>learning_rate</td>\n",
       "      <td>2e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bert-base-cased-finetuned-qnli</td>\n",
       "      <td>lr_scheduler_type</td>\n",
       "      <td>linear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>bert-base-cased-finetuned-qnli</td>\n",
       "      <td>num_epochs</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bert-base-cased-finetuned-qnli</td>\n",
       "      <td>optimizer</td>\n",
       "      <td>Adam with betas=(0.9,0.999) and epsilon=1e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>bert-base-cased-finetuned-qnli</td>\n",
       "      <td>seed</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>bert-base-cased-finetuned-qnli</td>\n",
       "      <td>train_batch_size</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>bert-base-uncased-mrpc</td>\n",
       "      <td>eval_batch_size</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bert-base-uncased-mrpc</td>\n",
       "      <td>learning_rate</td>\n",
       "      <td>2e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>bert-base-uncased-mrpc</td>\n",
       "      <td>lr_scheduler_type</td>\n",
       "      <td>linear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>bert-base-uncased-mrpc</td>\n",
       "      <td>num_epochs</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>bert-base-uncased-mrpc</td>\n",
       "      <td>optimizer</td>\n",
       "      <td>Adam with betas=(0.9,0.999) and epsilon=1e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>bert-base-uncased-mrpc</td>\n",
       "      <td>seed</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>bert-base-uncased-mrpc</td>\n",
       "      <td>train_batch_size</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>distilbert-base-uncased-emotion</td>\n",
       "      <td>batch_size</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>distilbert-base-uncased-emotion</td>\n",
       "      <td>learning_rate</td>\n",
       "      <td>2e-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>distilbert-base-uncased-emotion</td>\n",
       "      <td>num_train_epochs</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>food_embeddings</td>\n",
       "      <td>multi_dataset_batch_sampler</td>\n",
       "      <td>round_robin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>food_embeddings</td>\n",
       "      <td>num_train_epochs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>food_embeddings</td>\n",
       "      <td>per_device_eval_batch_size</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>food_embeddings</td>\n",
       "      <td>per_device_train_batch_size</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>led-large-book-summary</td>\n",
       "      <td>early_stopping</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>led-large-book-summary</td>\n",
       "      <td>encoder_no_repeat_ngram_size</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>led-large-book-summary</td>\n",
       "      <td>max_length</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>led-large-book-summary</td>\n",
       "      <td>min_length</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>led-large-book-summary</td>\n",
       "      <td>no_repeat_ngram_size</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>led-large-book-summary</td>\n",
       "      <td>num_beams</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>led-large-book-summary</td>\n",
       "      <td>repetition_penalty</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>mDeBERTa-v3-base-xnli-multilingual-nli-2mil7</td>\n",
       "      <td>fp16</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mDeBERTa-v3-base-xnli-multilingual-nli-2mil7</td>\n",
       "      <td>gradient_accumulation_steps</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>mDeBERTa-v3-base-xnli-multilingual-nli-2mil7</td>\n",
       "      <td>learning_rate</td>\n",
       "      <td>2e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>mDeBERTa-v3-base-xnli-multilingual-nli-2mil7</td>\n",
       "      <td>num_train_epochs</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>mDeBERTa-v3-base-xnli-multilingual-nli-2mil7</td>\n",
       "      <td>per_device_train_batch_size</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mDeBERTa-v3-base-xnli-multilingual-nli-2mil7</td>\n",
       "      <td>warmup_ratio</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>mDeBERTa-v3-base-xnli-multilingual-nli-2mil7</td>\n",
       "      <td>weight_decay</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>roberta-base-emotion</td>\n",
       "      <td>batch_size</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>roberta-base-emotion</td>\n",
       "      <td>learning_rate</td>\n",
       "      <td>2e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>roberta-base-emotion</td>\n",
       "      <td>num_train_epochs</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>whisper-small-Cantonese</td>\n",
       "      <td>augmentation</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>whisper-small-Cantonese</td>\n",
       "      <td>eval_batch_size</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>whisper-small-Cantonese</td>\n",
       "      <td>gradient_accumulation_steps</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>whisper-small-Cantonese</td>\n",
       "      <td>learning_rate</td>\n",
       "      <td>5e-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>whisper-small-Cantonese</td>\n",
       "      <td>lr_scheduler_type</td>\n",
       "      <td>linear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>whisper-small-Cantonese</td>\n",
       "      <td>lr_scheduler_warmup_steps</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>whisper-small-Cantonese</td>\n",
       "      <td>optimizer</td>\n",
       "      <td>Adam with betas=(0.9,0.999) and epsilon=1e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>whisper-small-Cantonese</td>\n",
       "      <td>total_train_batch_size</td>\n",
       "      <td>25x4=100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>whisper-small-Cantonese</td>\n",
       "      <td>train_batch_size</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>whisper-small-Cantonese</td>\n",
       "      <td>training_steps</td>\n",
       "      <td>15000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_query(g, \"queries/results/q9.rq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve models along with papers, abstract and disadvantages and advantages extract from the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">SELECT</span> <span class=\"nv\">?modelName</span> <span class=\"nv\">?paperID</span> <span class=\"nv\">?paperTitle</span> <span class=\"nv\">?paperSummary</span> <span class=\"nv\">?advantages</span> <span class=\"nv\">?limitations</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "  <span class=\"nv\">?model</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">name</span> <span class=\"nv\">?modelName</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">advantages</span> <span class=\"nv\">?advantages</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">limitations</span> <span class=\"nv\">?limitations</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">hasPaper</span> <span class=\"nv\">?paper</span> <span class=\"p\">.</span>\n",
       "  <span class=\"nv\">?paper</span> <span class=\"nn\">bibo</span><span class=\"p\">:</span><span class=\"nt\">title</span> <span class=\"nv\">?paperTitle</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">bibo</span><span class=\"p\">:</span><span class=\"nt\">abstract</span> <span class=\"nv\">?paperSummary</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">bibo</span><span class=\"p\">:</span><span class=\"nt\">identifier</span> <span class=\"nv\">?paperID</span>\n",
       "<span class=\"p\">}</span>\n",
       "<span class=\"k\">ORDER BY</span> <span class=\"nv\">?modelName</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelName</th>\n",
       "      <th>paperID</th>\n",
       "      <th>paperTitle</th>\n",
       "      <th>paperSummary</th>\n",
       "      <th>advantages</th>\n",
       "      <th>limitations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>financial-summarization-pegasus</td>\n",
       "      <td>1912.08777</td>\n",
       "      <td>PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive\\n  Summarization</td>\n",
       "      <td>Recent work pre-training Transformers with self-supervised objectives on\\nlarge text corpora has shown great success when fine-tuned on downstream NLP\\ntasks including text summarization. However, pre-training objectives tailored\\nfor abstractive text summarization have not been explored. Furthermore there is\\na lack of systematic evaluation across diverse domains. In this work, we\\npropose pre-training large Transformer-based encoder-decoder models on massive\\ntext corpora with a new self-supervised objective. In PEGASUS, important\\nsentences are removed/masked from an input document and are generated together\\nas one output sequence from the remaining sentences, similar to an extractive\\nsummary. We evaluated our best PEGASUS model on 12 downstream summarization\\ntasks spanning news, science, stories, instructions, emails, patents, and\\nlegislative bills. Experiments demonstrate it achieves state-of-the-art\\nperformance on all 12 downstream datasets measured by ROUGE scores. Our model\\nalso shows surprising performance on low-resource summarization, surpassing\\nprevious state-of-the-art results on 6 datasets with only 1000 examples.\\nFinally we validated our results using human evaluation and show that our model\\nsummaries achieve human performance on multiple datasets.</td>\n",
       "      <td>Achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Surpasses previous state-of-the-art results on 6 datasets with only 1000 examples. Summaries achieve human performance on multiple datasets.</td>\n",
       "      <td>There has been little work on systematic evaluation of models across diverse domains.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jina-embeddings-v3</td>\n",
       "      <td>2409.10173</td>\n",
       "      <td>jina-embeddings-v3: Multilingual Embeddings With Task LoRA</td>\n",
       "      <td>We introduce jina-embeddings-v3, a novel text embedding model with 570\\nmillion parameters, achieves state-of-the-art performance on multilingual data\\nand long-context retrieval tasks, supporting context lengths of up to 8192\\ntokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)\\nadapters to generate high-quality embeddings for query-document retrieval,\\nclustering, classification, and text matching. Additionally, Matryoshka\\nRepresentation Learning is integrated into the training process, allowing\\nflexible truncation of embedding dimensions without compromising performance.\\nEvaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the\\nlatest proprietary embeddings from OpenAI and Cohere on English tasks, while\\nachieving superior performance compared to multilingual-e5-large-instruct\\nacross all multilingual tasks.</td>\n",
       "      <td>Achieves state-of-the-art performance on multilingual data and long-context retrieval tasks, supporting context lengths of up to 8192 tokens. Includes task-specific Low-Rank Adaptation (LoRA) adapters for high-quality embeddings. Integrates Matryoshka Representation Learning for flexible truncation of embedding dimensions without compromising performance. Outperforms latest proprietary embeddings from OpenAI and Cohere on English tasks and multilingual-e5-large-instruct on all multilingual tasks. More cost-efficient compared to LLM-based embeddings like e5-mistral-7b-instruct.</td>\n",
       "      <td>Traditional embedding models often require fine-tuning for specific tasks and struggle with common failure cases. Large language models (LLMs) as the backbone for general-purpose embedding generation pose challenges in real-world applications and offer marginal improvements compared to encoder-only embedding models.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kotoba-whisper-v2.0</td>\n",
       "      <td>2212.04356</td>\n",
       "      <td>Robust Speech Recognition via Large-Scale Weak Supervision</td>\n",
       "      <td>We study the capabilities of speech processing systems trained simply to\\npredict large amounts of transcripts of audio on the internet. When scaled to\\n680,000 hours of multilingual and multitask supervision, the resulting models\\ngeneralize well to standard benchmarks and are often competitive with prior\\nfully supervised results but in a zero-shot transfer setting without the need\\nfor any fine-tuning. When compared to humans, the models approach their\\naccuracy and robustness. We are releasing models and inference code to serve as\\na foundation for further work on robust speech processing.</td>\n",
       "      <td>Models trained at 680,000 hours scale transfer well to existing datasets zero-shot, removing the need for any dataset-specific fine-tuning to achieve high-quality results.</td>\n",
       "      <td>The lack of an equivalently high-quality pre-trained decoder, combined with a recommended protocol of dataset-specific fine-tuning, limits the usefulness and robustness of the model.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ldm3d-4c</td>\n",
       "      <td>2305.10853</td>\n",
       "      <td>LDM3D: Latent Diffusion Model for 3D</td>\n",
       "      <td>This research paper proposes a Latent Diffusion Model for 3D (LDM3D) that\\ngenerates both image and depth map data from a given text prompt, allowing\\nusers to generate RGBD images from text prompts. The LDM3D model is fine-tuned\\non a dataset of tuples containing an RGB image, depth map and caption, and\\nvalidated through extensive experiments. We also develop an application called\\nDepthFusion, which uses the generated RGB images and depth maps to create\\nimmersive and interactive 360-degree-view experiences using TouchDesigner. This\\ntechnology has the potential to transform a wide range of industries, from\\nentertainment and gaming to architecture and design. Overall, this paper\\npresents a significant contribution to the field of generative AI and computer\\nvision, and showcases the potential of LDM3D and DepthFusion to revolutionize\\ncontent creation and digital experiences. A short video summarizing the\\napproach can be found at https://t.ly/tdi2.</td>\n",
       "      <td>Provides finer-grained and more globally coherent predictions</td>\n",
       "      <td>None mentioned in the provided text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ldm3d-4c</td>\n",
       "      <td>2112.10752</td>\n",
       "      <td>High-Resolution Image Synthesis with Latent Diffusion Models</td>\n",
       "      <td>By decomposing the image formation process into a sequential application of\\ndenoising autoencoders, diffusion models (DMs) achieve state-of-the-art\\nsynthesis results on image data and beyond. Additionally, their formulation\\nallows for a guiding mechanism to control the image generation process without\\nretraining. However, since these models typically operate directly in pixel\\nspace, optimization of powerful DMs often consumes hundreds of GPU days and\\ninference is expensive due to sequential evaluations. To enable DM training on\\nlimited computational resources while retaining their quality and flexibility,\\nwe apply them in the latent space of powerful pretrained autoencoders. In\\ncontrast to previous work, training diffusion models on such a representation\\nallows for the first time to reach a near-optimal point between complexity\\nreduction and detail preservation, greatly boosting visual fidelity. By\\nintroducing cross-attention layers into the model architecture, we turn\\ndiffusion models into powerful and flexible generators for general conditioning\\ninputs such as text or bounding boxes and high-resolution synthesis becomes\\npossible in a convolutional manner. Our latent diffusion models (LDMs) achieve\\na new state of the art for image inpainting and highly competitive performance\\non various tasks, including unconditional image generation, semantic scene\\nsynthesis, and super-resolution, while significantly reducing computational\\nrequirements compared to pixel-based DMs. Code is available at\\nhttps://github.com/CompVis/latent-diffusion .</td>\n",
       "      <td>Provides finer-grained and more globally coherent predictions</td>\n",
       "      <td>None mentioned in the provided text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ldm3d-4c</td>\n",
       "      <td>2103.13413</td>\n",
       "      <td>Vision Transformers for Dense Prediction</td>\n",
       "      <td>We introduce dense vision transformers, an architecture that leverages vision\\ntransformers in place of convolutional networks as a backbone for dense\\nprediction tasks. We assemble tokens from various stages of the vision\\ntransformer into image-like representations at various resolutions and\\nprogressively combine them into full-resolution predictions using a\\nconvolutional decoder. The transformer backbone processes representations at a\\nconstant and relatively high resolution and has a global receptive field at\\nevery stage. These properties allow the dense vision transformer to provide\\nfiner-grained and more globally coherent predictions when compared to\\nfully-convolutional networks. Our experiments show that this architecture\\nyields substantial improvements on dense prediction tasks, especially when a\\nlarge amount of training data is available. For monocular depth estimation, we\\nobserve an improvement of up to 28% in relative performance when compared to a\\nstate-of-the-art fully-convolutional network. When applied to semantic\\nsegmentation, dense vision transformers set a new state of the art on ADE20K\\nwith 49.02% mIoU. We further show that the architecture can be fine-tuned on\\nsmaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets\\nthe new state of the art. Our models are available at\\nhttps://github.com/intel-isl/DPT.</td>\n",
       "      <td>Provides finer-grained and more globally coherent predictions</td>\n",
       "      <td>None mentioned in the provided text</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_query(g, \"queries/results/q10.rq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01//EN\"\n",
       "   \"http://www.w3.org/TR/html4/strict.dtd\">\n",
       "<!--\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "-->\n",
       "<html>\n",
       "<head>\n",
       "  <title></title>\n",
       "  <meta http-equiv=\"content-type\" content=\"text/html; charset=None\">\n",
       "  <style type=\"text/css\">\n",
       "/*\n",
       "generated by Pygments <https://pygments.org/>\n",
       "Copyright 2006-2025 by the Pygments team.\n",
       "Licensed under the BSD license, see LICENSE for details.\n",
       "*/\n",
       "pre { line-height: 125%; }\n",
       "td.linenos .normal { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: #586e75; background-color: #073642; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "body .hll { background-color: #073642 }\n",
       "body .c { color: #586E75; font-style: italic } /* Comment */\n",
       "body .err { color: #839496; background-color: #DC322F } /* Error */\n",
       "body .esc { color: #839496 } /* Escape */\n",
       "body .g { color: #839496 } /* Generic */\n",
       "body .k { color: #859900 } /* Keyword */\n",
       "body .l { color: #839496 } /* Literal */\n",
       "body .n { color: #839496 } /* Name */\n",
       "body .o { color: #586E75 } /* Operator */\n",
       "body .x { color: #839496 } /* Other */\n",
       "body .p { color: #839496 } /* Punctuation */\n",
       "body .ch { color: #586E75; font-style: italic } /* Comment.Hashbang */\n",
       "body .cm { color: #586E75; font-style: italic } /* Comment.Multiline */\n",
       "body .cp { color: #D33682 } /* Comment.Preproc */\n",
       "body .cpf { color: #586E75 } /* Comment.PreprocFile */\n",
       "body .c1 { color: #586E75; font-style: italic } /* Comment.Single */\n",
       "body .cs { color: #586E75; font-style: italic } /* Comment.Special */\n",
       "body .gd { color: #DC322F } /* Generic.Deleted */\n",
       "body .ge { color: #839496; font-style: italic } /* Generic.Emph */\n",
       "body .ges { color: #839496; font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       "body .gr { color: #DC322F } /* Generic.Error */\n",
       "body .gh { color: #839496; font-weight: bold } /* Generic.Heading */\n",
       "body .gi { color: #859900 } /* Generic.Inserted */\n",
       "body .go { color: #839496 } /* Generic.Output */\n",
       "body .gp { color: #268BD2; font-weight: bold } /* Generic.Prompt */\n",
       "body .gs { color: #839496; font-weight: bold } /* Generic.Strong */\n",
       "body .gu { color: #839496; text-decoration: underline } /* Generic.Subheading */\n",
       "body .gt { color: #268BD2 } /* Generic.Traceback */\n",
       "body .kc { color: #2AA198 } /* Keyword.Constant */\n",
       "body .kd { color: #2AA198 } /* Keyword.Declaration */\n",
       "body .kn { color: #CB4B16 } /* Keyword.Namespace */\n",
       "body .kp { color: #859900 } /* Keyword.Pseudo */\n",
       "body .kr { color: #859900 } /* Keyword.Reserved */\n",
       "body .kt { color: #B58900 } /* Keyword.Type */\n",
       "body .ld { color: #839496 } /* Literal.Date */\n",
       "body .m { color: #2AA198 } /* Literal.Number */\n",
       "body .s { color: #2AA198 } /* Literal.String */\n",
       "body .na { color: #839496 } /* Name.Attribute */\n",
       "body .nb { color: #268BD2 } /* Name.Builtin */\n",
       "body .nc { color: #268BD2 } /* Name.Class */\n",
       "body .no { color: #268BD2 } /* Name.Constant */\n",
       "body .nd { color: #268BD2 } /* Name.Decorator */\n",
       "body .ni { color: #268BD2 } /* Name.Entity */\n",
       "body .ne { color: #268BD2 } /* Name.Exception */\n",
       "body .nf { color: #268BD2 } /* Name.Function */\n",
       "body .nl { color: #268BD2 } /* Name.Label */\n",
       "body .nn { color: #268BD2 } /* Name.Namespace */\n",
       "body .nx { color: #839496 } /* Name.Other */\n",
       "body .py { color: #839496 } /* Name.Property */\n",
       "body .nt { color: #268BD2 } /* Name.Tag */\n",
       "body .nv { color: #268BD2 } /* Name.Variable */\n",
       "body .ow { color: #859900 } /* Operator.Word */\n",
       "body .pm { color: #839496 } /* Punctuation.Marker */\n",
       "body .w { color: #839496 } /* Text.Whitespace */\n",
       "body .mb { color: #2AA198 } /* Literal.Number.Bin */\n",
       "body .mf { color: #2AA198 } /* Literal.Number.Float */\n",
       "body .mh { color: #2AA198 } /* Literal.Number.Hex */\n",
       "body .mi { color: #2AA198 } /* Literal.Number.Integer */\n",
       "body .mo { color: #2AA198 } /* Literal.Number.Oct */\n",
       "body .sa { color: #2AA198 } /* Literal.String.Affix */\n",
       "body .sb { color: #2AA198 } /* Literal.String.Backtick */\n",
       "body .sc { color: #2AA198 } /* Literal.String.Char */\n",
       "body .dl { color: #2AA198 } /* Literal.String.Delimiter */\n",
       "body .sd { color: #586E75 } /* Literal.String.Doc */\n",
       "body .s2 { color: #2AA198 } /* Literal.String.Double */\n",
       "body .se { color: #2AA198 } /* Literal.String.Escape */\n",
       "body .sh { color: #2AA198 } /* Literal.String.Heredoc */\n",
       "body .si { color: #2AA198 } /* Literal.String.Interpol */\n",
       "body .sx { color: #2AA198 } /* Literal.String.Other */\n",
       "body .sr { color: #CB4B16 } /* Literal.String.Regex */\n",
       "body .s1 { color: #2AA198 } /* Literal.String.Single */\n",
       "body .ss { color: #2AA198 } /* Literal.String.Symbol */\n",
       "body .bp { color: #268BD2 } /* Name.Builtin.Pseudo */\n",
       "body .fm { color: #268BD2 } /* Name.Function.Magic */\n",
       "body .vc { color: #268BD2 } /* Name.Variable.Class */\n",
       "body .vg { color: #268BD2 } /* Name.Variable.Global */\n",
       "body .vi { color: #268BD2 } /* Name.Variable.Instance */\n",
       "body .vm { color: #268BD2 } /* Name.Variable.Magic */\n",
       "body .il { color: #2AA198 } /* Literal.Number.Integer.Long */\n",
       "\n",
       "  </style>\n",
       "</head>\n",
       "<body>\n",
       "<h2></h2>\n",
       "\n",
       "<div class=\"highlight\"><pre><span></span><span class=\"k\">SELECT</span> <span class=\"nv\">?paperTitle</span> <span class=\"nv\">?paperAbstract</span> <span class=\"p\">(</span><span class=\"nf\">GROUP_CONCAT</span><span class=\"p\">(</span><span class=\"nv\">?modelName</span><span class=\"p\">;</span> <span class=\"nf\">separator</span><span class=\"o\">=</span><span class=\"s\">&quot;, &quot;</span><span class=\"p\">)</span> <span class=\"k\">AS</span> <span class=\"nv\">?models</span><span class=\"p\">)</span>\n",
       "<span class=\"k\">WHERE</span> <span class=\"p\">{</span>\n",
       "  <span class=\"nv\">?model</span> <span class=\"k\">a</span> <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">Model</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">name</span> <span class=\"nv\">?modelName</span> <span class=\"p\">;</span>\n",
       "         <span class=\"nn\">kg</span><span class=\"p\">:</span><span class=\"nt\">hasPaper</span> <span class=\"nv\">?paper</span> <span class=\"p\">.</span>\n",
       "  <span class=\"k\">OPTIONAL</span> <span class=\"p\">{</span> <span class=\"nv\">?paper</span> <span class=\"nn\">bibo</span><span class=\"p\">:</span><span class=\"nt\">title</span> <span class=\"nv\">?paperTitle</span> <span class=\"p\">.</span> <span class=\"p\">}</span>\n",
       "  <span class=\"k\">OPTIONAL</span> <span class=\"p\">{</span> <span class=\"nv\">?paper</span> <span class=\"nn\">bibo</span><span class=\"p\">:</span><span class=\"nt\">abstract</span> <span class=\"nv\">?paperAbstract</span> <span class=\"p\">.</span> <span class=\"p\">}</span>\n",
       "<span class=\"p\">}</span>\n",
       "<span class=\"k\">GROUP BY</span> <span class=\"nv\">?paper</span> <span class=\"nv\">?paperTitle</span> <span class=\"nv\">?paperAbstract</span>\n",
       "<span class=\"k\">HAVING</span> <span class=\"p\">(</span><span class=\"nf\">COUNT</span><span class=\"p\">(</span><span class=\"nv\">?model</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n",
       "<span class=\"k\">ORDER BY</span> <span class=\"nv\">?paper</span>\n",
       "</pre></div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperTitle</th>\n",
       "      <th>paperAbstract</th>\n",
       "      <th>models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Overcoming catastrophic forgetting in neural networks</td>\n",
       "      <td>The ability to learn tasks in a sequential fashion is crucial to the\\ndevelopment of artificial intelligence. Neural networks are not, in general,\\ncapable of this and it has been widely thought that catastrophic forgetting is\\nan inevitable feature of connectionist models. We show that it is possible to\\novercome this limitation and train networks that can maintain expertise on\\ntasks which they have not experienced for a long time. Our approach remembers\\nold tasks by selectively slowing down learning on the weights important for\\nthose tasks. We demonstrate our approach is scalable and effective by solving a\\nset of classification tasks based on the MNIST hand written digit dataset and\\nby learning several Atari 2600 games sequentially.</td>\n",
       "      <td>stella-large-zh, stella-base-zh-v2, stella-base-zh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Defense of the Triplet Loss for Person Re-Identification</td>\n",
       "      <td>In the past few years, the field of computer vision has gone through a\\nrevolution fueled mainly by the advent of large datasets and the adoption of\\ndeep convolutional neural networks for end-to-end learning. The person\\nre-identification subfield is no exception to this. Unfortunately, a prevailing\\nbelief in the community seems to be that the triplet loss is inferior to using\\nsurrogate losses (classification, verification) followed by a separate metric\\nlearning step. We show that, for models trained from scratch as well as\\npretrained ones, using a variant of the triplet loss to perform end-to-end deep\\nmetric learning outperforms most other published methods by a large margin.</td>\n",
       "      <td>finetuned-snowflake-arctic-embed-m, all-MiniLM-L6-v2-triplet-loss, distilbert-base-uncased-wikipedia-sections-triplet, all_minilm_finetuned_context_phyto, multilingual-e5-small-triplet-final-1, fine_tuned_model, e-small-triplet-balanced, e-small-triplet, triplet_CloseHlabel_farLabel_andnegativ-1M-5eps-XLMR_29may, modernbert-embed-base-biencoder-human-rights, bge-small-matryoshka-fine-tuned, bge-base-en, bge-base-argilla-sdk-matryoshka, cese5020-contrastive-model, sentest, test9, test7, test3, test13, test12, test11, SBertBaseMittanbudver1, Italian-ModernBERT-base-embed-mmarco-triplet, gattina-ha-classifier-cossim-fpt, gattina-ha-classifier-cossim-ffpt, gattina-ha-classifier-cossim, phobert_Tripel, nomic-embed-philosophy-triplets_v9, nomic-embed-philosophy-triplets_v7, nomic-embed-philosophy-triplets_v5, nomic-embed-philosophy-triplets_v3, nomic-embed-philosophy-triplets_v1, discipline-tuned_specter_2_024, discipline-tuned_specter_2_019, discipline-tuned_specter_2_015, discipline-tuned_specter_2_010, discipline-tuned_specter_2_009, discipline-tuned_specter_2_001, discipline-tuned_specter_1_001, discipline-bert-modern-large_v02, discipline-bert-modern-large_01, bge-m3-philosophy-triplets_v3, bge-m3-philosophy-triplets_v1, finetuned-bge-base-en, finetuned-bge-base-v2, finetuned-bge-bai, finetuned-BAAI-bge-base-en, logembed_a1, stag_123_cp8000, stag_123_cp10000, stag_123, model_stage1_latest, model_stage1, distilroberta-base-sentence-transformer-triplets, facet_retriever, philai-embeddings-2.0, pb-small-10e-tsdae6e-philsim-cosine-6e-beatai-cosine-80e, pb-small-10e-tsdae6e-philsim-cosine-6e-beatai-cosine-50e, pb-small-10e-tsdae6e-philsim-cosine-6e-beatai-30e, multilingual-e5-large-triplet_loss, gutenberg_authorship, Velvet-2B-embedding-news, bert-base-multilingual-cased-finetuned-yoruba-IR, amharic-xlmr-finetuned, BAA-finetuned-yoruba-IR, paraphrase-multLing-L12-v2_custom, custom-paraphrase-v2, me5-large-construction-v2, me5-large-construction-cat, me5-large-construction-adapter-v3, me5-large-construction-adapter-v2, me5-large-construction-adapter, me5-large-construction, jina-embeddings-v2-base-code-mbpp, bge-base-mbpp-processed, bge-base-mbpp, FT-triple-2, paraphrase-multilingual-MiniLM-L12-v2-job-cv-multi-dataset, multilingual-e5-large-instruct-embedder-tgd, multilingual-e5-large-instruct-embedder-tg, USER-bge-m3-embedder-td, trained_on_all_data_model_push_00, mini_lm_l6_v2_trained_on_all_data_model_push_00, bert_lang_trained_on_all_data_model_push_00, Sentence-Transformer_1, gte-multilingual-base-v2.1-similarity, retriever-v3-2000, my-retriever-4000, my-retriever-3000, my-retriever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Efficient Natural Language Response Suggestion for Smart Reply</td>\n",
       "      <td>This paper presents a computationally efficient machine-learned method for\\nnatural language response suggestion. Feed-forward neural networks using n-gram\\nembedding features encode messages into vectors which are optimized to give\\nmessage-response pairs a high dot-product value. An optimized search finds\\nresponse suggestions. The method is evaluated in a large-scale commercial\\ne-mail application, Inbox by Gmail. Compared to a sequence-to-sequence\\napproach, the new system achieves the same quality at a small fraction of the\\ncomputational requirements and latency.</td>\n",
       "      <td>vietnamese-sbert-soc, sup-SimCSE-VietNamese-phobert-base-soc, finetuned-snowflake-arctic-embed-m-v1.5, roberta-amharic-embed-medium, roberta-amharic-embed-base-v0, finetuned_MiniLM, intfloat-triplet-v2, au-blog-rag-embedder, specter2_pubmed-v0.6, specter2_pubmed-v0.5, pubmedncl-pubmed-v0.1, modernbert-pubmed-v0.1, cde-small-pubmed-v0.1, bge-m3-retromae-pubmed-v0.1, arctic-pubmed-v0.2, arctic-pubmed-v0.1, cc-uffs-ppc-ft-test-multiqa, cc-uffs-ppc-distiluse-base-multilingual-cased-v1-finetuned, cc-uffs-ppc, new_model_3, fm2, fm1, fm, finetuned_arctic, nomic-v1.5-financial-matryoshka, snowflake-arctic-embed-m-klej-dyk-v0.1, privacy_embedding_rag_10k_base_checkpoint_2-klej-dyk-v0.1, mmlw-roberta-base-klej-dyk-v0.1, gte-base-en-v1.5-klej-dyk-v0.1, bge-base-en-v1.5-klej-dyk-v0.2, bge-base-en-v1.5-klej-dyk, all-MiniLM-L6-v2-klej-dyk-v0.1, asc_embedding, distilroberta-base-nli-v0.2, distilroberta-base-nli-v0.1, distilroberta-base-nli-v0, bert-base-uncased-nli-v0, halong_embedding-legal-document-finetune, test-ModernBERT-base-nq-mnrl, stsb-distilbert-base-quora-duplicate-questions, stsb-distilbert-base-mnrl-cl-multi, stsb-distilbert-base-mnrl, mpnet-base-natural-questions-mnrl, mpnet-base-gooaq-hard-negatives, mpnet-base-gooaq, mpnet-base-all-nli-triplet, distilroberta-base-paraphrases-multi, distilroberta-base-nli-v2, distilroberta-base-nli-matryoshka-v3, distilroberta-base-nli-matryoshka-reduced, distilroberta-base-nli-adaptive-layer, distilroberta-base-nli-2d-matryoshka, bert-base-uncased-gooaq-og, MiniLM-L6-H384-uncased-gooaq-no-asym, MiniLM-L6-H384-uncased-gooaq-asym, bge-base-st-phyto, stella_en_400M_v5-FinanceRAG-v2, stella_en_400M_v5-FinanceRAG-md, stella_en_400M_v5-FinanceRAG, bge-large-repmus-matryoshka, bge-large-repmus-cross_entropy, Fin-ModernBERT-RAG-embed-base, finetuned-all-MiniLM-L6-v2, all-mpnet-base-v2-patabs-1epoc-batch32-100000, fine_tuned_model_3, fine_tuned_model_16, fine_tuned_model_13, fine_tuned_model_10, silma-embeddding-matryoshka-v0.1, all-mpnet-base-v2-sample, finetune-sentence-transformer, finetune, distilroberta-ai-job-embeddings, nomic-embed-financial-matryoshka, all-nli-bert-tiny-dense, gte-base-ko, swahili-paraphrase-multilingual-mpnet-base-v2-nli-matryoshka, bge-base-swahili-matryoshka, worksphere-regulations-embedding_bge, custom-bge, thenlper-gte-base-fine-tuned, sentence-transformers-all-MiniLM-L6-v2-fine-tuned, intfloat-multilingual-e5-small-fine-tuned, bgem3-shakespeare_st_3, BAAI-bge-m3-fine-tuned, BAAI-bge-large-en-v1.5-fine-tuned, bge-base-financial, sentence-transformers-all-mpnet-base-v2, sbert_ft_cross-encoder-nli-deberta-v3-large, ai-policy-ft, bge-finetuned-reranker, bge-finetuned, finetuned-arctic-model-2, finetuned-arctic-model, mxbai-embed-large-v1-financial-rag-matryoshka, mpnet-base-financial-rag-matryoshka, financial-rag-matryoshka, bge-base-financial-nvidia-matryoshka, UAE-Large-V1-financial-rag-matryoshka, bge-small-en-v1.5-RIRAG_ObliQA, roberta-amharic-text-embedding-medium, roberta-amharic-text-embedding-base, bert-amharic-text-embedding-medium, indo-islamic-sentence-bert-v2, indo-islamic-sentence-bert, indobert-base-p2-sts-arxiv-id, LEGAL_EMBEDDING, bl_ademe_large, distilbert-base-multilingual-cased-indicxnli-random-negatives-v1, assamese-bert-nli-v2, bge_model_fine_tuned_law, msmarco-distilbert-base-v4, clphobert-base, bge-base-financial-matryoshka, finbeddings_bert, bge-base-en-sec10k-embed, bge-base-en-honsec10k-embed, bge-base-en-bioembed768, bge-base-en-bioembed, AIE4_midterm_tuned_embeddings_2, AIE4_midterm_tuned_embeddings, bge-m3-trained-2, bge-m3-trained, bge-m3-spa-law-qa-trained-2, bge-m3-spa-law-qa-trained, bge-m3-retrained, test9, test3, test13, test12, test11, SBertBaseMittanbudver1, sentence-distilbert-turkish, Finetuned_Alibaba_Large, arabic-english-sts-matryoshka-v2.0, arabic-english-sts-matryoshka, Arabic-STS-Matryoshka-V2, Arabic-STS-Matryoshka, Arabic-Retrieval-v1.0, artic_ft_midterm, Italian-ModernBERT-base-embed-mmarco-mnrl, bge-small-qs, embedding1, mxbai-embed-large-v1-ft-webinstruct, modernbert-embed-base-ft-finetome, gte-large-ft-webinstruct, distilroberta-base-ft-webinstruct, distilroberta-base-ft-allnli-matryoshka-768-64-1e-256bs, distilroberta-base-ft-allnli-matryoshka-768-16-1e-128bs, mpac-bge-large-v1.2, mpac-bge-large, gte-base-law-matryoshka, multilingual-e5-large-ita, mpnet-base-all-nli-triplet-turkish-v4-dgx, mpnet-base-all-nli-triplet-turkish-v3, mpnet-base-all-nli-triplet-turkish-v2, vn_bi_encoder_MultipleNegativesRankingLoss, vn_bi_encoder_16neg, phobert-finetune-512, phobert-finetune, paraphrase-multilingual-mpnet-base-v2_finetune_med, paraphrase-multilingual-mpnet-base-v2_finetune-512, paraphrase-multilingual-MiniLM-L12-v2_finetune, e5_large_finetune_16neg, e5_large_finetune, demo_bi_encoder, me5-small-preskripsi-embedding-pos-neg, bge-99GPT-v1-test, bge-99GPT-v1, ModernBERT-large-BORA, mpnet-base-nli-v2, roberta-base-ft-all-nli, modernbert-large-ft-all-nli, modernbert-base-ft-all-nli, ft-modern-bert-emb-all-nli, bert-base-uncased-ft-all-nli, vietnamese-bi-encoder-for-SoICT-2024, vietnamese-bi-encoder-fine-tuning-for-law-chatbot, bge-base-custom-matryoshka, bge-small-en-v1.5-ft-orc-0930-dates, bge-small-en-v1.5-ft-orc-0813, bge-small-en-v1.5-ft-orc-0806, policy_gte_large_7, policy_gte_large_5, policy_gte_large_2plus, policy_gte_large_2, policy_gte_large, proba, legal-ft-arctic-l, legal-ft, mpnet-base-all-pittsburgh-squad, bge-m3-spa-law-qa, modernbert-embed-ft-const-legal-matryoshka, modernbert-embed-base-legaltextai-matryoshka-legaldataset, comp-embedding-matching, snowflake-arctic-embed-xs-ms-marco-triplet, fine-tuned-bge-base-raw_pdf-v1, fine-tune-embedding-bge-base-HrPolicy_vfinal, fine-tune-embedding-bge-base-HrPolicy, bge-base-raw_pdf_finetuned_vf1, mxbai-de-abat-matryoshka, mxbai-abat-matryoshka, paraphrase-multilingual-MiniLM-L12-hu_v1, paraphrase-multilingual-MiniLM-L12-hu-v3, paraphrase-multilingual-MiniLM-L12-hu-v2, paraphrase-multilingual-MiniLM-L12-hu, gte-multilingual-base-hu, bge-m3-hu, ModernBERT-base-hu_v3, ModernBERT-base-hu_v2, ModernBERT-base-hu, legal_paraphrase, sentence-t5-base-bioasq-1epoch-batch32-100steps, bge-small-bioasq-3epochs-batch32, bge-small-bioasq-1epochs-batch32, bge-small-bioasq-1epoch-batch32-step50, bge-small-bioasq-1epoch-batch32-100steps, bge-small-bioasq-1epoch-batch32, bge-base-bioasq-matryoshka, all-mpnet-base-v2-bioasq-matryoshka, all-mpnet-base-v2-bioasq-1epoch-batch32-100steps, all-mpnet-base-v2-bioasq-1epoc-batch32-100, fine-tuned-matryoshka-500, fine-tuned-matryoshka-200, fine-tuned-matryoshka-1725, fine-tuned-matryoshka-1500, fine-tuned-matryoshka-1000, fine-tuned-matryoshka-100, fine-tuned-matryoshka, improve_vibi, improve_halong, snowflake-arctic-embed-xs_finetuned_aipolicy, snowflake_finetuned_semantic, snowflake_finetuned_recursive, mpnet_finetuned_semantic, mpnet_finetuned_recursive, kicon_e5large_15_v1, finetuned_arctic-embedd-l, CR-biodiversity-sentence-similarity-es, CR-biodiversity-preprocessed-sentence-similarity-es, klue-roberta-base-klue-sts-mrc, stag_123_cp8000, stag_123_cp10000, stag_123, model_stage2_latest, model_stage2_1436, model_stage2, bkai-2024-retrival-e5-finetune-v2, bge-m3-nvidia-ko-v1, snowflake-arctic-embed-l-v2.0-pits, bge-small-financial-matryoshka, bge-m3-financial-matryoshka, finetuned_paraphrase-multilingual_v3, finetuned_paraphrase-multilingual_v2, finetuned_paraphrase-multilingual_test, finetuned_paraphrase-multilingual_mpnet_try6, finetuned_paraphrase-multilingual_mpnet_try5, finetuned_paraphrase-multilingual_mpnet_try4, finetuned_paraphrase-multilingual_mpnet_try3, finetuned_paraphrase-multilingual_mpnet_try2, finetuned_paraphrase-multilingual_mpnet, finetuned_paraphrase-multilingual, my-awesome-bi-encoder, bge-large-mpnet-base-all-nli-triplet-final-50000, bge-large-mpnet-base-all-nli-triplet-final, bge-base-financial-matryoshka-v1, bge-m3-uz-legal-matryoshka, bge-base-space-mt-tsdae, arctic-embed-m-space-sup, bge-finetuned-insurance-matryoshka, bge-base-insurance-matryoshka, ModernBERT-base-nli-v3, ModernBERT-base-marco, embed-andegpt-H768, embed-andegpt-H384, bge-small-en-v1.5-esg-v2, bge-small-en-v1.5-esg, bge-micro-v2-esg-v2, bge-micro-v2-esg, bge-base-financial-matryoshka-testing, sentence-roberta-small, phi-2-telecom-ft, bge-small-qa-telecom-ft, FT_RAG, all-distilroberta-v1_danish_law_fine_tune, Ko-sroberta-base-multitask, nomic-embed-text-v1, code-prompt-similarity-model, bge-base-for_text2sql, bge-m3-es-legal-tmp-6, bge-m3-es-legal-tmp-5, bge-m3-es-legal-tmp-3, french-document-embedding, negasibert-mnrl, bge-m3-aicacia, modernbert-embed-base-legal-matryoshka-2, bge-finetuned-train, all-MiniLM-L6-v2-finetuned-imdb, indobert-t4, indobert-t3, 3bs4lr2, slinger20241231-3, slinger20241231-2, slinger20241231-1, hi-di-hi, all-mpnet-base-v2-modulepred, jev2-legal, procedure-tool-matching_3_epochs, procedure-tool-matching_10_epochs, DeBERTaV3-small-SentenceTransformer-AdaptiveLayerBaseline, DeBERTaV3-small-SentenceTransformer-AdaptiveLayerAll, DeBERTaV3-small-ST-AdaptiveLayers-ep2, DeBERTaV3-small-ST-AdaptiveLayerAllNormalized, DeBERTaV3-small-ST-AdaptiveLayer-Norm-ep2, DeBERTaV3-small-ST-AdaptiveLayer-3L-ep2, DeBERTaV3-small-GeneralSentenceTransformer, DeBERTa-ST-AllLayers-v3.1bis, DeBERTa-ST-AllLayers-v3.1, bge-m3-finetuned-2, bge-m3-finetuned-1, bge-base-patentmatch, deep-learning-for-embedding-model-ssilwal-qpham6_army_doc, deep-learning-for-embedding-model-ssilwal-qpham6, Finance2_embedding_small_en-V1.5, int-e5-base-5tv5, all-MiniLM-L6-v2-MEDI-MTEB-triplet-randproj-trainable-512-final, all-MiniLM-L6-v2-MEDI-MTEB-triplet-randproj-64-final, all-MiniLM-L6-v2-MEDI-MTEB-triplet-randproj-512-final, all-MiniLM-L6-v2-MEDI-MTEB-triplet-final, German-RAG-ModernBERT-Base-TRIPLES, finetuned_bge_embeddings_v4_base_v1.5, legal-ft-1, technographics-marketing-matryoshka, finetuned-gte-base, retrieval-mpnet-dot-finetuned-llama3-synthetic-dataset, retrieval-mpnet-dot-finetuned-llama3-openbiollm-synthetic-dataset, xlm-roberta-base-msmarco-webfaq, bge-base-matryoshka-aws-casestudies, bge-base-financial-matryoshka-anisha, bge-base-aws-case-studies, bge-base-financial-matryoshka-nvda-iter20, bge-base-financial-matryoshka-nvda, vietnamese-sbert-Financial-Matryoshka-5e-11k, vietnamese-sbert-Financial-Matryoshka-2e-11k, vietnamese-sbert-Financial-Matryoshka-1e-200k, vietnamese-bi-encoder-financial-matryoshka-5, vietnamese-bi-encoder-financial-matryoshka-2, vietnamese-bi-encoder-Matryoshka-2e-9k, vietnamese-bi-encoder-Matryoshka-1e-9k, vietnamese-bi-encoder-Financial-Matryoshka-5e-11k, vietnamese-bi-encoder-Financial-Matryoshka-3e-200k, vietnamese-bi-encoder-Financial-Matryoshka-2e-11k, vietnamese-bi-encoder-Financial-Matryoshka-1e-200k, vietnamese-bi-encoder-Financial-Matryoshka, multilingual-e5-base-Matryoshka-7e-11k, multilingual-e5-base-Matryoshka-5e-11k, multilingual-e5-base-Matryoshka-2e-11k, multilingual-e5-base-Matryoshka-1e-200k, mordernBERT-multilingual-legal-1e, halong_embedding-Financial-Matryoshka-2e-11k, halong_embedding-Financial-Matryoshka-1e-200k, halong-embedding-Financial-Matryoshka-5e-11k, gte-multilingual-legal-1e, gte-multilingual-base-Matryoshka-4e-9k, gte-multilingual-base-Matryoshka-3e-9k, gte-multilingual-base-Matryoshka-2e-9k, gte-multilingual-base-Matryoshka-1e-9k, gte-multilingual-base-Matryoshka-1e-11k, bert-base-multilingual-uncased-Financial-Matryoshka-8e-11k, bert-base-multilingual-uncased-Financial-Matryoshka-5e-11k, bert-base-multilingual-uncased-Financial-Matryoshka-2e-11k, bert-base-multilingual-Financial-Matryoshka-2-v2, bert-base-multilingual-Financial-Matryoshka, ModernBERT-multilingual-legal-2e, ModernBERT-base-test-v2, ModernBERT-base-3e-9k, indic-bert-nli-matryoshka, jina-semantic-bmf-matryoshka-1024-10epochs, jina-semantic-bmf-matryoshka, german-semantic-bmf-matryoshka-512-10epochs, german-semantic-bmf-matryoshka, bge-semantic-bmf-matryoshka, sbert_nli_test, bge-base-financial-matryoshkafinetuning-tcz-webiste, bge-base-financial-matryoshka-finetuning-tcz-1, st-SIT-test, sqv-v3-10ep, sqv-v3, sqv-v2, sqv-5ep, sitgrsBAAIbge-m3-300824v2, sitgrsBAAIbge-m3-290824, sitges2608bai-4ep, sitges2608, sitges10242608-4ep-rerankv4-sp, sitges10242608-4ep-rerankv3-sp, sitges10242608-4ep-rerankv3, sitges10242608-4ep-rerankv2, sitges10242608-4ep-rerank, ST-tramits-sitges-006-5ep, ST-tramits-sitges-005-5ep, ST-tramits-sitges-003-5ep, ST-tramits-sitges-003-10ep, ST-tramits-sitges-002-5ep, ST-tramits-sitges-001-5ep, ST-tramits-VIL-001-5ep, ST-tramits-SQV-005-5ep, ST-tramits-SQV-005-10ep, ST-tramits-SQV-004-5ep, ST-tramits-SQV-004-10ep, ST-tramits-SITGES-007-5ep, ST-tramits-SB-003-5ep, ST-tramits-SB-001-5ep, ST-tramits-MONTGAT-001-5ep, SITGES-bge-FT1, SITGES-BAAI3, finetuned_arctic_ai_risk, bge-base-movie-matryoshka, batch32-100, midterm-finetuned-arctic, mpnet-base-all-medium-triplet, RUbert-tiny_custom_test_2, RUbert-tiny_custom_test, RUbert-tiny_custom, bge-base-automobile-matryoshka, Multilingual-base-soil-embedding, Multilingual-base-SWU-Matryoshka, tnt_v5_lega_new_tokens, bge_based_arg_minibio_matryoshka, votum-case-law-v1, votum-acts-v1, gte-base-legal-matryoshka-v1, gte-base-case-law-v2, bge-base-legal-matryoshka-v1, bge-base-case-law-v1, midterm-finetuned-embedding, modernbert-embed-base-bible, bge-base-bible-retrieval, BGE-Finetuned-FinBench, msmarco-distilbert-base-v4_1, bge-base-en-v1.5_v3, bge-base-en-v1.5_v2, bge-base-en-v1.5_v1, bge-base-en-v1.5, bge-base-en-trivia-anchor-positive, bge-base-financial-matryoshka_3, bge-base-financial-matryoshka_2, sentencetransformer_ftmodel_on_chemical_dataset, sentencetransformer-ft, streetlight_sql_embedding2, bge-embedding-model2, paraphrase-multilingual-MiniLM-L12-v2-ft-tr-rag-v1, gte-small-finetune-test, bge-small-en-v1.5-tr-rag-v1, bge-base-en-v1.5-41-keys-phase-2-v1, bge-base-en-41-keys-phase-2-v1, me5-large-construction-esp-cat-v2, me5-large-construction-esp-cat, bge-base-financial-matryoshka2, sentence-transformer2, Marbert-all-nli-triplet-Matryoshka, E5-all-nli-triplet-Matryoshka, Arabic-mpnet-base-all-nli-triplet, Arabic-labse-Matryoshka, Arabic-all-nli-triplet-Matryoshka, Arabic-MiniLM-L12-v2-all-nli-triplet, Arabert-all-nli-triplet-Matryoshka, Invoices_bilingual-embedding-large, bge-base-finetuned-financial, bge-base-financial-matryoshka_test_4, bge-base-financial-matryoshka_test_3, bge-base-financial-matryoshka_test_1, bge-base-financial-matryoshka_test_0, my-bge-base-financial-matryoshka, bge-base-securiti-dataset-3-v23, bge-base-securiti-dataset-1-v9, bge-base-securiti-dataset-1-v8, bge-base-securiti-dataset-1-v7, bge-base-securiti-dataset-1-v6, bge-base-securiti-dataset-1-v5, bge-base-securiti-dataset-1-v4, bge-base-securiti-dataset-1-v3, bge-base-securiti-dataset-1-v22, bge-base-securiti-dataset-1-v20, bge-base-securiti-dataset-1-v2, bge-base-securiti-dataset-1-v19, bge-base-securiti-dataset-1-v18, bge-base-securiti-dataset-1-v17, bge-base-securiti-dataset-1-v16, bge-base-securiti-dataset-1-v14, bge-base-securiti-dataset-1-v13, bge-base-securiti-dataset-1-v12, bge-base-securiti-dataset-1-v11, bge-base-securiti-dataset-1-v10, bge-base-scidocs-dataset-10k-2k-e1, bge-base-climate_fever-dataset-10k-2k-v1, bge-base-climate_fever-dataset-10k-2k-e2, bge-base-citi-dataset-detailed-9k-1_5k-e1, bge-base-citi-dataset-detailed-6k-0_5k-e2, bge-base-citi-dataset-9k-1k-e1, bge-base-arguana-dataset-10k-2k-e1, allmini-ai-embedding-similarity, sample-embedding, legal-ft-3, legal-ft-2, Indonesian-bge-m3, Indo-bge-m3, Base_Test1_, Base_T, norsbert3-base-matryoshka, pubmedbert-base-embedding-Chatbot-Matryoshk, nomic-embed-text-v1.5-Chatbot-matryoshka, bge-large-Chatbot-matryoshka, idf-go_embedder-mult_neg_rk, idf-chunk_embedder-mult_neg_rk2, sbert-base-ja, gte-large-en-v1.5_SEC_docs_ft_with_5_epochs, legal-french-matroshka, RhetoriBERT, ko-sroberta-itos-training-example_v0.04, ko-sroberta-itos-training-example_v0.03, ko-sroberta-itos-training-example_v0.02, ko-sroberta-itos-training-example, ko-sroberta-ggd-prototype, Noss, Niss, spectrum-doc-fine-tuned, snowflake-arctic-embed-l-v2.0_all-nli, bge-base-financial-matryoshka_test_my, bge-base-financial-matryoshka_test, gte-small-llama, chemBERTa-tuned-on-ClinTox-4, paraphrase-multilingual-MiniLM-L12-v2-job-cv-multi-dataset, solone-embedding, test-model-mpnet-base-all-nli-triplet, test-model-congen-mpnet-base-all-nli-triplet, model-sep-congen-debt, mixedbread-ai_mxbai-embed-large-v1_FareedKhan_prime_synthetic_data_2k_3_8, mixedbread-ai_deepset-mxbai-embed-de-large-v1_FareedKhan_prime_synthetic_data_2k_3_8, flax-sentence-embeddings_all_datasets_v4_MiniLM-L6_FareedKhan_prime_synthetic_data_2k_4_16, flax-sentence-embeddings_all_datasets_v4_MiniLM-L6_FareedKhan_prime_synthetic_data_2k_10_64, flax-sentence-embeddings_all_datasets_v4_MiniLM-L6_FareedKhan_prime_synthetic_data_2k_10_32, TaylorAI_bge-micro-v2_FareedKhan_prime_synthetic_data_2k_10_64, TaylorAI_bge-micro-v2_FareedKhan_prime_synthetic_data_2k_10_32, BAAI_bge-m3_FareedKhan_prime_synthetic_data_2k_2_4, Alibaba-NLP_gte-base-en-v1.5_FareedKhan_prime_synthetic_data_2k_10_32, FinguMv3, Fingu-M-v2, Fingu-M-v1, FingUEm_V3, stella-en-1.5B-v5-obliqa-5-epochs, bge-small-en-obliqa-5-epochs, SciTopicNomicEmbed, snowflake-l-marketing-tuned, bge-base-financial-matryoshka-2, finetuned-arctic-sentence, finetuned-arctic, modernbert-finqalab-embeddings, Morocco-Darija-Sentence-Embedding-v0.2, multilingual-e5-base-v3.1, multilingual-e5-base-v3, gte-multilingual-base-v2.1, gte-multilingual-base-v2.0, al-MiniLM-L6-v2, mdeberta-v3-base-sbert, bge-base-financial-matryoshka-abhiram, simcse-4000, simcse-2000, simcse-12000, bge-small-en-MultiplrRankingLoss-Tax-dataset, bge-small-en-MultiplrRankingLoss-30-Rag-paper-dataset, all-MiniLM-L6-v2_policy_doc_finetune, qwen_emb_6k, qwen_emb_600_best_21.11, qwen7k, qwen3k, qwen23k, qwen1k, qwen11k, qwen10k, Embedding-v2, Embedding-v1, Embedding-v0, modernbert-embed-quickb-video, modernbert-embed-quickb, ModernBERT-embed-base-legal-MRL, arabic_text_embedding_sts_arabertv02_arabicnlitriplet, Arabic_text_embedding_for_sts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Attention Is All You Need</td>\n",
       "      <td>The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.</td>\n",
       "      <td>slu_conformer_transformer_large_slurp, diar_sortformer_4spk-v1, canary-1b, SpaceTimeGPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Think you have Solved Question Answering? Try ARC, the AI2 Reasoning\\n  Challenge</td>\n",
       "      <td>We present a new question set, text corpus, and baselines assembled to\\nencourage AI research in advanced question answering. Together, these\\nconstitute the AI2 Reasoning Challenge (ARC), which requires far more powerful\\nknowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC\\nquestion set is partitioned into a Challenge Set and an Easy Set, where the\\nChallenge Set contains only questions answered incorrectly by both a\\nretrieval-based algorithm and a word co-occurence algorithm. The dataset\\ncontains only natural, grade-school science questions (authored for human\\ntests), and is the largest public-domain set of this kind (7,787 questions). We\\ntest several baselines on the Challenge Set, including leading neural models\\nfrom the SQuAD and SNLI tasks, and find that none are able to significantly\\noutperform a random baseline, reflecting the difficult nature of this task. We\\nare also releasing the ARC Corpus, a corpus of 14M science sentences relevant\\nto the task, and implementations of the three neural baseline models tested.\\nCan your model perform better? We pose ARC as a challenge to the community.</td>\n",
       "      <td>strix-rufipes-70b, aegolius-acadicus-v1-30b, aegolius-acadicus-34b-v3, Echidna-7b-128k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A Discourse-Aware Attention Model for Abstractive Summarization of Long\\n  Documents</td>\n",
       "      <td>Neural abstractive summarization models have led to promising results in\\nsummarizing relatively short documents. We propose the first model for\\nabstractive summarization of single, longer-form documents (e.g., research\\npapers). Our approach consists of a new hierarchical encoder that models the\\ndiscourse structure of a document, and an attentive discourse-aware decoder to\\ngenerate the summary. Empirical results on two large-scale datasets of\\nscientific papers show that our model significantly outperforms\\nstate-of-the-art models.</td>\n",
       "      <td>Research-Paper-Summarization-Pegasus-x-ArXiv, Research-Paper-ArXiv-Pegasus-Summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BERT: Pre-training of Deep Bidirectional Transformers for Language\\n  Understanding</td>\n",
       "      <td>We introduce a new language representation model called BERT, which stands\\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to pre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right context in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output layer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language inference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and empirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, including\\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).</td>\n",
       "      <td>bert-italian-uncased-question-answering, bert-italian-cased-question-answering, bert-base-uncased-emotion, bert-base-uncased-mrpc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WinoGrande: An Adversarial Winograd Schema Challenge at Scale</td>\n",
       "      <td>The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011),\\na benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun\\nresolution problems originally designed to be unsolvable for statistical models\\nthat rely on selectional preferences or word associations. However, recent\\nadvances in neural language models have already reached around 90% accuracy on\\nvariants of WSC. This raises an important question whether these models have\\ntruly acquired robust commonsense capabilities or whether they rely on spurious\\nbiases in the datasets that lead to an overestimation of the true capabilities\\nof machine commonsense. To investigate this question, we introduce WinoGrande,\\na large-scale dataset of 44k problems, inspired by the original WSC design, but\\nadjusted to improve both the scale and the hardness of the dataset. The key\\nsteps of the dataset construction consist of (1) a carefully designed\\ncrowdsourcing procedure, followed by (2) systematic bias reduction using a\\nnovel AfLite algorithm that generalizes human-detectable word associations to\\nmachine-detectable embedding associations. The best state-of-the-art methods on\\nWinoGrande achieve 59.4-79.1%, which are 15-35% below human performance of\\n94.0%, depending on the amount of the training data allowed. Furthermore, we\\nestablish new state-of-the-art results on five related benchmarks - WSC\\n(90.1%), DPR (93.1%), COPA (90.6%), KnowRef (85.6%), and Winogender (97.1%).\\nThese results have dual implications: on one hand, they demonstrate the\\neffectiveness of WinoGrande when used as a resource for transfer learning. On\\nthe other hand, they raise a concern that we are likely to be overestimating\\nthe true capabilities of machine commonsense across all these benchmarks. We\\nemphasize the importance of algorithmic bias reduction in existing and future\\nbenchmarks to mitigate such overestimation.</td>\n",
       "      <td>strix-rufipes-70b, aegolius-acadicus-v1-30b, aegolius-acadicus-34b-v3, Gemma-Wukong-2b, Echidna-7b-128k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RoBERTa: A Robustly Optimized BERT Pretraining Approach</td>\n",
       "      <td>Language model pretraining has led to significant performance gains but\\ncareful comparison between different approaches is challenging. Training is\\ncomputationally expensive, often done on private datasets of different sizes,\\nand, as we will show, hyperparameter choices have significant impact on the\\nfinal results. We present a replication study of BERT pretraining (Devlin et\\nal., 2019) that carefully measures the impact of many key hyperparameters and\\ntraining data size. We find that BERT was significantly undertrained, and can\\nmatch or exceed the performance of every model published after it. Our best\\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\\nhighlight the importance of previously overlooked design choices, and raise\\nquestions about the source of recently reported improvements. We release our\\nmodels and code.</td>\n",
       "      <td>indonesian-roberta-base-indonli, roberta-large-ca-v2-massive, roberta-large-ca-paraphrase, roberta-base-ca-v2-massive, roberta-base-ca-v2-cased-wikicat-ca, roberta-base-ca-v2-cased-te, roberta-base-ca-v2-cased-tc, roberta-base-ca-v2-cased-sts, roberta-base-ca-v2-cased-qa, roberta-base-ca-v2-cased-pos, roberta-base-ca-v2-cased-ner, roberta-base-ca-cased-te, roberta-base-ca-cased-tc, roberta-base-ca-cased-sts, roberta-base-ca-cased-pos, roberta-base-ca-cased-ner, roberta-base-emotion, roberta-large-bne-te, roberta-large-bne-sqac, roberta-large-bne-massive, roberta-large-bne-capitel-pos, roberta-large-bne-capitel-ner, roberta-base-es-wikicat-es, roberta-base-bne-sqac, roberta-base-bne-mldoc, roberta-base-bne-capitel-pos, bsc-bio-ehr-es-pharmaconer, bsc-bio-ehr-es-cantemist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</td>\n",
       "      <td>BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new\\nstate-of-the-art performance on sentence-pair regression tasks like semantic\\ntextual similarity (STS). However, it requires that both sentences are fed into\\nthe network, which causes a massive computational overhead: Finding the most\\nsimilar pair in a collection of 10,000 sentences requires about 50 million\\ninference computations (~65 hours) with BERT. The construction of BERT makes it\\nunsuitable for semantic similarity search as well as for unsupervised tasks\\nlike clustering.\\n  In this publication, we present Sentence-BERT (SBERT), a modification of the\\npretrained BERT network that use siamese and triplet network structures to\\nderive semantically meaningful sentence embeddings that can be compared using\\ncosine-similarity. This reduces the effort for finding the most similar pair\\nfrom 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while\\nmaintaining the accuracy from BERT.\\n  We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning\\ntasks, where it outperforms other state-of-the-art sentence embeddings methods.</td>\n",
       "      <td>vietnamese-sbert-soc, sup-SimCSE-VietNamese-phobert-base-soc, finetuned-snowflake-arctic-embed-m-v1.5, finetuned-snowflake-arctic-embed-m, all-mpnet-base-v2_snomed_expression, all-mpnet-base-v2-incident-similarity-tuned, gte-small-pairscore, all-MiniLM-L6-v5-pairscore-syn-fr, all-MiniLM-L6-v2-triplet-loss, all-MiniLM-L6-v2-pairscore, all-MiniLM-L6-v2-five-scores, all-MiniLM-L12-v2-pairscore, roberta-amharic-embed-medium, roberta-amharic-embed-base-v0, robbert-cosmetic-similarity-v2, robbert-cosmetic-similarity-v1, robbert-cosmetic-similarity, camembert-cosmetic-similarity-v2, camembert-cosmetic-similarity-cp1200, camembert-cosmetic-similarity, multilingual-e5-base-matryoshka2d-mnr-9, multilingual-e5-base-matryoshka2d-mnr-8, multilingual-e5-base-matryoshka2d-mnr-6, multilingual-e5-base-matryoshka2d-mnr-13, multilingual-e5-base-matryoshka2d-mnr-12, multilingual-e5-base-matryoshka2d-mnr-11, multilingual-e5-base-matryoshka2d-mnr-10, e5-3-test2, finetuned_MiniLM, allstats-v1-2, allstats-v1-1, allstats-semantic-search-model-v1-3-origin, allstats-semantic-search-model-v1-2, allstats-semantic-search-model-v1, allstats-semantic-search-mini-v1-old, allstats-semantic-search-mini-model-v2-2, allstats-semantic-search-mini-model-v2, allstats-semantic-mpnet-v2, allstats-semantic-mpnet-v1, allstats-semantic-mpnet, allstats-semantic-base-v1-3, allstats-semantic-base-v1-2, allstats-semantic-base-v1, allstats-search-distiluse-v1, allstats-ir-mpnet-base-v1, allstats-ir-indoSBERT-large-v1, allstat-semantic-search-paraphrase-mpnet-base-v2-2-sts, allstat-semantic-search-mpnet-base-v3-sts, allstat-semantic-search-mpnet-base-v2-sts, maux-gte-persian-v2, ModernBERT-SimCSE-multitask_v04, ModernBERT-SimCSE-multitask_v03-beta, ModernBERT-SimCSE-multitask_v03, KoModernBERT_SBERT_compare_mlmlv5, KoModernBERT_SBERT_compare_mlmlv3, KoModernBERT-base-nli-sts-SBERT_v01, intfloat-triplet-v2, gte-small-tr-v2, gte-small-tr, au-blog-rag-embedder, specter2_pubmed-v0.6, specter2_pubmed-v0.5, pubmedncl-pubmed-v0.1, modernbert-pubmed-v0.1, modernbert-bio-v0.1, cde-small-pubmed-v0.1, bge-m3-retromae-pubmed-v0.1, arctic-pubmed-v0.2, arctic-pubmed-v0.1, cc-uffs-ppc-ft-test-multiqa, cc-uffs-ppc-distiluse-base-multilingual-cased-v1-finetuned, cc-uffs-ppc, rubert-tiny2-distilled-from-LaBSE-en-ru, LaBSE-en-ru-distilled-each-third-layer, similarity-code-ai-generated, b1ade-embed-distilled-from-gte-large-en-v1.5, new_model_3, fm2, fm1, fm, finetuned_arctic, nomic-v1.5-financial-matryoshka, snowflake-arctic-embed-m-klej-dyk-v0.1, privacy_embedding_rag_10k_base_checkpoint_2-klej-dyk-v0.1, mmlw-roberta-base-klej-dyk-v0.1, gte-base-en-v1.5-klej-dyk-v0.1, bge-base-en-v1.5-klej-dyk-v0.2, bge-base-en-v1.5-klej-dyk, all-MiniLM-L6-v2-klej-dyk-v0.1, asc_embedding, xlm-roberta-base-multilingual-en-es, gte-base-korean, e5-small-korean, e5-base-korean, distilroberta-base-sts, distilroberta-base-nli-v0.2, distilroberta-base-nli-v0.1, distilroberta-base-nli-v0, bert-base-uncased-nli-v0, halong_embedding-legal-document-finetune, xlm-roberta-base-multilingual-en-ar-fr-de-es-tr-it, test-ModernBERT-base-nq-mnrl, test-ModernBERT-base-nq-debiased-mnrl, stsb-distilbert-base-quora-duplicate-questions, stsb-distilbert-base-mnrl-cl-multi, stsb-distilbert-base-mnrl, st-v3-test-mpnet-base-allnli-stsb, reranker-distilroberta-base-stsb, reranker-distilroberta-base-quora-duplicates, reranker-distilroberta-base-nli, mpnet-base-nq-prompts, mpnet-base-nq-cgist-triplet-gt, mpnet-base-nq, mpnet-base-natural-questions-mnsrl, mpnet-base-natural-questions-mnrl, mpnet-base-natural-questions-icl, mpnet-base-gooaq-hard-negatives, mpnet-base-gooaq-cmnrl-mrl, mpnet-base-gooaq, mpnet-base-allnli, mpnet-base-all-nli-triplet, distilroberta-base-paraphrases-multi, distilroberta-base-nli-v3, distilroberta-base-nli-v2, distilroberta-base-nli-matryoshka-v3, distilroberta-base-nli-matryoshka-reduced, distilroberta-base-nli-adaptive-layer, distilroberta-base-nli-2d-matryoshka, distilbert-base-uncased-wikipedia-sections-triplet, distilbert-base-uncased-sts-matryoshka, distilbert-base-uncased-sts-adaptive-layer, distilbert-base-uncased-sts-2d-matryoshka, distilbert-base-uncased-sts, bert-base-uncased-tsdae-askubuntu, bert-base-uncased-stsb-tsdae, bert-base-uncased-nli-v1, bert-base-uncased-multi-task, bert-base-uncased-gooaq-og, bert-base-uncased-gooaq, bert-base-uncased-cnn, bert-base-uncased-augmentation-indomain-nlpaug-sts, bert-base-nq-prompts-exclude-pooling-prompts, bert-base-nq-prompts, bert-base-nq, all-mpnet-base-v2-sts, TinyBERT_L-4_H-312_v2-distilled-from-stsb-roberta-base-v2, ModernBERT-base-gooaq, MiniLM-L6-H384-uncased-gooaq-no-asym, MiniLM-L6-H384-uncased-gooaq-asym, bge-base-st-phyto, all_minilm_finetuned_context_phyto, bert-base-dutch-cased-sts, stella_en_400M_v5-FinanceRAG-v2, stella_en_400M_v5-FinanceRAG-md, stella_en_400M_v5-FinanceRAG, Toxic-Retriever, finetuned_model_0613, bge-large-repmus-matryoshka, bge-large-repmus-cross_entropy, sentence_similarity_nepali_v2, Fin-ModernBERT-RAG-embed-base, finetuned-all-MiniLM-L6-v2, all-mpnet-base-v2-patabs-1epoc-batch32-100000, multilingual-e5-small-triplet-final-1, multilingual-e5-small-pairclass-contrastive, multilingual-e5-small-pairclass-4, multilingual-e5-small-pairclass-3, multilingual-e5-small-pairclass-2, multilingual-e5-small-pairclass-1, multilingual-e5-small-cogcache-contrastive, fine_tuned_model_9, fine_tuned_model_8, fine_tuned_model_7, fine_tuned_model_6, fine_tuned_model_5, fine_tuned_model_4, fine_tuned_model_3, fine_tuned_model_2, fine_tuned_model_17, fine_tuned_model_16, fine_tuned_model_15, fine_tuned_model_14-sts, fine_tuned_model_14, fine_tuned_model_13, fine_tuned_model_12, fine_tuned_model_11, fine_tuned_model_10, fine_tuned_model_1, fine_tuned_model, e5-small-cogcachedata-6, e5-small-cogcachedata-1, e5-small-cogcachedata, e-small-triplet-balanced, e-small-triplet, KR-SBERT-Medium-klueNLItriplet_PARpair-klueSTS, KR-SBERT-Medium-klueNLI-klueSTS, KR-SBERT-Medium-extended-klueNLItriplet_PARpair_QApair-klueSTS, KLUE-SRoBERTa-Large-SNUExtended-klueNLI-klueSTS, embedding-finetuned, bge_pairs, SRBedding-base-distilled-v1, triplet_CloseHlabel_farLabel_andnegativ-1M-5eps-XLMR_29may, student-multilang-XLMR-14jun, silma-embeddding-matryoshka-v0.1, ModernBERT-korean-large-preview, all-mpnet-base-v2-sample, finetune-sentence-transformer, finetune, distilroberta-ai-job-embeddings, nomic-embed-financial-matryoshka, test_bge_2_10ep, test_bge_10ep, t2, all-nli-bert-tiny-dense, modernbert-embed-base-biencoder-human-rights, gte-base-ko, bge-m3-ko-v1.1, swahili-paraphrase-multilingual-mpnet-base-v2-nli-matryoshka, bge-base-swahili-matryoshka, econo-sentence-v2, worksphere-regulations-embedding_bge, multilingual-e5-base-test, embedding-BOK, custom-bge, thenlper-gte-base-fine-tuned, sentence-transformers-all-MiniLM-L6-v2-fine-tuned, intfloat-multilingual-e5-small-fine-tuned, bgem3-shakespeare_st_3, BAAI-bge-m3-fine-tuned, BAAI-bge-large-en-v1.5-fine-tuned, bge-base-financial, sentence-transformers-all-mpnet-base-v2, sbert_ft_cross-encoder-nli-deberta-v3-large, ai-policy-ft, bge-finetuned-reranker, bge-finetuned, embedding_criteria_profile_summary_matching_qa_minilm_v1, embedding_criteria_profile_summary_matching_from_criteria_minilm_v3, finetuned-arctic-model-2, finetuned-arctic-model, mxbai-embed-large-v1-financial-rag-matryoshka, mpnet-base-financial-rag-matryoshka, financial-rag-matryoshka, bge-base-financial-nvidia-matryoshka, UAE-Large-V1-financial-rag-matryoshka, bge-small-en-v1.5-RIRAG_ObliQA, roberta-amharic-text-embedding-medium, roberta-amharic-text-embedding-base, bert-amharic-text-embedding-medium, indo-islamic-sentence-bert-v2, indo-islamic-sentence-bert, bge-small-matryoshka-fine-tuned, indobert-large-stsb, indobert-base-stsb, indobert-base-p2-sts-arxiv-id, LEGAL_EMBEDDING, tpbank-dense_embedding, bl_ademe_large, muril-base-cased-assamese-indicxnli-random-negatives-v1-sts, distilbert-base-multilingual-cased-indicxnli-random-negatives-v1-sts, distilbert-base-multilingual-cased-indicxnli-random-negatives-v1, assamese-bert-nli-v2-sts, assamese-bert-nli-v2-assamese-sts, assamese-bert-nli-v2, all-MiniLM-L6-v2-sts, bge_model_fine_tuned_law, bge-base-en, bge-base-argilla-sdk-matryoshka, msmarco-distilbert-base-v4, custom-v2, custom-v1, clphobert-base, bge-base-financial-matryoshka, cese5020-contrastive-model, MiniLM-similarity-small, finbeddings_bert, sentence-transformer-trained-tweet, fine-tuned-sts-embedder, mfds-all-mpnet-base-v2, bge-base-en-sec10k-embed, bge-base-en-honsec10k-embed, bge-base-en-bioembed768, bge-base-en-bioembed, finetuned-sts-roberta-base-ca-v2, finetuned-sts-ca-mpnet-base, AIE4_midterm_tuned_embeddings_2, AIE4_midterm_tuned_embeddings, sentest, bge-m3-trained-2, bge-m3-trained, bge-m3-spa-law-qa-trained-2, bge-m3-spa-law-qa-trained, bge-m3-retrained, A2P-constrastive-all, roberta-base-klue-similarity-sts, test9, test7, test3, test13, test12, test11, SBertBaseMittanbudver1, sentence-distilbert-turkish, Finetuned_Alibaba_Large, stsb-distilbert-base-ocl, arabic-english-sts-matryoshka-v2.0, arabic-english-sts-matryoshka, Arabic-STS-Matryoshka-V2, Arabic-STS-Matryoshka, Arabic-Retrieval-v1.0, makeML-snowflake, artic_ft_midterm, ModernBERT-large-sts, ModernBERT-base-sts, Italian-ModernBERT-base-embed-mmarco-triplet, Italian-ModernBERT-base-embed-mmarco-mnrl, bge-small-qs, embedding1, twitter-paraphrase-embeddings, turkish-legal-bert-base-uncased-stsb-v1-sts, legal-text-embedding-turkish-v1, mxbai-embed-large-v1-ft-webinstruct, multilingual-e5-large-ft-sts-spanish-matryoshka-768-64-5e, multilingual-e5-large-ft-sts-spanish-matryoshka-768-16-5e, modernbert-embed-base-ft-sts-spanish-matryoshka-768-64, modernbert-embed-base-ft-finetome, gte-large-ft-webinstruct, distilroberta-base-ft-webinstruct, distilroberta-base-ft-allnli-matryoshka-768-64-1e-256bs, distilroberta-base-ft-allnli-matryoshka-768-16-1e-128bs, distilbert-base-matryoshka-sts-v2, distilbert-base-matryoshka-sts, gattina-ha-classifier-cossim-fpt, gattina-ha-classifier-cossim-ffpt, gattina-ha-classifier-cossim, mpac-bge-large-v1.2, mpac-bge-large, hateBERT-cl-rlhf-5-epochs, hateBERT-cl-rlhf-10-epochs, hateBERT-cl-rlhf, bert-base-uncased-cl-rlhf-5-epochs, bert-base-uncased-cl-rlhf-10-epochs, bert-base-uncased-cl-rlhf, gte-base-law-matryoshka, multilingual-e5-large-ita, xlm-roberta-small-all-nli-triplet, mpnet-base-all-nli-triplet-turkish-v4-dgx, mpnet-base-all-nli-triplet-turkish-v3, mpnet-base-all-nli-triplet-turkish-v2, vn_biencoder_MultipleNegativesSymmetricRankingLoss, vn_biencoder_CachedMultipleNegativesSymmetricRankingLoss, vn_biencoder_CachedMultipleNegativesRankingLoss, vn_bi_encoder_OnlineContrastiveLoss, vn_bi_encoder_MultipleNegativesRankingLoss, vn_bi_encoder_16neg, phobert_Tripel, phobert_OnlineContrastiveLoss, phobert_GISTEmbedLoss, phobert_ContrastiveLoss, phobert-finetune-512, phobert-finetune, paraphrase-multilingual-mpnet-base-v2_finetune_med, paraphrase-multilingual-mpnet-base-v2_finetune-512, paraphrase-multilingual-MiniLM-L12-v2_finetune, e5_large_finetune_16neg, e5_large_finetune, demo_bi_encoder, bgeEmbeddingsRetailedFT, me5-small-preskripsi-relevancy-and-binary-sentiment-acc_sent91-acc_rel98-20250131_093535, me5-small-preskripsi-embedding-pos-neg, bge-99GPT-v1-test, bge-99GPT-v1, minilm-l12-v2-toxic-ft, minilm-l12-v2-simple, ModernBERT-large-BORA, mpnet-base-nli-v2, roberta-base-ft-all-nli, modernbert-large-ft-all-nli, modernbert-base-ft-all-nli, ft-modern-bert-emb-all-nli, bert-base-uncased-ft-all-nli, ModernBERT-large-DPR-msmarco, ModernBERT-base-DPR-msmarco, vietnamese-bi-encoder-for-SoICT-2024, vietnamese-bi-encoder-fine-tuning-for-law-chatbot, bge-base-custom-matryoshka, bge-small-en-v1.5-ft-orc-0930-dates, bge-small-en-v1.5-ft-orc-0813, bge-small-en-v1.5-ft-orc-0806, nomic-embed-philosophy-triplets_v9, nomic-embed-philosophy-triplets_v7, nomic-embed-philosophy-triplets_v5, nomic-embed-philosophy-triplets_v3, nomic-embed-philosophy-triplets_v1, discipline-tuned_specter_2_024, discipline-tuned_specter_2_019, discipline-tuned_specter_2_015, discipline-tuned_specter_2_010, discipline-tuned_specter_2_009, discipline-tuned_specter_2_001, discipline-tuned_specter_1_001, discipline-bert-modern-large_v02, discipline-bert-modern-large_01, bge-m3-philosophy-triplets_v3, bge-m3-philosophy-triplets_v1, policy_gte_large_7, policy_gte_large_5, policy_gte_large_2plus, policy_gte_large_2, policy_gte_large, esci-nomic-embed-text-v1_5, proba, xlm-roberta-base-multilingual-mkqa, bert-es-pt-cased-matryoshka, bert-en-es-pt-matryoshka_v3, bert-en-es-pt-matryoshka_v2, bert-en-es-pt-matryoshka_v1, bert-base-multilingual-uncased-matryoshka-mkqa, bert-base-multilingual-cased-matryoshka-mkqa, TriLingual-BERT-Distil, lemone-embed-s-boost, lemone-embed-s, lemone-embed-pro, lemone-embed-m-boost, lemone-embed-m, lemone-embed-l-boost, lemone-embed-l, legal-ft-arctic-l, legal-ft, finetuned-bge-base-en, mpnet-base-all-pittsburgh-squad, bge-m3-spa-law-qa, modernbert-embed-ft-const-legal-matryoshka, modernbert-embed-base-legaltextai-matryoshka-legaldataset, comp-embedding-matching, snowflake-arctic-embed-xs-ms-marco-triplet, fine-tuned-bge-base-raw_pdf-v1, fine-tune-embedding-bge-base-HrPolicy_vfinal, fine-tune-embedding-bge-base-HrPolicy, bge-base-raw_pdf_finetuned_vf1, mxbai-de-abat-matryoshka, mxbai-abat-matryoshka, paraphrase-multilingual-MiniLM-L12-hu_v1, paraphrase-multilingual-MiniLM-L12-hu-v3, paraphrase-multilingual-MiniLM-L12-hu-v2, paraphrase-multilingual-MiniLM-L12-hu, gte-multilingual-base-hu, bge-m3-hu, ModernBERT-base-hu_v3, ModernBERT-base-hu_v2, ModernBERT-base-hu, all-miniLM-L6-en-ja, all-MiniLM-L6-multilingual-v2-en-es-pt-pt-br-v2, legal_paraphrase, sentence-t5-base-bioasq-1epoch-batch32-100steps, bge-small-bioasq-3epochs-batch32, bge-small-bioasq-1epochs-batch32, bge-small-bioasq-1epoch-batch32-step50, bge-small-bioasq-1epoch-batch32-100steps, bge-small-bioasq-1epoch-batch32, bge-base-bioasq-matryoshka, all-mpnet-base-v2-bioasq-matryoshka, all-mpnet-base-v2-bioasq-1epoch-batch32-100steps, all-mpnet-base-v2-bioasq-1epoc-batch32-100, fine-tuned-matryoshka-500, fine-tuned-matryoshka-200, fine-tuned-matryoshka-1725, fine-tuned-matryoshka-1500, fine-tuned-matryoshka-1000, fine-tuned-matryoshka-100, fine-tuned-matryoshka, food_embeddings5, food_embeddings4, food_embeddings3, food_embeddings2, food_embeddings, paraphrase-multilingual-MiniLM-L12-v2-helpfulness, improve_vibi, improve_halong, gte-en-mlm-base-msmarco, ModernBERT-large-msmarco, ModernBERT-base-msmarco, ColModernBERT-base-msmarco-en-bge, snowflake-arctic-embed-xs_finetuned_aipolicy, snowflake_finetuned_semantic, snowflake_finetuned_recursive, mpnet_finetuned_semantic, mpnet_finetuned_recursive, kicon_e5large_15_v1, embedding_BAAI-bge-m3, finetuned_arctic-embedd-l, all-MiniLM-L6-v2_tuned_on_deepparse_address_mutations_comb_3, all-MiniLM-L6-v2-nepali, finetuned-bge-base-v2, finetuned-bge-bai, finetuned-BAAI-bge-base-en, logembed_a1, CR-biodiversity-sentence-similarity-es, CR-biodiversity-preprocessed-sentence-similarity-es, Starbucks_STS, klue-roberta-base-klue-sts-mrc, klue-roberta-base-klue-sts, gte_base_MIMICCXR_FT, stage4_1, stag_123_cp8000, stag_123_cp10000, stag_123, model_stage4_v2_latest_new, model_stage4_v1_latest, model_stage4_score, model_stage4, model_stage3_silver, model_stage3_latest, model_stage3_2_score, model_stage3_2_loss, model_stage3, model_stage2_latest, model_stage2_1436, model_stage2, model_stage1_latest, model_stage1, final_model_main, final-model-v2, bkai-2024-retrival-e5-finetune-v2, e5-large-v2-nli-v1, yue-embed, bert-large-cantonese-sts, bert-large-cantonese-nli, distilroberta-base-sentence-transformer-triplets, mmarco-Arabic-mMiniLML-bi-encoder-NoKD-v1, mmarco-Arabic-mMiniLML-bi-encoder-KD-v1, mmarco-Arabic-AraElectra-bi-encoder-NoKD-v1, mmarco-Arabic-AraElectra-bi-encoder-KD-v1, mmarco-Arabic-AraDPR-bi-encoder-NoKD-v1, mmarco-Arabic-AraDPR-bi-encoder-KD-v1, bge-m3-nvidia-ko-v1, snowflake-arctic-embed-l-v2.0-pits, bge-small-financial-matryoshka, bge-m3-financial-matryoshka, sts-distilcamembert-base, sts-camembert-base, bge-small-en-v1.5-2025-01-01_21-55-17, facet_retriever, paraphrase-multilingual-minilm-l12-v3-mn, paraphrase-multilingual-minilm-l12-v2-mn, paraphrase-mongolian-minilm-mntoken, paraphrase-mongolian-minilm-mn_v2, paraphrase-mongolian-minilm, finetuned_paraphrase-multilingual_v3, finetuned_paraphrase-multilingual_v2, finetuned_paraphrase-multilingual_test, finetuned_paraphrase-multilingual_mpnet_try6, finetuned_paraphrase-multilingual_mpnet_try5, finetuned_paraphrase-multilingual_mpnet_try4, finetuned_paraphrase-multilingual_mpnet_try3, finetuned_paraphrase-multilingual_mpnet_try2, finetuned_paraphrase-multilingual_mpnet, finetuned_paraphrase-multilingual, snowflake-arctic-embed-m-finetuned, crash_encoder2-sts, crash_encoder1-sts, chemembed-chemselfies, my-awesome-bi-encoder, bge-large-mpnet-base-all-nli-triplet-final-50000, bge-large-mpnet-base-all-nli-triplet-final, bge-base-financial-matryoshka-v1, model_3, bge-m3-uz-legal-matryoshka, bge-base-space-mt-tsdae, arctic-embed-m-space-sup, job_and_title_siamese_binary, bge-finetuned-insurance-matryoshka, bge-base-insurance-matryoshka, ModernBERT-base-nli-v3, ModernBERT-base-marco, embed-andegpt-H768, embed-andegpt-H384, xlm-similarity-large, xlm-similarity, bge-small-en-v1.5-esg-v2, bge-small-en-v1.5-esg, bge-micro-v2-esg-v2, bge-micro-v2-esg, bge-base-financial-matryoshka-testing, news-similarity-ukr, sentence-roberta-small, xlm-roberta-large-sts-matryoshka, phi-2-telecom-ft, bge-small-qa-telecom-ft, FT_RAG, all-distilroberta-v1_danish_law_fine_tune, Ko-sroberta-base-multitask, reranker_dialog_items_biencoder_rubert-tiny-turbo-7, reranker_dialog_items_biencoder_rubert-tiny-turbo-6, reranker_dialog_items_biencoder_rubert-tiny-turbo-5, reranker_dialog_items_biencoder_rubert-tiny-turbo-4, reranker_dialog_items_biencoder_rubert-tiny-turbo-3, nomic-embed-text-v1, minilm-odds-events-weval-float-1epoch, philai-embeddings-2.0, pb-small-10e-tsdae6e-philsim-cosine-6e-beatai-cosine-80e, pb-small-10e-tsdae6e-philsim-cosine-6e-beatai-cosine-50e, pb-small-10e-tsdae6e-philsim-cosine-6e-beatai-30e, pb-ds1-48K-philsim, minilm6_perfumerecommender_v4, minilm6_perfumerecommender_v3, minilm6_perfumerecommender_v2, minilm6_perfumerecommender_v1, code-prompt-similarity-model, sbert-encode-cellines-tuned, bge-base-for_text2sql, bge-m3-es-legal-tmp-6, bge-m3-es-legal-tmp-5, bge-m3-es-legal-tmp-3, multilingual-e5-large-triplet_loss, french-document-embedding, negasibert-mnrls, negasibert-mnrl, negasibert-mbm, negasibert-ct, software-15, my-finetuned-sbert, all-mpnet-base-v2-unfair-tos-rationale, bge-m3-aicacia, modernbert-embed-base-legal-matryoshka-2, bge-finetuned-train, finetuned2-snli-MiniLM-L12-v2, finetuned-snli-MiniLM-L12-v2-100k-en-fr, finetuned-snli-MiniLM-L12-v2, all-MiniLM-L6-v2-finetuned-imdb, indobert-t4, indobert-t3, indobert-snli-v1, indobert-base-p2-nli-v2, indobert-base-p2-nli-v1, 4bs8lr2, 4bs4lr2, 3bs4lr2, 2bs8lr2, 2bs4lr2, 2bs32lr2, 2bs16lr2, slinger20241231-3, slinger20241231-2, slinger20241231-1, hi-di-hi, all-mpnet-base-v2-modulepred, sbert_model_jobcv, jev2-legal, procedure-tool-matching_3_epochs, procedure-tool-matching_10_epochs, RoBERTa-base-unsupervised-TSDAE, E5-base-unsupervised-TSDAE-2, E5-base-unsupervised-TSDAE, DeBERTaV3-small-SentenceTransformer-AdaptiveLayerBaseline, DeBERTaV3-small-SentenceTransformer-AdaptiveLayerAll, DeBERTaV3-small-SenTra-AdaptiveLayers-AllSoft-LowTemp, DeBERTaV3-small-SenTra-AdaptiveLayers-AllSoft-HighTemp, DeBERTaV3-small-ST-AdaptiveLayers-ep2, DeBERTaV3-small-ST-AdaptiveLayerAllNormalized, DeBERTaV3-small-ST-AdaptiveLayer-Norm-ep2, DeBERTaV3-small-ST-AdaptiveLayer-3L-ep2, DeBERTaV3-small-GeneralSentenceTransformer-v3-step1, DeBERTaV3-small-GeneralSentenceTransformer-v2-checkpoints-tmp, DeBERTaV3-small-GeneralSentenceTransformer-v2-AllSoft, DeBERTaV3-small-GeneralSentenceTransformer, DeBERTaV3-TR-AllSoft-HT, DeBERTa3-s-CustomPoolin-toytest3-step1, DeBERTa3-s-CustomPoolin-toytest2-step1, DeBERTa3-s-CustomPoolin-toytest-step1, DeBERTa3-base-STr-CosineWaves, DeBERTa-small-ST-v1-toytest, DeBERTa-small-ST-v1-test-step3, DeBERTa-small-ST-v1-test-step2, DeBERTa-small-ST-v1-test-UnifiedDatasets-Ft2, DeBERTa-ST-AllLayers-v3.1bis, DeBERTa-ST-AllLayers-v3.1, DeBERTa-ST-AllLayers-testing, bge-m3-finetuned-2, bge-m3-finetuned-1, bert-base-uncased-sts, minilm-bo, bge-base-patentmatch, deep-learning-for-embedding-model-ssilwal-qpham6_army_doc, deep-learning-for-embedding-model-ssilwal-qpham6, Finance2_embedding_small_en-V1.5, int-e5-base-5tv5, mpnet-base-all-mqp-binary, all-MiniLM-L6-v2-MEDI-MTEB-triplet-randproj-trainable-512-final, all-MiniLM-L6-v2-MEDI-MTEB-triplet-randproj-64-final, all-MiniLM-L6-v2-MEDI-MTEB-triplet-randproj-512-final, all-MiniLM-L6-v2-MEDI-MTEB-triplet-final, German-RAG-ModernBERT-Base-TRIPLES, finetuned_bge_embeddings_v4_base_v1.5, gutenberg_authorship, paraphrase-multilingual-mpnet-base-v2-7, legal-ft-1, bert-base-uncased-augmentation-indomain-bm25-sts, all-mpnet-base-v2-augmentation-indomain-bm25-sts, fine-tuned-mpnet-v3, technographics-marketing-matryoshka, finetuned-gte-base, retrieval-mpnet-dot-finetuned-llama3-synthetic-dataset, retrieval-mpnet-dot-finetuned-llama3-openbiollm-synthetic-dataset, xlm-roberta-base-msmarco-webfaq, xlm-roberta-base-msmarco, bge-base-matryoshka-aws-casestudies, bge-base-financial-matryoshka-anisha, bge-base-aws-case-studies, bge-base-financial-matryoshka-nvda-iter20, bge-base-financial-matryoshka-nvda, vietnamese-sbert-Financial-Matryoshka-5e-11k, vietnamese-sbert-Financial-Matryoshka-2e-11k, vietnamese-sbert-Financial-Matryoshka-1e-200k, vietnamese-bi-encoder-financial-matryoshka-5, vietnamese-bi-encoder-financial-matryoshka-2, vietnamese-bi-encoder-Matryoshka-2e-9k, vietnamese-bi-encoder-Matryoshka-1e-9k, vietnamese-bi-encoder-Financial-Matryoshka-5e-11k, vietnamese-bi-encoder-Financial-Matryoshka-3e-200k, vietnamese-bi-encoder-Financial-Matryoshka-2e-11k, vietnamese-bi-encoder-Financial-Matryoshka-1e-200k, vietnamese-bi-encoder-Financial-Matryoshka, multilingual-e5-base-Matryoshka-7e-11k, multilingual-e5-base-Matryoshka-5e-11k, multilingual-e5-base-Matryoshka-2e-11k, multilingual-e5-base-Matryoshka-1e-200k, mordernBERT-multilingual-legal-1e, halong_embedding-Financial-Matryoshka-2e-11k, halong_embedding-Financial-Matryoshka-1e-200k, halong-embedding-Financial-Matryoshka-5e-11k, gte-multilingual-legal-1e, gte-multilingual-base-Matryoshka-4e-9k, gte-multilingual-base-Matryoshka-3e-9k, gte-multilingual-base-Matryoshka-2e-9k, gte-multilingual-base-Matryoshka-1e-9k, gte-multilingual-base-Matryoshka-1e-11k, bert-base-multilingual-uncased-Financial-Matryoshka-8e-11k, bert-base-multilingual-uncased-Financial-Matryoshka-5e-11k, bert-base-multilingual-uncased-Financial-Matryoshka-2e-11k, bert-base-multilingual-Financial-Matryoshka-2-v2, bert-base-multilingual-Financial-Matryoshka, ModernBERT-multilingual-legal-2e, ModernBERT-base-test-v2, ModernBERT-base-test, ModernBERT-base-3e-9k, Velvet-2B-embedding-news, indic-bert-nli-matryoshka, Indic_Bert-8-layers, multilingual-e5-g39, xlmrsim-mar_cos, xlmrsim-mar_2ep, bge-m3-distill-8l, indic-mxbai-L8-embed, d-mxbai-L8-embed, bge-m3-8-layers, all-MiniLM-L6-v2-8-layers, exxon-semantic-search, jina-semantic-bmf-matryoshka-1024-10epochs, jina-semantic-bmf-matryoshka, german-semantic-bmf-matryoshka-512-10epochs, german-semantic-bmf-matryoshka, bge-semantic-bmf-matryoshka, sbert_nli_test, bge-base-financial-matryoshkafinetuning-tcz-webiste, bge-base-financial-matryoshka-finetuning-tcz-1, st-SIT-test, sqv-v3-10ep, sqv-v3, sqv-v2, sqv-5ep, sitgrsBAAIbge-m3-300824v2, sitgrsBAAIbge-m3-290824, sitges2608bai-4ep, sitges2608, sitges10242608-4ep-rerankv4-sp, sitges10242608-4ep-rerankv3-sp, sitges10242608-4ep-rerankv3, sitges10242608-4ep-rerankv2, sitges10242608-4ep-rerank, ST-tramits-sitges-006-5ep, ST-tramits-sitges-005-5ep, ST-tramits-sitges-003-5ep, ST-tramits-sitges-003-10ep, ST-tramits-sitges-002-5ep, ST-tramits-sitges-001-5ep, ST-tramits-VIL-001-5ep, ST-tramits-SQV-005-5ep, ST-tramits-SQV-005-10ep, ST-tramits-SQV-004-5ep, ST-tramits-SQV-004-10ep, ST-tramits-SITGES-007-5ep, ST-tramits-SB-003-5ep, ST-tramits-SB-001-5ep, ST-tramits-MONTGAT-001-5ep, SITGES-bge-FT1, SITGES-BAAI3, bert-base-multilingual-cased-finetuned-yoruba-IR, finetuned_arctic_ai_risk, amharic-xlmr-finetuned, BAA-finetuned-yoruba-IR, bge-base-movie-matryoshka, batch32-100, midterm-finetuned-arctic, mpnet-base-all-medium-triplet, paraphrase-multLing-L12-v2_custom, custom-paraphrase-v2, RUbert-tiny_custom_test_2, RUbert-tiny_custom_test, RUbert-tiny_custom, bge-base-automobile-matryoshka, Multilingual-base-soil-embedding, Multilingual-base-SWU-Matryoshka, legal_nli_TR_V1, multilingual-e5-base-trimm-vocab-1024-v3, multilingual-e5-base-trimm-vocab-1024-v2, gte-base-v1__trim_vocab-1024, gte-base-v0__trim_vocab-1024, tnt_v5_lega_new_tokens, bge_based_arg_minibio_matryoshka, sentence_CafeBERT, votum-case-law-v1, votum-acts-v1, gte-base-legal-matryoshka-v1, gte-base-case-law-v2, bge-base-legal-matryoshka-v1, bge-base-case-law-v1, midterm-finetuned-embedding, MiniLM6-v2-sport, modernbert-embed-base-bible, bge-base-bible-retrieval, BGE-Finetuned-FinBench, msmarco-distilbert-base-v4_1, bge-base-en-v1.5_v3, bge-base-en-v1.5_v2, bge-base-en-v1.5_v1, bge-base-en-v1.5, tiny_sent_transformer_v2, tiny_sent_transformer, bge-base-en-trivia-anchor-positive, multiling-e5-large-instruct-claim-matching, embedding_finetuned_test, embedding_finetuned, bge-base-financial-matryoshka_3, bge-base-financial-matryoshka_2, sentencetransformer_ftmodel_on_chemical_dataset, sentencetransformer-ft, streetlight_sql_embedding2, bge-embedding-model2, paraphrase-multilingual-MiniLM-L12-v2-ft-tr-rag-v1, gte-small-finetune-test, bge-small-en-v1.5-tr-rag-v1, paraphrase-multilingual-mpnet-base-v2-sts, preTrained_meanPooling_mistranslationModel, multiSts_meanPooling_mistranslationModel, monoSts_meanPooling_mistranslationModel, directTwoEpoch_dotProductPooling_randomInit_mistranslationModel, directTwoEpoch_additivePooling_randomInit_mistranslationModel, directTwoEpoch_additivePooling_noisedInit_mistranslationModel, directThreeEpoch_dotProductPooling_randomInit_mistranslationModel, directThreeEpoch_additivePooling_randomInit_mistranslationModel, directThreeEpoch_additivePooling_noisedInit_mistranslationModel, directOneEpoch_dotProductPooling_randomInit_mistranslationModel, directOneEpoch_additivePooling_randomInit_mistranslationModel, directOneEpoch_additivePooling_noisedInit_mistranslationModel, directFourEpoch_meanPooling_mistranslationModel, directFourEpoch_additivePooling_noisedInit_mistranslationModel, bge-base-en-v1.5-41-keys-phase-2-v1, bge-base-en-41-keys-phase-2-v1, me5-large-construction-v2, me5-large-construction-esp-cat-v2, me5-large-construction-esp-cat, me5-large-construction-cat, me5-large-construction-adapter-v3, me5-large-construction-adapter-v2, me5-large-construction-adapter, me5-large-construction, bge-base-financial-matryoshka2, indoedubert-bge-m3-exp2, anime-recommendation-model, sentence-transformer2, Marbert-all-nli-triplet-Matryoshka, E5-all-nli-triplet-Matryoshka, Arabic-mpnet-base-all-nli-triplet, Arabic-labse-Matryoshka, Arabic-all-nli-triplet-Matryoshka, Arabic-MiniLM-L12-v2-all-nli-triplet, Arabert-all-nli-triplet-Matryoshka, Checket_Antwerpen_Huisstijl_MiniLM, ChecketV2, jina-embeddings-v2-base-code-mbpp, bge-base-mbpp-processed, bge-base-mbpp, Invoices_bilingual-embedding-large, GO-Term-Embeddings-Snowflake-m-1.5, ModernBERT-base-DPR-fullneg-gte-0.0002, bge-base-finetuned-financial, bge-base-financial-matryoshka_test_4, bge-base-financial-matryoshka_test_3, bge-base-financial-matryoshka_test_1, bge-base-financial-matryoshka_test_0, finetuned2-MiniLM-L12-v2, Finetune2-MiniLM-L12-v2, all_MiniLM_L6_nav1, UAE_Large_V1_nav2, UAE_Large_V1_nav1, my-bge-base-financial-matryoshka, bge-base-securiti-dataset-3-v23, bge-base-securiti-dataset-1-v9, bge-base-securiti-dataset-1-v8, bge-base-securiti-dataset-1-v7, bge-base-securiti-dataset-1-v6, bge-base-securiti-dataset-1-v5, bge-base-securiti-dataset-1-v4, bge-base-securiti-dataset-1-v3, bge-base-securiti-dataset-1-v22, bge-base-securiti-dataset-1-v20, bge-base-securiti-dataset-1-v2, bge-base-securiti-dataset-1-v19, bge-base-securiti-dataset-1-v18, bge-base-securiti-dataset-1-v17, bge-base-securiti-dataset-1-v16, bge-base-securiti-dataset-1-v14, bge-base-securiti-dataset-1-v13, bge-base-securiti-dataset-1-v12, bge-base-securiti-dataset-1-v11, bge-base-securiti-dataset-1-v10, bge-base-scidocs-dataset-10k-2k-e1, bge-base-climate_fever-dataset-10k-2k-v1, bge-base-climate_fever-dataset-10k-2k-e2, bge-base-citi-dataset-detailed-9k-1_5k-e1, bge-base-citi-dataset-detailed-6k-0_5k-e2, bge-base-citi-dataset-9k-1k-e1, bge-base-arguana-dataset-10k-2k-e1, ModernBERT-TR-base-nli-stsb-tr, allmini-ai-embedding-similarity, sample-embedding, legal-ft-3, legal-ft-2, Indonesian-bge-m3, Indo-bge-m3, Base_Test1_, Base_T, norsbert3-base-matryoshka, pubmedbert-base-embedding-Chatbot-Matryoshk, nomic-embed-text-v1.5-Chatbot-matryoshka, bge-large-Chatbot-matryoshka, idf-go_embedder-mult_neg_rk, idf-go_embedder-contrastive-after_epoch_4, idf-go_embedder-contrastive-after_epoch_1, idf-go_embedder-contrastive-after_epoch_0, idf-go_embedder-contrastive, idf-chunk_embedder-mult_neg_rk2, idf-chunk_embedder-contrastive2, sbert-base-ja-arc-temp, sbert-base-ja, all-MiniLM-L6-v2, sentence-flaubert-base, bilingual-embedding-small, bilingual-embedding-large, bilingual-embedding-base, bilingual-document-embedding, gte-large-en-v1.5_SEC_docs_ft_with_5_epochs, legal-french-matroshka, RhetoriBERT, ko-sroberta-itos-training-example_v0.04, ko-sroberta-itos-training-example_v0.03, ko-sroberta-itos-training-example_v0.02, ko-sroberta-itos-training-example, ko-sroberta-ggd-prototype, Noss, Niss, spectrum-doc-fine-tuned, snowflake-arctic-embed-l-v2.0_all-nli, bge-base-financial-matryoshka_test_my, bge-base-financial-matryoshka_test, all-mpnet-base-v2-anteater, ramdam_fingerprint_embedding_model, FT-triple-2, FT-label-consent-20, FT-label-consent-10, FT-label-aug-consent-10, gte-small-llama, chemberta-clintox-tunned-3, chemBERTa-tuned-on-ClinTox-4, chemBERTa-tuned-on-ClinTox-3, paraphrase-multilingual-MiniLM-L12-v2-job-cv-multi-dataset, solone-embedding, finetune-embedding-all-MiniLM-L6-v2-geotechnical-test-v4, finetune-embedding-all-MiniLM-L6-v2-geotechnical-test-v3, STS-multilingual-mpnet-base-v2, test-model-mpnet-base-all-nli-triplet, test-model-congen-mpnet-base-all-nli-triplet, model-sep-congen-debt, mixedbread-ai_mxbai-embed-large-v1_FareedKhan_prime_synthetic_data_2k_3_8, mixedbread-ai_deepset-mxbai-embed-de-large-v1_FareedKhan_prime_synthetic_data_2k_3_8, flax-sentence-embeddings_all_datasets_v4_MiniLM-L6_FareedKhan_prime_synthetic_data_2k_4_16, flax-sentence-embeddings_all_datasets_v4_MiniLM-L6_FareedKhan_prime_synthetic_data_2k_10_64, flax-sentence-embeddings_all_datasets_v4_MiniLM-L6_FareedKhan_prime_synthetic_data_2k_10_32, TaylorAI_bge-micro-v2_FareedKhan_prime_synthetic_data_2k_10_64, TaylorAI_bge-micro-v2_FareedKhan_prime_synthetic_data_2k_10_32, BAAI_bge-m3_FareedKhan_prime_synthetic_data_2k_2_4, Alibaba-NLP_gte-base-en-v1.5_FareedKhan_prime_synthetic_data_2k_10_32, FinguMv3, Fingu-M-v2, Fingu-M-v1, FingUEm_V3, stella-en-1.5B-v5-obliqa-5-epochs, bge-small-en-obliqa-5-epochs, finetuned-sentence-transformers-multi-qa-mpnet-base-dot-v1, Finetuned-electra-large, multilingual-e5-small-cross-encoder-v0.1, multilingual-e5-large-instruct-embedder_distill-tgd, multilingual-e5-large-instruct-embedder_distill-tg, multilingual-e5-large-instruct-embedder-tgd, multilingual-e5-large-instruct-embedder-tg, USER-bge-m3-embedder_distill-tgd, USER-bge-m3-embedder_distill-tg, USER-bge-m3-embedder-td, sentence-transformers_paraphrase-multilingual-MiniLM-L12-v2_FINETUNED_on_torob_data_v6, sentence-transformers_paraphrase-multilingual-MiniLM-L12-v2_FINETUNED_on_torob_data_v5, sentence-transformers_paraphrase-multilingual-MiniLM-L12-v2_FINETUNED_on_torob_data_v4, sentence-transformers_paraphrase-multilingual-MiniLM-L12-v2_FINETUNED_on_torob_data_v2_3, paraphrase-multilingual-MiniLM-L12-v2_QuoraDuplicateDetection_FINETUNED, trained_on_all_data_model_push_00, mini_lm_l6_v2_trained_on_all_data_model_push_00, bert_lang_trained_on_all_data_model_push_00, Sentence-Transformer_1, SciTopicNomicEmbed, ModernBERT-SimCSE-multitask_v05, snowflake-l-marketing-tuned, bge-base-financial-matryoshka-2, finetuned-arctic-sentence, finetuned-arctic, modernbert-finqalab-embeddings, Morocco-Darija-Sentence-Embedding-v0.2, multilingual-e5-base-v3.1, multilingual-e5-base-v3, gte-multilingual-base-v2.1-similarity, gte-multilingual-base-v2.1, gte-multilingual-base-v2.0, al-MiniLM-L6-v2, mdeberta-v3-base-sbert, bge-base-financial-matryoshka-abhiram, simcse-4000, simcse-2000, simcse-12000, retriever-v3-2000, my-retriever-4000, my-retriever-3000, my-retriever, distilbert-en-id-qa, mpnet-base-GISTEmbedLoss-MSEE_Evaluator-salestax-docs, bge-small-en-MultiplrRankingLoss-Tax-dataset, bge-small-en-MultiplrRankingLoss-30-Rag-paper-dataset, bge-large-en-v1.5-CosentLoss, bge-large-en-v1.5-AngleLoss-25-Epochs, all-MiniLM-L6-v2_policy_doc_finetune, qwen_emb_6k, qwen_emb_600_best_21.11, qwen7k, qwen3k, qwen23k, qwen1k, qwen11k, qwen10k, trait-embeddings-1, Embedding-v2, Embedding-v1, Embedding-v0, modernbert-embed-quickb-video, modernbert-embed-quickb, ModernBERT-embed-base-legal-MRL, arabic_text_embedding_sts_arabertv02_arabicnlitriplet, Arabic_text_embedding_for_sts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and\\n  lighter</td>\n",
       "      <td>As Transfer Learning from large-scale pre-trained models becomes more\\nprevalent in Natural Language Processing (NLP), operating these large models in\\non-the-edge and/or under constrained computational training or inference\\nbudgets remains challenging. In this work, we propose a method to pre-train a\\nsmaller general-purpose language representation model, called DistilBERT, which\\ncan then be fine-tuned with good performances on a wide range of tasks like its\\nlarger counterparts. While most prior work investigated the use of distillation\\nfor building task-specific models, we leverage knowledge distillation during\\nthe pre-training phase and show that it is possible to reduce the size of a\\nBERT model by 40%, while retaining 97% of its language understanding\\ncapabilities and being 60% faster. To leverage the inductive biases learned by\\nlarger models during pre-training, we introduce a triple loss combining\\nlanguage modeling, distillation and cosine-distance losses. Our smaller, faster\\nand lighter model is cheaper to pre-train and we demonstrate its capabilities\\nfor on-device computations in a proof-of-concept experiment and a comparative\\non-device study.</td>\n",
       "      <td>distilbert-base-uncased-finetuned-squad-d5716d28, sts-distilcamembert-base, distilbert-base-uncased-emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HuggingFace's Transformers: State-of-the-art Natural Language Processing</td>\n",
       "      <td>Recent progress in natural language processing has been driven by advances in\\nboth model architecture and model pretraining. Transformer architectures have\\nfacilitated building higher-capacity models and pretraining has made it\\npossible to effectively utilize this capacity for a wide variety of tasks.\\nTransformers is an open-source library with the goal of opening up\\nthese advances to the wider machine learning community. The library consists of\\ncarefully engineered state-of-the art Transformer architectures under a unified\\nAPI. Backing this library is a curated collection of pretrained models made by\\nand available for the community. Transformers is designed to be\\nextensible by researchers, simple for practitioners, and fast and robust in\\nindustrial deployments. The library is available at\\nhttps://github.com/huggingface/transformers.</td>\n",
       "      <td>FrenchMedMCQA-BioBERT-V1.1-Wikipedia-BM25, FrenchMedMCQA-BART-base-Wikipedia-BM25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language\\n  Generation, Translation, and Comprehension</td>\n",
       "      <td>We present BART, a denoising autoencoder for pretraining sequence-to-sequence\\nmodels. BART is trained by (1) corrupting text with an arbitrary noising\\nfunction, and (2) learning a model to reconstruct the original text. It uses a\\nstandard Tranformer-based neural machine translation architecture which,\\ndespite its simplicity, can be seen as generalizing BERT (due to the\\nbidirectional encoder), GPT (with the left-to-right decoder), and many other\\nmore recent pretraining schemes. We evaluate a number of noising approaches,\\nfinding the best performance by both randomly shuffling the order of the\\noriginal sentences and using a novel in-filling scheme, where spans of text are\\nreplaced with a single mask token. BART is particularly effective when fine\\ntuned for text generation but also works well for comprehension tasks. It\\nmatches the performance of RoBERTa with comparable training resources on GLUE\\nand SQuAD, achieves new state-of-the-art results on a range of abstractive\\ndialogue, question answering, and summarization tasks, with gains of up to 6\\nROUGE. BART also provides a 1.1 BLEU increase over a back-translation system\\nfor machine translation, with only target language pretraining. We also report\\nablation experiments that replicate other pretraining schemes within the BART\\nframework, to better measure which factors most influence end-task performance.</td>\n",
       "      <td>bart-finetuned-samsum, bart-large-xsum, bart-large-nl2sql</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Unsupervised Cross-lingual Representation Learning at Scale</td>\n",
       "      <td>This paper shows that pretraining multilingual language models at scale leads\\nto significant performance gains for a wide range of cross-lingual transfer\\ntasks. We train a Transformer-based masked language model on one hundred\\nlanguages, using more than two terabytes of filtered CommonCrawl data. Our\\nmodel, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a\\nvariety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI,\\n+13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs\\nparticularly well on low-resource languages, improving 15.7% in XNLI accuracy\\nfor Swahili and 11.4% for Urdu over previous XLM models. We also present a\\ndetailed empirical analysis of the key factors that are required to achieve\\nthese gains, including the trade-offs between (1) positive transfer and\\ncapacity dilution and (2) the performance of high and low resource languages at\\nscale. Finally, we show, for the first time, the possibility of multilingual\\nmodeling without sacrificing per-language performance; XLM-R is very\\ncompetitive with strong monolingual models on the GLUE and XNLI benchmarks. We\\nwill make our code, data and models publicly available.</td>\n",
       "      <td>xlm-roberta-base-lora-language-detection, mDeBERTa-v3-base-xnli-multilingual-nli-2mil7, bilingual-embedding-small, bilingual-embedding-large, bilingual-embedding-base, bilingual-document-embedding, xlm-roberta-large-it-mnli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Open Domain Web Keyphrase Extraction Beyond Language Modeling</td>\n",
       "      <td>This paper studies keyphrase extraction in real-world scenarios where\\ndocuments are from diverse domains and have variant content quality. We curate\\nand release OpenKP, a large scale open domain keyphrase extraction dataset with\\nnear one hundred thousand web documents and expert keyphrase annotations. To\\nhandle the variations of domain and content quality, we develop BLING-KPE, a\\nneural keyphrase extraction model that goes beyond language understanding using\\nvisual presentations of documents and weak supervision from search queries.\\nExperimental results on OpenKP confirm the effectiveness of BLING-KPE and the\\ncontributions of its neural architecture, visual features, and search log weak\\nsupervision. Zero-shot evaluations on DUC-2001 demonstrate the improved\\ngeneralization ability of learning from the open domain data compared to a\\nspecific domain.</td>\n",
       "      <td>keyphrase-generation-t5-small-openkp, keyphrase-extraction-kbir-openkp, keyphrase-extraction-distilbert-openkp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CamemBERT: a Tasty French Language Model</td>\n",
       "      <td>Pretrained language models are now ubiquitous in Natural Language Processing.\\nDespite their success, most available models have either been trained on\\nEnglish data or on the concatenation of data in multiple languages. This makes\\npractical use of such models --in all languages except English-- very limited.\\nIn this paper, we investigate the feasibility of training monolingual\\nTransformer-based language models for other languages, taking French as an\\nexample and evaluating our language models on part-of-speech tagging,\\ndependency parsing, named entity recognition and natural language inference\\ntasks. We show that the use of web crawled data is preferable to the use of\\nWikipedia data. More surprisingly, we show that a relatively small web crawled\\ndataset (4GB) leads to results that are as good as those obtained using larger\\ndatasets (130+GB). Our best performing model CamemBERT reaches or improves the\\nstate of the art in all four downstream tasks.</td>\n",
       "      <td>sts-distilcamembert-base, sts-camembert-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>KPTimes: A Large-Scale Dataset for Keyphrase Generation on News\\n  Documents</td>\n",
       "      <td>Keyphrase generation is the task of predicting a set of lexical units that\\nconveys the main content of a source text. Existing datasets for keyphrase\\ngeneration are only readily available for the scholarly domain and include\\nnon-expert annotations. In this paper we present KPTimes, a large-scale dataset\\nof news texts paired with editor-curated keyphrases. Exploring the dataset, we\\nshow how editors tag documents, and how their annotations differ from those\\nfound in existing datasets. We also train and evaluate state-of-the-art neural\\nkeyphrase generation models on KPTimes to gain insights on how well they\\nperform on the news domain. The dataset is available online at\\nhttps://github.com/ygorg/KPTimes .</td>\n",
       "      <td>keyphrase-extraction-kbir-kptimes, keyphrase-extraction-distilbert-kptimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive\\n  Summarization</td>\n",
       "      <td>Recent work pre-training Transformers with self-supervised objectives on\\nlarge text corpora has shown great success when fine-tuned on downstream NLP\\ntasks including text summarization. However, pre-training objectives tailored\\nfor abstractive text summarization have not been explored. Furthermore there is\\na lack of systematic evaluation across diverse domains. In this work, we\\npropose pre-training large Transformer-based encoder-decoder models on massive\\ntext corpora with a new self-supervised objective. In PEGASUS, important\\nsentences are removed/masked from an input document and are generated together\\nas one output sequence from the remaining sentences, similar to an extractive\\nsummary. We evaluated our best PEGASUS model on 12 downstream summarization\\ntasks spanning news, science, stories, instructions, emails, patents, and\\nlegislative bills. Experiments demonstrate it achieves state-of-the-art\\nperformance on all 12 downstream datasets measured by ROUGE scores. Our model\\nalso shows surprising performance on low-resource summarization, surpassing\\nprevious state-of-the-art results on 6 datasets with only 1000 examples.\\nFinally we validated our results using human evaluation and show that our model\\nsummaries achieve human performance on multiple datasets.</td>\n",
       "      <td>financial-summarization-pegasus, pegasus-xsum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework\\n  for Natural Language Generation</td>\n",
       "      <td>Current pre-training works in natural language generation pay little\\nattention to the problem of exposure bias on downstream tasks. To address this\\nissue, we propose an enhanced multi-flow sequence to sequence pre-training and\\nfine-tuning framework named ERNIE-GEN, which bridges the discrepancy between\\ntraining and inference with an infilling generation mechanism and a noise-aware\\ngeneration method. To make generation closer to human writing patterns, this\\nframework introduces a span-by-span generation flow that trains the model to\\npredict semantically-complete spans consecutively rather than predicting word\\nby word. Unlike existing pre-training methods, ERNIE-GEN incorporates\\nmulti-granularity target sampling to construct pre-training data, which\\nenhances the correlation between encoder and decoder. Experimental results\\ndemonstrate that ERNIE-GEN achieves state-of-the-art results with a much\\nsmaller amount of pre-training data and parameters on a range of language\\ngeneration tasks, including abstractive summarization (Gigaword and\\nCNN/DailyMail), question generation (SQuAD), dialogue generation (Persona-Chat)\\nand generative question answering (CoQA).</td>\n",
       "      <td>t5-small-squad-qg-default, t5-large-squad-qg-default, t5-base-squad-qg-default, bart-large-squad-qg-default, bart-base-squad-qg-default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Beat the AI: Investigating Adversarial Human Annotation for Reading\\n  Comprehension</td>\n",
       "      <td>Innovations in annotation methodology have been a catalyst for Reading\\nComprehension (RC) datasets and models. One recent trend to challenge current\\nRC models is to involve a model in the annotation process: humans create\\nquestions adversarially, such that the model fails to answer them correctly. In\\nthis work we investigate this annotation methodology and apply it in three\\ndifferent settings, collecting a total of 36,000 samples with progressively\\nstronger models in the annotation loop. This allows us to explore questions\\nsuch as the reproducibility of the adversarial effect, transfer from data\\ncollected with varying model-in-the-loop strengths, and generalisation to data\\ncollected without a model. We find that training on adversarially collected\\nsamples leads to strong generalisation to non-adversarially collected datasets,\\nyet with progressive performance deterioration with increasingly stronger\\nmodels-in-the-loop. Furthermore, we find that stronger models can still learn\\nfrom datasets collected with substantially weaker models-in-the-loop. When\\ntrained on data collected with a BiDAF model in the loop, RoBERTa achieves\\n39.9F1 on questions that it cannot answer when trained on SQuAD - only\\nmarginally lower than when trained on data collected using RoBERTa itself\\n(41.0F1).</td>\n",
       "      <td>roberta-large-synqa-ext, roberta-large-synqa, electra-large-synqa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Making Monolingual Sentence Embeddings Multilingual using Knowledge\\n  Distillation</td>\n",
       "      <td>We present an easy and efficient method to extend existing sentence embedding\\nmodels to new languages. This allows to create multilingual versions from\\npreviously monolingual models. The training is based on the idea that a\\ntranslated sentence should be mapped to the same location in the vector space\\nas the original sentence. We use the original (monolingual) model to generate\\nsentence embeddings for the source language and then train a new system on\\ntranslated sentences to mimic the original model. Compared to other methods for\\ntraining multilingual sentence embeddings, this approach has several\\nadvantages: It is easy to extend existing models with relatively few samples to\\nnew languages, it is easier to ensure desired properties for the vector space,\\nand the hardware requirements for training is lower. We demonstrate the\\neffectiveness of our approach for 50+ languages from various language families.\\nCode to extend sentence embeddings models to more than 400 languages is\\npublicly available.</td>\n",
       "      <td>rubert-tiny2-distilled-from-LaBSE-en-ru, LaBSE-en-ru-distilled-each-third-layer, b1ade-embed-distilled-from-gte-large-en-v1.5, xlm-roberta-base-multilingual-en-es, xlm-roberta-base-multilingual-en-ar-fr-de-es-tr-it, TinyBERT_L-4_H-312_v2-distilled-from-stsb-roberta-base-v2, SRBedding-base-distilled-v1, student-multilang-XLMR-14jun, makeML-snowflake, xlm-roberta-base-multilingual-mkqa, bert-base-multilingual-uncased-matryoshka-mkqa, bert-base-multilingual-cased-matryoshka-mkqa, all-miniLM-L6-en-ja, all-MiniLM-L6-multilingual-v2-en-es-pt-pt-br-v2, all-MiniLM-L6-v2-nepali, minilm-bo, Indic_Bert-8-layers, bge-m3-distill-8l, indic-mxbai-L8-embed, d-mxbai-L8-embed, bge-m3-8-layers, all-MiniLM-L6-v2-8-layers, distilbert-en-id-qa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Don't Stop Pretraining: Adapt Language Models to Domains and Tasks</td>\n",
       "      <td>Language models pretrained on text from a wide variety of sources form the\\nfoundation of today's NLP. In light of the success of these broad-coverage\\nmodels, we investigate whether it is still helpful to tailor a pretrained model\\nto the domain of a target task. We present a study across four domains\\n(biomedical and computer science publications, news, and reviews) and eight\\nclassification tasks, showing that a second phase of pretraining in-domain\\n(domain-adaptive pretraining) leads to performance gains, under both high- and\\nlow-resource settings. Moreover, adapting to the task's unlabeled data\\n(task-adaptive pretraining) improves performance even after domain-adaptive\\npretraining. Finally, we show that adapting to a task corpus augmented using\\nsimple data selection strategies is an effective alternative, especially when\\nresources for domain-adaptive pretraining might be unavailable. Overall, we\\nconsistently find that multi-phase adaptive pretraining offers large gains in\\ntask performance.</td>\n",
       "      <td>bert-large-uncased-sentiment, bert-base-uncased-yelp-reviews, bert-base-uncased-yelp-polarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Language Models are Few-Shot Learners</td>\n",
       "      <td>Recent work has demonstrated substantial gains on many NLP tasks and\\nbenchmarks by pre-training on a large corpus of text followed by fine-tuning on\\na specific task. While typically task-agnostic in architecture, this method\\nstill requires task-specific fine-tuning datasets of thousands or tens of\\nthousands of examples. By contrast, humans can generally perform a new language\\ntask from only a few examples or from simple instructions - something which\\ncurrent NLP systems still largely struggle to do. Here we show that scaling up\\nlanguage models greatly improves task-agnostic, few-shot performance, sometimes\\neven reaching competitiveness with prior state-of-the-art fine-tuning\\napproaches. Specifically, we train GPT-3, an autoregressive language model with\\n175 billion parameters, 10x more than any previous non-sparse language model,\\nand test its performance in the few-shot setting. For all tasks, GPT-3 is\\napplied without any gradient updates or fine-tuning, with tasks and few-shot\\ndemonstrations specified purely via text interaction with the model. GPT-3\\nachieves strong performance on many NLP datasets, including translation,\\nquestion-answering, and cloze tasks, as well as several tasks that require\\non-the-fly reasoning or domain adaptation, such as unscrambling words, using a\\nnovel word in a sentence, or performing 3-digit arithmetic. At the same time,\\nwe also identify some datasets where GPT-3's few-shot learning still struggles,\\nas well as some datasets where GPT-3 faces methodological issues related to\\ntraining on large web corpora. Finally, we find that GPT-3 can generate samples\\nof news articles which human evaluators have difficulty distinguishing from\\narticles written by humans. We discuss broader societal impacts of this finding\\nand of GPT-3 in general.</td>\n",
       "      <td>opt-66b_eval, opt-6.7b_eval, opt-350m_eval, opt-30b_eval, opt-2.7b_eval, opt-13b_eval, opt-125m_eval, opt-1.3b_eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>DeBERTa: Decoding-enhanced BERT with Disentangled Attention</td>\n",
       "      <td>Recent progress in pre-trained neural language models has significantly\\nimproved the performance of many natural language processing (NLP) tasks. In\\nthis paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT\\nwith disentangled attention) that improves the BERT and RoBERTa models using\\ntwo novel techniques. The first is the disentangled attention mechanism, where\\neach word is represented using two vectors that encode its content and\\nposition, respectively, and the attention weights among words are computed\\nusing disentangled matrices on their contents and relative positions,\\nrespectively. Second, an enhanced mask decoder is used to incorporate absolute\\npositions in the decoding layer to predict the masked tokens in model\\npre-training. In addition, a new virtual adversarial training method is used\\nfor fine-tuning to improve models' generalization. We show that these\\ntechniques significantly improve the efficiency of model pre-training and the\\nperformance of both natural language understanding (NLU) and natural langauge\\ngeneration (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model\\ntrained on half of the training data performs consistently better on a wide\\nrange of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%),\\non SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).\\nNotably, we scale up DeBERTa by training a larger version that consists of 48\\nTransform layers with 1.5 billion parameters. The significant performance boost\\nmakes the single DeBERTa model surpass the human performance on the SuperGLUE\\nbenchmark (Wang et al., 2019a) for the first time in terms of macro-average\\nscore (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the\\nSuperGLUE leaderboard as of January 6, 2021, out performing the human baseline\\nby a decent margin (90.3 versus 89.8).</td>\n",
       "      <td>deberta-v3-small-finetuned-mnli, deberta-v3-small-finetuned-cola, deberta-v3-large-finetuned-mnli, DeBERTa-v3-base-mnli-fever-anli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Visual Transformers: Token-based Image Representation and Processing for\\n  Computer Vision</td>\n",
       "      <td>Computer vision has achieved remarkable success by (a) representing images as\\nuniformly-arranged pixel arrays and (b) convolving highly-localized features.\\nHowever, convolutions treat all image pixels equally regardless of importance;\\nexplicitly model all concepts across all images, regardless of content; and\\nstruggle to relate spatially-distant concepts. In this work, we challenge this\\nparadigm by (a) representing images as semantic visual tokens and (b) running\\ntransformers to densely model token relationships. Critically, our Visual\\nTransformer operates in a semantic token space, judiciously attending to\\ndifferent image parts based on context. This is in sharp contrast to\\npixel-space transformers that require orders-of-magnitude more compute. Using\\nan advanced training recipe, our VTs significantly outperform their\\nconvolutional counterparts, raising ResNet accuracy on ImageNet top-1 by 4.6 to\\n7 points while using fewer FLOPs and parameters. For semantic segmentation on\\nLIP and COCO-stuff, VT-based feature pyramid networks (FPN) achieve 0.35 points\\nhigher mIoU while reducing the FPN module's FLOPs by 6.5x.</td>\n",
       "      <td>vit_base-224-in21k-ft-cifar100, vit_base-224-in21k-ft-cifar10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\\n  Representations</td>\n",
       "      <td>We show for the first time that learning powerful representations from speech\\naudio alone followed by fine-tuning on transcribed speech can outperform the\\nbest semi-supervised methods while being conceptually simpler. wav2vec 2.0\\nmasks the speech input in the latent space and solves a contrastive task\\ndefined over a quantization of the latent representations which are jointly\\nlearned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER\\non the clean/other test sets. When lowering the amount of labeled data to one\\nhour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour\\nsubset while using 100 times less labeled data. Using just ten minutes of\\nlabeled data and pre-training on 53k hours of unlabeled data still achieves\\n4.8/8.2 WER. This demonstrates the feasibility of speech recognition with\\nlimited amounts of labeled data.</td>\n",
       "      <td>wav2vec2-large-960h-lv60, wav2vec2-xls-r-300m-sami-parl-ext-ft, wav2vec2-xls-r-300m-sami-parl-direct-ft, wav2vec2-xls-r-300m-sami-parl-cont-pt-20h, wav2vec2-xls-r-300m-sami-parl-cont-pt-108h, wav2vec2-large-uralic-voxpopuli-v2-sami-parl-ext-ft, wav2vec2-large-uralic-voxpopuli-v2-sami-parl-direct-ft, wav2vec2-large-uralic-voxpopuli-v2-sami-parl-cont-pt-20h, wav2vec2-large-uralic-voxpopuli-v2-sami-parl-cont-pt-108h, wav2vec2-large-uralic-voxpopuli-v2-1500h, wav2vec2-large-uralic-voxpopuli-v2-100h, wav2vec2-large-sami-cont-pt-22k-finetuned, wav2vec2-large-sami-22k-finetuned, wav2vec2-large-fi-lp-from-scratch-1500h, wav2vec2-large-fi-lp-from-scratch-100h, wav2vec2-large-fi-lp-cont-pt-1500h, wav2vec2-large-fi-lp-cont-pt-100h, wav2vec2-large-fi-150k-finetuned, wav2vec2-base-sami-cont-pt-22k-finetuned, wav2vec2-base-sami-22k-finetuned, wav2vec2-base-fi-voxpopuli-v2-sami-parl-ext-ft, wav2vec2-base-fi-voxpopuli-v2-sami-parl-direct-ft, wav2vec2-base-fi-voxpopuli-v2-sami-parl-cont-pt-20h, wav2vec2-base-fi-voxpopuli-v2-sami-parl-cont-pt-108h, wav2vec2-base-fi-voxpopuli-v2-1500h, wav2vec2-base-fi-voxpopuli-v2-100h, wav2vec2-base-fi-lp-from-scratch-1500h, wav2vec2-base-fi-lp-from-scratch-100h, wav2vec2-base-fi-lp-cont-pt-1500h, wav2vec2-base-fi-lp-cont-pt-100h, wav2vec2-base-fi-150k-finetuned, wav2vec2-large-uralic-voxpopuli-v2-finnish, wav2vec2-base-fi-voxpopuli-v2-finetuned, wav2vec2-large-xlsr-vietnamese, wav2vec2-french-phonemizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Leveraging Passage Retrieval with Generative Models for Open Domain\\n  Question Answering</td>\n",
       "      <td>Generative models for open domain question answering have proven to be\\ncompetitive, without resorting to external knowledge. While promising, this\\napproach requires to use models with billions of parameters, which are\\nexpensive to train and query. In this paper, we investigate how much these\\nmodels can benefit from retrieving text passages, potentially containing\\nevidence. We obtain state-of-the-art results on the Natural Questions and\\nTriviaQA open benchmarks. Interestingly, we observe that the performance of\\nthis method significantly improves when increasing the number of retrieved\\npassages. This is evidence that generative models are good at aggregating and\\ncombining evidence from multiple passages.</td>\n",
       "      <td>fid_t5_large_nq, fid_flan_t5_base_nq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Improving Efficient Neural Ranking Models with Cross-Architecture\\n  Knowledge Distillation</td>\n",
       "      <td>Retrieval and ranking models are the backbone of many applications such as\\nweb search, open domain QA, or text-based recommender systems. The latency of\\nneural ranking models at query time is largely dependent on the architecture\\nand deliberate choices by their designers to trade-off effectiveness for higher\\nefficiency. This focus on low query latency of a rising number of efficient\\nranking architectures make them feasible for production deployment. In machine\\nlearning an increasingly common approach to close the effectiveness gap of more\\nefficient models is to apply knowledge distillation from a large teacher model\\nto a smaller student model. We find that different ranking architectures tend\\nto produce output scores in different magnitudes. Based on this finding, we\\npropose a cross-architecture training procedure with a margin focused loss\\n(Margin-MSE), that adapts knowledge distillation to the varying score output\\ndistributions of different BERT and non-BERT passage ranking architectures. We\\napply the teachable information as additional fine-grained labels to existing\\ntraining triples of the MSMARCO-Passage collection. We evaluate our procedure\\nof distilling knowledge from state-of-the-art concatenated BERT models to four\\ndifferent efficient architectures (TK, ColBERT, PreTT, and a BERT CLS dot\\nproduct model). We show that across our evaluated architectures our Margin-MSE\\nknowledge distillation significantly improves re-ranking effectiveness without\\ncompromising their efficiency. Additionally, we show our general distillation\\nmethod to improve nearest neighbor based index retrieval with the BERT dot\\nproduct model, offering competitive results with specialized and much more\\ncostly training methods. To benefit the community, we publish the teacher-score\\ntraining files in a ready-to-use package.</td>\n",
       "      <td>mmarco-Arabic-mMiniLML-bi-encoder-NoKD-v1, mmarco-Arabic-mMiniLML-bi-encoder-KD-v1, mmarco-Arabic-AraElectra-bi-encoder-NoKD-v1, mmarco-Arabic-AraElectra-bi-encoder-KD-v1, mmarco-Arabic-AraDPR-bi-encoder-NoKD-v1, mmarco-Arabic-AraDPR-bi-encoder-KD-v1, DeBERTaV3-small-GeneralSentenceTransformer-v2-checkpoints-tmp, xlm-roberta-base-msmarco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>fairseq S2T: Fast Speech-to-Text Modeling with fairseq</td>\n",
       "      <td>We introduce fairseq S2T, a fairseq extension for speech-to-text (S2T)\\nmodeling tasks such as end-to-end speech recognition and speech-to-text\\ntranslation. It follows fairseq's careful design for scalability and\\nextensibility. We provide end-to-end workflows from data pre-processing, model\\ntraining to offline (online) inference. We implement state-of-the-art\\nRNN-based, Transformer-based as well as Conformer-based models and open-source\\ndetailed training recipes. Fairseq's machine translation models and language\\nmodels can be seamlessly integrated into S2T workflows for multi-task learning\\nor transfer learning. Fairseq S2T documentation and examples are available at\\nhttps://github.com/pytorch/fairseq/tree/master/examples/speech_to_text.</td>\n",
       "      <td>wav2vec2-conformer-rope-large-960h-ft, wav2vec2-conformer-rel-pos-large-960h-ft, s2t-large-librispeech-asr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MLS: A Large-Scale Multilingual Dataset for Speech Research</td>\n",
       "      <td>This paper introduces Multilingual LibriSpeech (MLS) dataset, a large\\nmultilingual corpus suitable for speech research. The dataset is derived from\\nread audiobooks from LibriVox and consists of 8 languages, including about\\n44.5K hours of English and a total of about 6K hours for other languages.\\nAdditionally, we provide Language Models (LM) and baseline Automatic Speech\\nRecognition (ASR) models and for all the languages in our dataset. We believe\\nsuch a large transcribed dataset will open new avenues in ASR and\\nText-To-Speech (TTS) research. The dataset will be made freely available for\\nanyone at http://www.openslr.org.</td>\n",
       "      <td>wav2vec2-large-xlsr-open-brazilian-portuguese-v2, bp500-xlsr, bp500-base10k_voxpopuli, bp400-xlsr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Switch Transformers: Scaling to Trillion Parameter Models with Simple\\n  and Efficient Sparsity</td>\n",
       "      <td>In deep learning, models typically reuse the same parameters for all inputs.\\nMixture of Experts (MoE) defies this and instead selects different parameters\\nfor each incoming example. The result is a sparsely-activated model -- with\\noutrageous numbers of parameters -- but a constant computational cost. However,\\ndespite several notable successes of MoE, widespread adoption has been hindered\\nby complexity, communication costs and training instability -- we address these\\nwith the Switch Transformer. We simplify the MoE routing algorithm and design\\nintuitive improved models with reduced communication and computational costs.\\nOur proposed training techniques help wrangle the instabilities and we show\\nlarge sparse models may be trained, for the first time, with lower precision\\n(bfloat16) formats. We design models based off T5-Base and T5-Large to obtain\\nup to 7x increases in pre-training speed with the same computational resources.\\nThese improvements extend into multilingual settings where we measure gains\\nover the mT5-Base version across all 101 languages. Finally, we advance the\\ncurrent scale of language models by pre-training up to trillion parameter\\nmodels on the \"Colossal Clean Crawled Corpus\" and achieve a 4x speedup over the\\nT5-XXL model.</td>\n",
       "      <td>BurningBruce-005, BurningBruce-004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup</td>\n",
       "      <td>Contrastive learning has been applied successfully to learn vector\\nrepresentations of text. Previous research demonstrated that learning\\nhigh-quality representations benefits from batch-wise contrastive loss with a\\nlarge number of negatives. In practice, the technique of in-batch negative is\\nused, where for each example in a batch, other batch examples' positives will\\nbe taken as its negatives, avoiding encoding extra negatives. This, however,\\nstill conditions each example's loss on all batch examples and requires fitting\\nthe entire large batch into GPU memory. This paper introduces a gradient\\ncaching technique that decouples backpropagation between contrastive loss and\\nthe encoder, removing encoder backward pass data dependency along the batch\\ndimension. As a result, gradients can be computed for one subset of the batch\\nat a time, leading to almost constant memory usage.</td>\n",
       "      <td>modernbert-bio-v0.1, mpnet-base-nq-prompts, mpnet-base-nq, mpnet-base-gooaq-cmnrl-mrl, bert-base-uncased-gooaq, bert-base-nq-prompts-exclude-pooling-prompts, bert-base-nq-prompts, bert-base-nq, ModernBERT-base-gooaq, ModernBERT-korean-large-preview, xlm-roberta-small-all-nli-triplet, vn_biencoder_CachedMultipleNegativesRankingLoss, ModernBERT-large-DPR-msmarco, ModernBERT-base-DPR-msmarco, esci-nomic-embed-text-v1_5, gte-en-mlm-base-msmarco, ModernBERT-large-msmarco, ModernBERT-base-msmarco, ModernBERT-base-test, multilingual-e5-g39, multilingual-e5-base-trimm-vocab-1024-v3, multilingual-e5-base-trimm-vocab-1024-v2, gte-base-v1__trim_vocab-1024, gte-base-v0__trim_vocab-1024, ModernBERT-base-DPR-fullneg-gte-0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged\\n  Gradient Method for Stochastic Optimization</td>\n",
       "      <td>We introduce MADGRAD, a novel optimization method in the family of AdaGrad\\nadaptive gradient methods. MADGRAD shows excellent performance on deep learning\\noptimization problems from multiple fields, including classification and\\nimage-to-image tasks in vision, and recurrent and bidirectionally-masked models\\nin natural language processing. For each of these tasks, MADGRAD matches or\\noutperforms both SGD and ADAM in test set performance, even on problems for\\nwhich adaptive methods normally perform poorly.</td>\n",
       "      <td>drugtargetpred-chemselfies, chemselfies-base-bertmlm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Vision Transformers for Dense Prediction</td>\n",
       "      <td>We introduce dense vision transformers, an architecture that leverages vision\\ntransformers in place of convolutional networks as a backbone for dense\\nprediction tasks. We assemble tokens from various stages of the vision\\ntransformer into image-like representations at various resolutions and\\nprogressively combine them into full-resolution predictions using a\\nconvolutional decoder. The transformer backbone processes representations at a\\nconstant and relatively high resolution and has a global receptive field at\\nevery stage. These properties allow the dense vision transformer to provide\\nfiner-grained and more globally coherent predictions when compared to\\nfully-convolutional networks. Our experiments show that this architecture\\nyields substantial improvements on dense prediction tasks, especially when a\\nlarge amount of training data is available. For monocular depth estimation, we\\nobserve an improvement of up to 28% in relative performance when compared to a\\nstate-of-the-art fully-convolutional network. When applied to semantic\\nsegmentation, dense vision transformers set a new state of the art on ADE20K\\nwith 49.02% mIoU. We further show that the architecture can be fine-tuned on\\nsmaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets\\nthe new state of the art. Our models are available at\\nhttps://github.com/intel-isl/DPT.</td>\n",
       "      <td>ldm3d-4c, ldm3d, dpt-hybrid-midas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Citrinet: Closing the Gap between Non-Autoregressive and Autoregressive\\n  End-to-End Models for Automatic Speech Recognition</td>\n",
       "      <td>We propose Citrinet - a new end-to-end convolutional Connectionist Temporal\\nClassification (CTC) based automatic speech recognition (ASR) model. Citrinet\\nis deep residual neural model which uses 1D time-channel separable convolutions\\ncombined with sub-word encoding and squeeze-and-excitation. The resulting\\narchitecture significantly reduces the gap between non-autoregressive and\\nsequence-to-sequence and transducer models. We evaluate Citrinet on\\nLibriSpeech, TED-LIUM2, AISHELL-1 and Multilingual LibriSpeech (MLS) English\\nspeech datasets. Citrinet accuracy on these datasets is close to the best\\nautoregressive Transducer models.</td>\n",
       "      <td>stt_zh_citrinet_1024_gamma_0_25, stt_uk_citrinet_1024_gamma_0_25, stt_en_citrinet_768_ls, stt_en_citrinet_512_ls, stt_en_citrinet_384_ls, stt_en_citrinet_256_ls, stt_en_citrinet_1024_gamma_0_25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Towards Measuring Fairness in AI: the Casual Conversations Dataset</td>\n",
       "      <td>This paper introduces a novel dataset to help researchers evaluate their\\ncomputer vision and audio models for accuracy across a diverse set of age,\\ngenders, apparent skin tones and ambient lighting conditions. Our dataset is\\ncomposed of 3,011 subjects and contains over 45,000 videos, with an average of\\n15 videos per person. The videos were recorded in multiple U.S. states with a\\ndiverse set of adults in various age, gender and apparent skin tone groups. A\\nkey feature is that each subject agreed to participate for their likenesses to\\nbe used. Additionally, our age and gender annotations are provided by the\\nsubjects themselves. A group of trained annotators labeled the subjects'\\napparent skin tone using the Fitzpatrick skin type scale. Moreover, annotations\\nfor videos recorded in low ambient lighting are also provided. As an\\napplication to measure robustness of predictions across certain attributes, we\\nprovide a comprehensive study on the top five winners of the DeepFake Detection\\nChallenge (DFDC). Experimental evaluation shows that the winning models are\\nless performant on some specific groups of people, such as subjects with darker\\nskin tones and thus may not generalize to all people. In addition, we also\\nevaluate the state-of-the-art apparent age and gender classification methods.\\nOur experiments provides a thorough analysis on these models in terms of fair\\ntreatment of people from various backgrounds.</td>\n",
       "      <td>parakeet-tdt_ctc-110m, parakeet-tdt_ctc-1.1b, parakeet-tdt-1.1b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for\\n  Unsupervised Sentence Embedding Learning</td>\n",
       "      <td>Learning sentence embeddings often requires a large amount of labeled data.\\nHowever, for most tasks and domains, labeled data is seldom available and\\ncreating it is expensive. In this work, we present a new state-of-the-art\\nunsupervised method based on pre-trained Transformers and Sequential Denoising\\nAuto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points.\\nIt can achieve up to 93.1% of the performance of in-domain supervised\\napproaches. Further, we show that TSDAE is a strong domain adaptation and\\npre-training method for sentence embeddings, significantly outperforming other\\napproaches like Masked Language Model.\\n  A crucial shortcoming of previous studies is the narrow evaluation: Most work\\nmainly evaluates on the single task of Semantic Textual Similarity (STS), which\\ndoes not require any domain knowledge. It is unclear if these proposed methods\\ngeneralize to other domains and tasks. We fill this gap and evaluate TSDAE and\\nother recent approaches on four different datasets from heterogeneous domains.</td>\n",
       "      <td>bert-base-uncased-tsdae-askubuntu, bert-base-uncased-stsb-tsdae, bge-base-space-mt-tsdae, RoBERTa-base-unsupervised-TSDAE, E5-base-unsupervised-TSDAE-2, E5-base-unsupervised-TSDAE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information\\n  Retrieval Models</td>\n",
       "      <td>Existing neural information retrieval (IR) models have often been studied in\\nhomogeneous and narrow settings, which has considerably limited insights into\\ntheir out-of-distribution (OOD) generalization capabilities. To address this,\\nand to facilitate researchers to broadly evaluate the effectiveness of their\\nmodels, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous\\nevaluation benchmark for information retrieval. We leverage a careful selection\\nof 18 publicly available datasets from diverse text retrieval tasks and domains\\nand evaluate 10 state-of-the-art retrieval systems including lexical, sparse,\\ndense, late-interaction and re-ranking architectures on the BEIR benchmark. Our\\nresults show BM25 is a robust baseline and re-ranking and\\nlate-interaction-based models on average achieve the best zero-shot\\nperformances, however, at high computational costs. In contrast, dense and\\nsparse-retrieval models are computationally more efficient but often\\nunderperform other approaches, highlighting the considerable room for\\nimprovement in their generalization capabilities. We hope this framework allows\\nus to better evaluate and understand existing retrieval systems, and\\ncontributes to accelerating progress towards better robust and generalizable\\nsystems in the future. BEIR is publicly available at\\nhttps://github.com/UKPLab/beir.</td>\n",
       "      <td>Linq-Embed-Mistral, speed-embedding-7b-instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Improving Question Answering Model Robustness with Synthetic Adversarial\\n  Data Generation</td>\n",
       "      <td>Despite recent progress, state-of-the-art question answering models remain\\nvulnerable to a variety of adversarial attacks. While dynamic adversarial data\\ncollection, in which a human annotator tries to write examples that fool a\\nmodel-in-the-loop, can improve model robustness, this process is expensive\\nwhich limits the scale of the collected data. In this work, we are the first to\\nuse synthetic adversarial data generation to make question answering models\\nmore robust to human adversaries. We develop a data generation pipeline that\\nselects source passages, identifies candidate answers, generates questions,\\nthen finally filters or re-labels them to improve quality. Using this approach,\\nwe amplify a smaller human-written adversarial dataset to a much larger set of\\nsynthetic question-answer pairs. By incorporating our synthetic data, we\\nimprove the state-of-the-art on the AdversarialQA dataset by 3.7F1 and improve\\nmodel generalisation on nine of the twelve MRQA datasets. We further conduct a\\nnovel human-in-the-loop evaluation to show that our models are considerably\\nmore robust to new human-written adversarial examples: crowdworkers can fool\\nour model only 8.8% of the time on average, compared to 17.6% for a model\\ntrained without synthetic data.</td>\n",
       "      <td>roberta-large-synqa-ext, roberta-large-synqa, electra-large-synqa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>RoFormer: Enhanced Transformer with Rotary Position Embedding</td>\n",
       "      <td>Position encoding recently has shown effective in the transformer\\narchitecture. It enables valuable supervision for dependency modeling between\\nelements at different positions of the sequence. In this paper, we first\\ninvestigate various methods to integrate positional information into the\\nlearning process of transformer-based language models. Then, we propose a novel\\nmethod named Rotary Position Embedding(RoPE) to effectively leverage the\\npositional information. Specifically, the proposed RoPE encodes the absolute\\nposition with a rotation matrix and meanwhile incorporates the explicit\\nrelative position dependency in self-attention formulation. Notably, RoPE\\nenables valuable properties, including the flexibility of sequence length,\\ndecaying inter-token dependency with increasing relative distances, and the\\ncapability of equipping the linear self-attention with relative position\\nencoding. Finally, we evaluate the enhanced transformer with rotary position\\nembedding, also called RoFormer, on various long text classification benchmark\\ndatasets. Our experiments show that it consistently overcomes its alternatives.\\nFurthermore, we provide a theoretical analysis to explain some experimental\\nresults. RoFormer is already integrated into Huggingface:\\nhttps://huggingface.co/docs/transformers/model_doc/roformer.</td>\n",
       "      <td>Llama-2-70B-Instruct-v0.1, snowflake-arctic-embed-m-long, DeciLM-6b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>FNet: Mixing Tokens with Fourier Transforms</td>\n",
       "      <td>We show that Transformer encoder architectures can be sped up, with limited\\naccuracy costs, by replacing the self-attention sublayers with simple linear\\ntransformations that \"mix\" input tokens. These linear mixers, along with\\nstandard nonlinearities in feed-forward layers, prove competent at modeling\\nsemantic relationships in several text classification tasks. Most surprisingly,\\nwe find that replacing the self-attention sublayer in a Transformer encoder\\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the\\naccuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on\\nGPUs and 70% faster on TPUs at standard 512 input lengths. At longer input\\nlengths, our FNet model is significantly faster: when compared to the\\n\"efficient\" Transformers on the Long Range Arena benchmark, FNet matches the\\naccuracy of the most accurate models, while outpacing the fastest models across\\nall sequence lengths on GPUs (and across relatively shorter lengths on TPUs).\\nFinally, FNet has a light memory footprint and is particularly efficient at\\nsmaller model sizes; for a fixed speed and accuracy budget, small FNet models\\noutperform Transformer counterparts.</td>\n",
       "      <td>fnet-base-finetuned-stsb, fnet-base-finetuned-sst2, fnet-base-finetuned-rte, fnet-base-finetuned-qqp, fnet-base-finetuned-qnli, fnet-base-finetuned-mrpc, fnet-base-finetuned-mnli, fnet-base-finetuned-cola, bert-base-cased-finetuned-stsb, bert-base-cased-finetuned-qqp, bert-base-cased-finetuned-qnli, bert-base-cased-finetuned-mrpc, bert-base-cased-finetuned-mnli, bert-base-cased-finetuned-cola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>BookSum: A Collection of Datasets for Long-form Narrative Summarization</td>\n",
       "      <td>The majority of available text summarization datasets include short-form\\nsource documents that lack long-range causal and temporal dependencies, and\\noften contain strong layout and stylistic biases. While relevant, such datasets\\nwill offer limited challenges for future generations of text summarization\\nsystems. We address these issues by introducing BookSum, a collection of\\ndatasets for long-form narrative summarization. Our dataset covers source\\ndocuments from the literature domain, such as novels, plays and stories, and\\nincludes highly abstractive, human written summaries on three levels of\\ngranularity of increasing difficulty: paragraph-, chapter-, and book-level. The\\ndomain and structure of our dataset poses a unique set of challenges for\\nsummarization systems, which include: processing very long documents,\\nnon-trivial causal and temporal dependencies, and rich discourse structures. To\\nfacilitate future work, we trained and evaluated multiple extractive and\\nabstractive summarization models as baselines for our dataset.</td>\n",
       "      <td>long-t5-tglobal-xl-16384-book-summary, led-large-book-summary, bigbird-pegasus-large-K-booksum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>SpeechBrain: A General-Purpose Speech Toolkit</td>\n",
       "      <td>SpeechBrain is an open-source and all-in-one speech toolkit. It is designed\\nto facilitate the research and development of neural speech processing\\ntechnologies by being simple, flexible, user-friendly, and well-documented.\\nThis paper describes the core architecture designed to support several tasks of\\ncommon interest, allowing users to naturally conceive, compare and share novel\\nspeech processing pipelines. SpeechBrain achieves competitive or\\nstate-of-the-art performance in a wide range of speech benchmarks. It also\\nprovides training recipes, pretrained models, and inference scripts for popular\\nspeech datasets, as well as tutorials which allow anyone with basic Python\\nproficiency to familiarize themselves with speech technologies.</td>\n",
       "      <td>sepformer-dns4-16k-enhancement, asr-wav2vec2-librispeech, asr-wav2vec2-commonvoice-14-zh-CN, asr-wav2vec2-commonvoice-14-rw, asr-wav2vec2-commonvoice-14-pt, asr-wav2vec2-commonvoice-14-it, asr-wav2vec2-commonvoice-14-fr, asr-wav2vec2-commonvoice-14-es, asr-wav2vec2-commonvoice-14-en, asr-wav2vec2-commonvoice-14-de, asr-wav2vec2-commonvoice-14-ar, asr-transformer-transformerlm-librispeech, asr-streaming-conformer-librispeech, asr-streaming-conformer-gigaspeech, asr-conformersmall-transformerlm-librispeech, asr-conformer-transformerlm-librispeech, asr-wav2vec2-commonvoice-es-finetuned-rtve, asr-wav2vec2-commonvoice-es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>HuBERT: Self-Supervised Speech Representation Learning by Masked\\n  Prediction of Hidden Units</td>\n",
       "      <td>Self-supervised approaches for speech representation learning are challenged\\nby three unique problems: (1) there are multiple sound units in each input\\nutterance, (2) there is no lexicon of input sound units during the pre-training\\nphase, and (3) sound units have variable lengths with no explicit segmentation.\\nTo deal with these three problems, we propose the Hidden-Unit BERT (HuBERT)\\napproach for self-supervised speech representation learning, which utilizes an\\noffline clustering step to provide aligned target labels for a BERT-like\\nprediction loss. A key ingredient of our approach is applying the prediction\\nloss over the masked regions only, which forces the model to learn a combined\\nacoustic and language model over the continuous inputs. HuBERT relies primarily\\non the consistency of the unsupervised clustering step rather than the\\nintrinsic quality of the assigned cluster labels. Starting with a simple\\nk-means teacher of 100 clusters, and using two iterations of clustering, the\\nHuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0\\nperformance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with\\n10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model,\\nHuBERT shows up to 19% and 13% relative WER reduction on the more challenging\\ndev-other and test-other evaluation subsets.</td>\n",
       "      <td>hubert-xlarge-ls960-ft, hubert-large-ls960-ft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>LoRA: Low-Rank Adaptation of Large Language Models</td>\n",
       "      <td>An important paradigm of natural language processing consists of large-scale\\npre-training on general domain data and adaptation to particular tasks or\\ndomains. As we pre-train larger models, full fine-tuning, which retrains all\\nmodel parameters, becomes less feasible. Using GPT-3 175B as an example --\\ndeploying independent instances of fine-tuned models, each with 175B\\nparameters, is prohibitively expensive. We propose Low-Rank Adaptation, or\\nLoRA, which freezes the pre-trained model weights and injects trainable rank\\ndecomposition matrices into each layer of the Transformer architecture, greatly\\nreducing the number of trainable parameters for downstream tasks. Compared to\\nGPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable\\nparameters by 10,000 times and the GPU memory requirement by 3 times. LoRA\\nperforms on-par or better than fine-tuning in model quality on RoBERTa,\\nDeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher\\ntraining throughput, and, unlike adapters, no additional inference latency. We\\nalso provide an empirical investigation into rank-deficiency in language model\\nadaptation, which sheds light on the efficacy of LoRA. We release a package\\nthat facilitates the integration of LoRA with PyTorch models and provide our\\nimplementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at\\nhttps://github.com/microsoft/LoRA.</td>\n",
       "      <td>Falcon-40B-Chat-v0.1, FFusionXL-BASE, DeciLM-6b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>panda-gym: Open-source goal-conditioned environments for robotic\\n  learning</td>\n",
       "      <td>This paper presents panda-gym, a set of Reinforcement Learning (RL)\\nenvironments for the Franka Emika Panda robot integrated with OpenAI Gym. Five\\ntasks are included: reach, push, slide, pick &amp; place and stack. They all follow\\na Multi-Goal RL framework, allowing to use goal-oriented RL algorithms. To\\nfoster open-research, we chose to use the open-source physics engine PyBullet.\\nThe implementation chosen for this package allows to define very easily new\\ntasks or new robots. This paper also presents a baseline of results obtained\\nwith state-of-the-art model-free off-policy algorithms. panda-gym is\\nopen-source and freely available at https://github.com/qgallouedec/panda-gym.</td>\n",
       "      <td>A2C-PandaReachDense-v2, tqc-PandaStack-v1, tqc-PandaSlide-v1, tqc-PandaReach-v1, tqc-PandaPush-v1, tqc-PandaPickAndPlace-v1, a2c-PandaReachDense-v2g, a2c-PandaReachDense-v2f, tqc-PandaSlide-v1-2035054536, basic-a2c-PandaReachDense-v2, a2c-PandaReachDense-v2-1, TQC-PandaReachDense-v2, tqc-PandaSlide-v3, tpc-PandaReachDense-v2, a2c-PandaReachDense-v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Ranger21: a synergistic deep learning optimizer</td>\n",
       "      <td>As optimizers are critical to the performances of neural networks, every year\\na large number of papers innovating on the subject are published. However,\\nwhile most of these publications provide incremental improvements to existing\\nalgorithms, they tend to be presented as new optimizers rather than composable\\nalgorithms. Thus, many worthwhile improvements are rarely seen out of their\\ninitial publication. Taking advantage of this untapped potential, we introduce\\nRanger21, a new optimizer which combines AdamW with eight components, carefully\\nselected after reviewing and testing ideas from the literature. We found that\\nthe resulting optimizer provides significantly improved validation accuracy and\\ntraining speed, smoother training curves, and is even able to train a ResNet50\\non ImageNet2012 without Batch Normalization layers. A problem on which AdamW\\nstays systematically stuck in a bad initial state.</td>\n",
       "      <td>drugtargetpred-chemselfies, chemselfies-base-bertmlm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Evaluating Large Language Models Trained on Code</td>\n",
       "      <td>We introduce Codex, a GPT language model fine-tuned on publicly available\\ncode from GitHub, and study its Python code-writing capabilities. A distinct\\nproduction version of Codex powers GitHub Copilot. On HumanEval, a new\\nevaluation set we release to measure functional correctness for synthesizing\\nprograms from docstrings, our model solves 28.8% of the problems, while GPT-3\\nsolves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling\\nfrom the model is a surprisingly effective strategy for producing working\\nsolutions to difficult prompts. Using this method, we solve 70.2% of our\\nproblems with 100 samples per problem. Careful investigation of our model\\nreveals its limitations, including difficulty with docstrings describing long\\nchains of operations and with binding operations to variables. Finally, we\\ndiscuss the potential broader impacts of deploying powerful code generation\\ntechnologies, covering safety, security, and economics.</td>\n",
       "      <td>starcoder2-7b-quantized.w8a8, starcoder2-7b-quantized.w8a16, starcoder2-3b-quantized.w8a8, starcoder2-3b-quantized.w8a16, starcoder2-15b-quantized.w8a8, starcoder2-15b-quantized.w8a16, Gemma-Wukong-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Train Short, Test Long: Attention with Linear Biases Enables Input\\n  Length Extrapolation</td>\n",
       "      <td>Since the introduction of the transformer model by Vaswani et al. (2017), a\\nfundamental question has yet to be answered: how does a model achieve\\nextrapolation at inference time for sequences that are longer than it saw\\nduring training? We first show that extrapolation can be enabled by simply\\nchanging the position representation method, though we find that current\\nmethods do not allow for efficient extrapolation. We therefore introduce a\\nsimpler and more efficient position method, Attention with Linear Biases\\n(ALiBi). ALiBi does not add positional embeddings to word embeddings; instead,\\nit biases query-key attention scores with a penalty that is proportional to\\ntheir distance. We show that this method trains a 1.3 billion parameter model\\non input sequences of length 1024 that extrapolates to input sequences of\\nlength 2048, achieving the same perplexity as a sinusoidal position embedding\\nmodel trained on inputs of length 2048 but training 11% faster and using 11%\\nless memory. ALiBi's inductive bias towards recency also leads it to outperform\\nmultiple strong position methods on the WikiText-103 benchmark.</td>\n",
       "      <td>jina-embeddings-v2-small-en, jina-embeddings-v2-base-es, jina-embeddings-v2-base-de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>TruthfulQA: Measuring How Models Mimic Human Falsehoods</td>\n",
       "      <td>We propose a benchmark to measure whether a language model is truthful in\\ngenerating answers to questions. The benchmark comprises 817 questions that\\nspan 38 categories, including health, law, finance and politics. We crafted\\nquestions that some humans would answer falsely due to a false belief or\\nmisconception. To perform well, models must avoid generating false answers\\nlearned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a\\nT5-based model. The best model was truthful on 58% of questions, while human\\nperformance was 94%. Models generated many false answers that mimic popular\\nmisconceptions and have the potential to deceive humans. The largest models\\nwere generally the least truthful. This contrasts with other NLP tasks, where\\nperformance improves with model size. However, this result is expected if false\\nanswers are learned from the training distribution. We suggest that scaling up\\nmodels alone is less promising for improving truthfulness than fine-tuning\\nusing training objectives other than imitation of text from the web.</td>\n",
       "      <td>strix-rufipes-70b, aegolius-acadicus-v1-30b, aegolius-acadicus-34b-v3, juanako-7b-UNA, Gemma-Wukong-2b, Echidna-7b-128k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Scale Efficiently: Insights from Pre-training and Fine-tuning\\n  Transformers</td>\n",
       "      <td>There remain many open questions pertaining to the scaling behaviour of\\nTransformer architectures. These scaling decisions and findings can be\\ncritical, as training runs often come with an associated computational cost\\nwhich have both financial and/or environmental impact. The goal of this paper\\nis to present scaling insights from pretraining and finetuning Transformers.\\nWhile Kaplan et al. presents a comprehensive study of the scaling behaviour of\\nTransformer language models, the scope is only on the upstream (pretraining)\\nloss. Therefore, it is still unclear if these set of findings transfer to\\ndownstream task within the context of the pretrain-finetune paradigm. The key\\nfindings of this paper are as follows: (1) we show that aside from only the\\nmodel size, model shape matters for downstream fine-tuning, (2) scaling\\nprotocols operate differently at different compute regions, (3) widely adopted\\nT5-base and T5-large sizes are Pareto-inefficient. To this end, we present\\nimproved scaling protocols whereby our redesigned models achieve similar\\ndownstream fine-tuning quality while having 50\\% fewer parameters and training\\n40\\% faster compared to the widely adopted T5-base model. We publicly release\\nover 100 pretrained checkpoints of different T5 configurations to facilitate\\nfuture research and analysis.</td>\n",
       "      <td>it5-efficient-small-el32-wiki-summarization, it5-efficient-small-el32-repubblica-to-ilgiornale, it5-efficient-small-el32-question-generation, it5-efficient-small-el32-news-summarization, it5-efficient-small-el32-informal-to-formal, it5-efficient-small-el32-ilgiornale-to-repubblica, it5-efficient-small-el32-headline-generation, it5-efficient-small-el32-formal-to-informal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Multitask Prompted Training Enables Zero-Shot Task Generalization</td>\n",
       "      <td>Large language models have recently been shown to attain reasonable zero-shot\\ngeneralization on a diverse set of tasks (Brown et al., 2020). It has been\\nhypothesized that this is a consequence of implicit multitask learning in\\nlanguage models' pretraining (Radford et al., 2019). Can zero-shot\\ngeneralization instead be directly induced by explicit multitask learning? To\\ntest this question at scale, we develop a system for easily mapping any natural\\nlanguage tasks into a human-readable prompted form. We convert a large set of\\nsupervised datasets, each with multiple prompts with diverse wording. These\\nprompted datasets allow for benchmarking the ability of a model to perform\\ncompletely held-out tasks. We fine-tune a pretrained encoder-decoder model\\n(Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a\\nwide variety of tasks. The model attains strong zero-shot performance on\\nseveral standard datasets, often outperforming models up to 16x its size.\\nFurther, our approach attains strong performance on a subset of tasks from the\\nBIG-bench benchmark, outperforming models up to 6x its size. All trained models\\nare available at https://github.com/bigscience-workshop/t-zero and all prompts\\nare available at https://github.com/bigscience-workshop/promptsource.</td>\n",
       "      <td>metro_t0pp_largepp, metro_t0pp_basepp, metro_t0pp_base, metro_t0p_largepp, metro_t0p_basepp, metro_t0p_base, metro_t0_largepp, metro_t0_basepp, metro_t0_base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Training Verifiers to Solve Math Word Problems</td>\n",
       "      <td>State-of-the-art language models can match human performance on many tasks,\\nbut they still struggle to robustly perform multi-step mathematical reasoning.\\nTo diagnose the failures of current models and support research, we introduce\\nGSM8K, a dataset of 8.5K high quality linguistically diverse grade school math\\nword problems. We find that even the largest transformer models fail to achieve\\nhigh test performance, despite the conceptual simplicity of this problem\\ndistribution. To increase performance, we propose training verifiers to judge\\nthe correctness of model completions. At test time, we generate many candidate\\nsolutions and select the one ranked highest by the verifier. We demonstrate\\nthat verification significantly improves performance on GSM8K, and we provide\\nstrong empirical evidence that verification scales more effectively with\\nincreased data than a finetuning baseline.</td>\n",
       "      <td>strix-rufipes-70b, aegolius-acadicus-v1-30b, aegolius-acadicus-34b-v3, Gemma-Wukong-2b, Echidna-7b-128k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>XLS-R: Self-supervised Cross-lingual Speech Representation Learning at\\n  Scale</td>\n",
       "      <td>This paper presents XLS-R, a large-scale model for cross-lingual speech\\nrepresentation learning based on wav2vec 2.0. We train models with up to 2B\\nparameters on nearly half a million hours of publicly available speech audio in\\n128 languages, an order of magnitude more public data than the largest known\\nprior work. Our evaluation covers a wide range of tasks, domains, data regimes\\nand languages, both high and low-resource. On the CoVoST-2 speech translation\\nbenchmark, we improve the previous state of the art by an average of 7.4 BLEU\\nover 21 translation directions into English. For speech recognition, XLS-R\\nimproves over the best known prior work on BABEL, MLS, CommonVoice as well as\\nVoxPopuli, lowering error rates by 14-34% relative on average. XLS-R also sets\\na new state of the art on VoxLingua107 language identification. Moreover, we\\nshow that with sufficient model size, cross-lingual pretraining can outperform\\nEnglish-only pretraining when translating English speech into other languages,\\na setting which favors monolingual pretraining. We hope XLS-R can help to\\nimprove speech processing tasks for many more languages of the world.</td>\n",
       "      <td>wav2vec2-xls-r-300m-zh-HK-v2, wav2vec2-xls-r-300m-zh-HK-lm-v2, wav2vec2-xls-r-300m-korean-lm, wav2vec2-xls-r-300m-korean, wav2vec2-xlsr-300m-finnish-lm, wav2vec2-xlsr-300m-finnish, wav2vec2-xlsr-1b-finnish-v2, wav2vec2-xlsr-1b-finnish-lm-v2, wav2vec2-xlsr-1b-finnish, wav2vec2-xlsr-1b-finnish-lm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with\\n  Gradient-Disentangled Embedding Sharing</td>\n",
       "      <td>This paper presents a new pre-trained language model, DeBERTaV3, which\\nimproves the original DeBERTa model by replacing mask language modeling (MLM)\\nwith replaced token detection (RTD), a more sample-efficient pre-training task.\\nOur analysis shows that vanilla embedding sharing in ELECTRA hurts training\\nefficiency and model performance. This is because the training losses of the\\ndiscriminator and the generator pull token embeddings in different directions,\\ncreating the \"tug-of-war\" dynamics. We thus propose a new gradient-disentangled\\nembedding sharing method that avoids the tug-of-war dynamics, improving both\\ntraining efficiency and the quality of the pre-trained model. We have\\npre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its\\nexceptional performance on a wide range of downstream natural language\\nunderstanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an\\nexample, the DeBERTaV3 Large model achieves a 91.37% average score, which is\\n1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art\\n(SOTA) among the models with a similar structure. Furthermore, we have\\npre-trained a multi-lingual model mDeBERTa and observed a larger improvement\\nover strong baselines compared to English models. For example, the mDeBERTa\\nBase achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6%\\nimprovement over XLM-R Base, creating a new SOTA on this benchmark. We have\\nmade our pre-trained models and inference code publicly available at\\nhttps://github.com/microsoft/DeBERTa.</td>\n",
       "      <td>deberta-italian-question-answering, deberta-v3-small-finetuned-mnli, deberta-v3-small-finetuned-cola, deberta-v3-large-finetuned-mnli, mDeBERTa-v3-base-xnli-multilingual-nli-2mil7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Learning Rich Representation of Keyphrases from Text</td>\n",
       "      <td>In this work, we explore how to train task-specific language models aimed\\ntowards learning rich representation of keyphrases from text documents. We\\nexperiment with different masking strategies for pre-training transformer\\nlanguage models (LMs) in discriminative as well as generative settings. In the\\ndiscriminative setting, we introduce a new pre-training objective - Keyphrase\\nBoundary Infilling with Replacement (KBIR), showing large gains in performance\\n(upto 8.16 points in F1) over SOTA, when the LM pre-trained using KBIR is\\nfine-tuned for the task of keyphrase extraction. In the generative setting, we\\nintroduce a new pre-training setup for BART - KeyBART, that reproduces the\\nkeyphrases related to the input text in the CatSeq format, instead of the\\ndenoised original input. This also led to gains in performance (upto 4.33\\npoints in F1@M) over SOTA for keyphrase generation. Additionally, we also\\nfine-tune the pre-trained language models on named entity recognition (NER),\\nquestion answering (QA), relation extraction (RE), abstractive summarization\\nand achieve comparable performance with that of the SOTA, showing that learning\\nrich representation of keyphrases is indeed beneficial for many other\\nfundamental NLP tasks.</td>\n",
       "      <td>keyphrase-generation-keybart-inspec, keyphrase-extraction-kbir-semeval2017, keyphrase-extraction-kbir-openkp, keyphrase-extraction-kbir-kptimes, keyphrase-extraction-kbir-kpcrowd, keyphrase-extraction-distilbert-inspec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Few-shot Learning with Multilingual Language Models</td>\n",
       "      <td>Large-scale generative language models such as GPT-3 are competitive few-shot\\nlearners. While these models are known to be able to jointly represent many\\ndifferent languages, their training data is dominated by English, potentially\\nlimiting their cross-lingual generalization. In this work, we train\\nmultilingual generative language models on a corpus covering a diverse set of\\nlanguages, and study their few- and zero-shot learning capabilities in a wide\\nrange of tasks. Our largest model with 7.5 billion parameters sets new state of\\nthe art in few-shot learning in more than 20 representative languages,\\noutperforming GPT-3 of comparable size in multilingual commonsense reasoning\\n(with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in\\n4-shot settings) and natural language inference (+5.4% in each of 0-shot and\\n4-shot settings). On the FLORES-101 machine translation benchmark, our model\\noutperforms GPT-3 on 171 out of 182 directions with 32 training examples, while\\nsurpassing the official supervised baseline in 45 directions. We conduct an\\nin-depth analysis of different multilingual prompting approaches, showing in\\nparticular that strong few-shot learning performance across languages can be\\nachieved via cross-lingual transfer through both templates and demonstration\\nexamples. Finally, we evaluate our models in social value tasks such as hate\\nspeech detection in five languages and find it has limitations similar to\\ncomparable sized GPT-3 models.</td>\n",
       "      <td>latxa-70b-v1.2, latxa-70b-v1.1, latxa-13b-v1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>High-Resolution Image Synthesis with Latent Diffusion Models</td>\n",
       "      <td>By decomposing the image formation process into a sequential application of\\ndenoising autoencoders, diffusion models (DMs) achieve state-of-the-art\\nsynthesis results on image data and beyond. Additionally, their formulation\\nallows for a guiding mechanism to control the image generation process without\\nretraining. However, since these models typically operate directly in pixel\\nspace, optimization of powerful DMs often consumes hundreds of GPU days and\\ninference is expensive due to sequential evaluations. To enable DM training on\\nlimited computational resources while retaining their quality and flexibility,\\nwe apply them in the latent space of powerful pretrained autoencoders. In\\ncontrast to previous work, training diffusion models on such a representation\\nallows for the first time to reach a near-optimal point between complexity\\nreduction and detail preservation, greatly boosting visual fidelity. By\\nintroducing cross-attention layers into the model architecture, we turn\\ndiffusion models into powerful and flexible generators for general conditioning\\ninputs such as text or bounding boxes and high-resolution synthesis becomes\\npossible in a convolutional manner. Our latent diffusion models (LDMs) achieve\\na new state of the art for image inpainting and highly competitive performance\\non various tasks, including unconditional image generation, semantic scene\\nsynthesis, and super-resolution, while significantly reducing computational\\nrequirements compared to pixel-based DMs. Code is available at\\nhttps://github.com/CompVis/latent-diffusion .</td>\n",
       "      <td>ldm3d-4c, ldm3d, FFusionXL-BASE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Automatic Speech Recognition Datasets in Cantonese: A Survey and New\\n  Dataset</td>\n",
       "      <td>Automatic speech recognition (ASR) on low resource languages improves the\\naccess of linguistic minorities to technological advantages provided by\\nartificial intelligence (AI). In this paper, we address the problem of data\\nscarcity for the Hong Kong Cantonese language by creating a new Cantonese\\ndataset. Our dataset, Multi-Domain Cantonese Corpus (MDCC), consists of 73.6\\nhours of clean read speech paired with transcripts, collected from Cantonese\\naudiobooks from Hong Kong. It comprises philosophy, politics, education,\\nculture, lifestyle and family domains, covering a wide range of topics. We also\\nreview all existing Cantonese datasets and analyze them according to their\\nspeech type, data source, total size and availability. We further conduct\\nexperiments with Fairseq S2T Transformer, a state-of-the-art ASR model, on the\\nbiggest existing dataset, Common Voice zh-HK, and our proposed MDCC, and the\\nresults show the effectiveness of our dataset. In addition, we create a\\npowerful and robust Cantonese ASR model by applying multi-dataset learning on\\nMDCC and Common Voice zh-HK.</td>\n",
       "      <td>whisper-small-cantonese, wav2vec2-BERT-cantonese, distil-whisper-small-cantonese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>SGPT: GPT Sentence Embeddings for Semantic Search</td>\n",
       "      <td>Decoder transformers have continued increasing in scale reaching hundreds of\\nbillions of parameters. Due to their scale the same decoder sets\\nstate-of-the-art results on various language tasks via prompting or\\nfine-tuning. Yet, these large foundation models remain unusable for the related\\nfields of semantic search and sentence embeddings. This prevents possibly new\\nstate-of-the-art results and forces organizations to train and maintain\\nseparate models. To this end, we propose SGPT to use decoders for sentence\\nembeddings and semantic search via prompting or fine-tuning. At 5.8 billion\\nparameters SGPT improves on the previously best sentence embeddings by a margin\\nof 7% and outperforms a concurrent method with 175 billion parameters as\\nmeasured on the BEIR search benchmark. Code, models and result files are freely\\navailable at https://github.com/Muennighoff/sgpt.</td>\n",
       "      <td>MiniCPM-Embedding-Light, MiniCPM-Embedding, sgpt-bloom-7b1-msmarco, sgpt-bloom-1b7-nli, SGPT-5.8B-weightedmean-nli-bitfit, SGPT-5.8B-weightedmean-msmarco-specb-bitfit, SGPT-2.7B-weightedmean-msmarco-specb-bitfit, SGPT-125M-weightedmean-nli-bitfit, SGPT-125M-weightedmean-msmarco-specb-bitfit, SGPT-1.3B-weightedmean-msmarco-specb-bitfit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Self-Supervised Vision Transformers Learn Visual Concepts in\\n  Histopathology</td>\n",
       "      <td>Tissue phenotyping is a fundamental task in learning objective\\ncharacterizations of histopathologic biomarkers within the tumor-immune\\nmicroenvironment in cancer pathology. However, whole-slide imaging (WSI) is a\\ncomplex computer vision in which: 1) WSIs have enormous image resolutions with\\nprecludes large-scale pixel-level efforts in data curation, and 2) diversity of\\nmorphological phenotypes results in inter- and intra-observer variability in\\ntissue labeling. To address these limitations, current efforts have proposed\\nusing pretrained image encoders (transfer learning from ImageNet,\\nself-supervised pretraining) in extracting morphological features from\\npathology, but have not been extensively validated. In this work, we conduct a\\nsearch for good representations in pathology by training a variety of\\nself-supervised models with validation on a variety of weakly-supervised and\\npatch-level tasks. Our key finding is in discovering that Vision Transformers\\nusing DINO-based knowledge distillation are able to learn data-efficient and\\ninterpretable features in histology images wherein the different attention\\nheads learn distinct morphological phenotypes. We make evaluation code and\\npretrained weights publicly-available at:\\nhttps://github.com/Richarizardd/Self-Supervised-ViT-Path.</td>\n",
       "      <td>vit_small_patch16_256.tcga_brca_dino, resnet50.tcga_brca_simclr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Mukayese: Turkish NLP Strikes Back</td>\n",
       "      <td>Having sufficient resources for language X lifts it from the under-resourced\\nlanguages class, but not necessarily from the under-researched class. In this\\npaper, we address the problem of the absence of organized benchmarks in the\\nTurkish language. We demonstrate that languages such as Turkish are left behind\\nthe state-of-the-art in NLP applications. As a solution, we present Mukayese, a\\nset of NLP benchmarks for the Turkish language that contains several NLP tasks.\\nWe work on one or more datasets for each benchmark and present two or more\\nbaselines. Moreover, we present four new benchmarking datasets in Turkish for\\nlanguage modeling, sentence segmentation, and spell checking. All datasets and\\nbaselines are available under: https://github.com/alisafaya/mukayese</td>\n",
       "      <td>transformer-turkish-summarization, mt5-base-turkish-summarization, mbart-large-turkish-summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>IT5: Large-scale Text-to-text Pretraining for Italian Language\\n  Understanding and Generation</td>\n",
       "      <td>The T5 model and its unified text-to-text paradigm contributed in advancing\\nthe state-of-the-art for many natural language processing tasks. While some\\nmultilingual variants of the T5 model have recently been introduced, their\\nperformances were found to provide suboptimal performances for languages other\\nthan English if compared to monolingual variants. We are motivated by these\\nfindings to introduce IT5, the first family of encoder-decoder transformer\\nmodels pretrained specifically on Italian. We perform a thorough cleaning of a\\nweb-crawled Italian corpus including more than 40 billion words and use it to\\npretrain three IT5 models of different sizes. The performance of IT5 models and\\ntheir multilingual counterparts is then evaluated on a broad range of natural\\nlanguage understanding and generation benchmarks for Italian. We find the\\nmonolingual IT5 models to provide the best scale-to-performance ratio across\\ntested models, consistently outperforming their multilingual counterparts and\\nsetting a new state-of-the-art for most Italian conditional language generation\\ntasks.</td>\n",
       "      <td>mt5-small-question-answering, mt5-base-question-answering, it5-small-question-answering, it5-large-question-answering, it5-efficient-small-el32-wiki-summarization, it5-efficient-small-el32-repubblica-to-ilgiornale, it5-efficient-small-el32-question-generation, it5-efficient-small-el32-question-answering, it5-efficient-small-el32-news-summarization, it5-efficient-small-el32-informal-to-formal, it5-efficient-small-el32-ilgiornale-to-repubblica, it5-efficient-small-el32-headline-generation, it5-efficient-small-el32-formal-to-informal, it5-base-question-answering, it5-base-news-summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Model soups: averaging weights of multiple fine-tuned models improves\\n  accuracy without increasing inference time</td>\n",
       "      <td>The conventional recipe for maximizing model accuracy is to (1) train\\nmultiple models with various hyperparameters and (2) pick the individual model\\nwhich performs best on a held-out validation set, discarding the remainder. In\\nthis paper, we revisit the second step of this procedure in the context of\\nfine-tuning large pre-trained models, where fine-tuned models often appear to\\nlie in a single low error basin. We show that averaging the weights of multiple\\nmodels fine-tuned with different hyperparameter configurations often improves\\naccuracy and robustness. Unlike a conventional ensemble, we may average many\\nmodels without incurring any additional inference or memory costs -- we call\\nthe results \"model soups.\" When fine-tuning large pre-trained models such as\\nCLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides\\nsignificant improvements over the best model in a hyperparameter sweep on\\nImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on\\nImageNet, achieved a new state of the art. Furthermore, we show that the model\\nsoup approach extends to multiple image classification and natural language\\nprocessing tasks, improves out-of-distribution performance, and improves\\nzero-shot performance on new downstream tasks. Finally, we analytically relate\\nthe performance similarity of weight-averaging and logit-ensembling to flatness\\nof the loss and confidence of the predictions, and validate this relation\\nempirically. Code is available at https://github.com/mlfoundations/model-soups.</td>\n",
       "      <td>Smol-Llama-3.2-3B, QwenMosaic-7B, QwQen-3B-LCoT, Phi-4-RR-Shoup, Phi-4-RP-v0, Llama-3.2-3B-RP-DeepThink, FwF-Qwen-7B-0.2, FuseQwQen-7B, Jajuka-3b, JAJUKA-WEWILLNEVERFORGETYOU-3B, llama3-8B-V2, Alita99-8B-LINEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>How Does Pre-trained Wav2Vec 2.0 Perform on Domain Shifted ASR? An\\n  Extensive Benchmark on Air Traffic Control Communications</td>\n",
       "      <td>Recent work on self-supervised pre-training focus on leveraging large-scale\\nunlabeled speech data to build robust end-to-end (E2E) acoustic models (AM)\\nthat can be later fine-tuned on downstream tasks e.g., automatic speech\\nrecognition (ASR). Yet, few works investigated the impact on performance when\\nthe data properties substantially differ between the pre-training and\\nfine-tuning phases, termed domain shift. We target this scenario by analyzing\\nthe robustness of Wav2Vec 2.0 and XLS-R models on downstream ASR for a\\ncompletely unseen domain, air traffic control (ATC) communications. We\\nbenchmark these two models on several open-source and challenging ATC databases\\nwith signal-to-noise ratio between 5 and 20 dB. Relative word error rate (WER)\\nreductions between 20% to 40% are obtained in comparison to hybrid-based ASR\\nbaselines by only fine-tuning E2E acoustic models with a smaller fraction of\\nlabeled data. We analyze WERs on the low-resource scenario and gender bias\\ncarried by one ATC dataset.</td>\n",
       "      <td>wav2vec2-xls-r-300m-en-atc-uwb-atcc-and-atcosim, wav2vec2-xls-r-300m-en-atc-uwb-atcc, wav2vec2-xls-r-300m-en-atc-atcosim, wav2vec2-large-960h-lv60-self-en-atc-uwb-atcc-and-atcosim, wav2vec2-large-960h-lv60-self-en-atc-uwb-atcc, wav2vec2-large-960h-lv60-self-en-atc-atcosim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>OPT: Open Pre-trained Transformer Language Models</td>\n",
       "      <td>Large language models, which are often trained for hundreds of thousands of\\ncompute days, have shown remarkable capabilities for zero- and few-shot\\nlearning. Given their computational cost, these models are difficult to\\nreplicate without significant capital. For the few that are available through\\nAPIs, no access is granted to the full model weights, making them difficult to\\nstudy. We present Open Pre-trained Transformers (OPT), a suite of decoder-only\\npre-trained transformers ranging from 125M to 175B parameters, which we aim to\\nfully and responsibly share with interested researchers. We show that OPT-175B\\nis comparable to GPT-3, while requiring only 1/7th the carbon footprint to\\ndevelop. We are also releasing our logbook detailing the infrastructure\\nchallenges we faced, along with code for experimenting with all of the released\\nmodels.</td>\n",
       "      <td>opt-66b_eval, opt-6.7b_eval, opt-350m_eval, opt-30b_eval, opt-2.7b_eval, opt-13b_eval, opt-125m_eval, opt-1.3b_eval</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>LingMess: Linguistically Informed Multi Expert Scorers for Coreference\\n  Resolution</td>\n",
       "      <td>While coreference resolution typically involves various linguistic\\nchallenges, recent models are based on a single pairwise scorer for all types\\nof pairs. We present LingMess, a new coreference model that defines different\\ncategories of coreference cases and optimize multiple pairwise scorers, where\\neach scorer learns a specific set of linguistic challenges. Our model\\nsubstantially improves pairwise scores for most categories and outperforms\\ncluster-level performance on Ontonotes and 5 additional datasets. Our model is\\navailable in https://github.com/shon-otmazgin/lingmess-coref</td>\n",
       "      <td>lingmess-coref, f-coref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Matryoshka Representation Learning</td>\n",
       "      <td>Learned representations are a central component in modern ML systems, serving\\na multitude of downstream tasks. When training such representations, it is\\noften the case that computational and statistical constraints for each\\ndownstream task are unknown. In this context rigid, fixed capacity\\nrepresentations can be either over or under-accommodating to the task at hand.\\nThis leads us to ask: can we design a flexible representation that can adapt to\\nmultiple downstream tasks with varying computational resources? Our main\\ncontribution is Matryoshka Representation Learning (MRL) which encodes\\ninformation at different granularities and allows a single embedding to adapt\\nto the computational constraints of downstream tasks. MRL minimally modifies\\nexisting representation learning pipelines and imposes no additional cost\\nduring inference and deployment. MRL learns coarse-to-fine representations that\\nare at least as accurate and rich as independently trained low-dimensional\\nrepresentations. The flexibility within the learned Matryoshka Representations\\noffer: (a) up to 14x smaller embedding size for ImageNet-1K classification at\\nthe same level of accuracy; (b) up to 14x real-world speed-ups for large-scale\\nretrieval on ImageNet-1K and 4K; and (c) up to 2% accuracy improvements for\\nlong-tail few-shot classification, all while being as robust as the original\\nrepresentations. Finally, we show that MRL extends seamlessly to web-scale\\ndatasets (ImageNet, JFT) across various modalities -- vision (ViT, ResNet),\\nvision + language (ALIGN) and language (BERT). MRL code and pretrained models\\nare open-sourced at https://github.com/RAIVNLab/MRL.</td>\n",
       "      <td>vietnamese-sbert-soc, sup-SimCSE-VietNamese-phobert-base-soc, finetuned-snowflake-arctic-embed-m-v1.5, finetuned-snowflake-arctic-embed-m, roberta-amharic-embed-medium, roberta-amharic-embed-base-v0, finetuned_MiniLM, cc-uffs-ppc-ft-test-multiqa, finetuned_arctic, nomic-v1.5-financial-matryoshka, snowflake-arctic-embed-m-klej-dyk-v0.1, privacy_embedding_rag_10k_base_checkpoint_2-klej-dyk-v0.1, mmlw-roberta-base-klej-dyk-v0.1, gte-base-en-v1.5-klej-dyk-v0.1, bge-base-en-v1.5-klej-dyk-v0.2, bge-base-en-v1.5-klej-dyk, all-MiniLM-L6-v2-klej-dyk-v0.1, asc_embedding, halong_embedding-legal-document-finetune, mpnet-base-gooaq-cmnrl-mrl, distilroberta-base-nli-matryoshka-v3, distilroberta-base-nli-matryoshka-reduced, distilroberta-base-nli-2d-matryoshka, distilbert-base-uncased-sts-matryoshka, distilbert-base-uncased-sts-2d-matryoshka, bert-base-uncased-gooaq, bge-base-st-phyto, bge-large-repmus-matryoshka, finetuned-all-MiniLM-L6-v2, silma-embeddding-matryoshka-v0.1, nomic-embed-financial-matryoshka, swahili-paraphrase-multilingual-mpnet-base-v2-nli-matryoshka, bge-base-swahili-matryoshka, worksphere-regulations-embedding_bge, bge-base-financial, ai-policy-ft, finetuned-arctic-model-2, finetuned-arctic-model, mxbai-embed-large-v1-financial-rag-matryoshka, mpnet-base-financial-rag-matryoshka, financial-rag-matryoshka, bge-base-financial-nvidia-matryoshka, UAE-Large-V1-financial-rag-matryoshka, roberta-amharic-text-embedding-medium, roberta-amharic-text-embedding-base, bert-amharic-text-embedding-medium, bge-small-matryoshka-fine-tuned, LEGAL_EMBEDDING, bl_ademe_large, bge_model_fine_tuned_law, bge-base-argilla-sdk-matryoshka, msmarco-distilbert-base-v4, bge-base-financial-matryoshka, bge-base-en-sec10k-embed, bge-base-en-honsec10k-embed, bge-base-en-bioembed768, bge-base-en-bioembed, AIE4_midterm_tuned_embeddings_2, AIE4_midterm_tuned_embeddings, bge-m3-trained-2, bge-m3-trained, bge-m3-spa-law-qa-trained-2, bge-m3-spa-law-qa-trained, bge-m3-retrained, arabic-english-sts-matryoshka-v2.0, arabic-english-sts-matryoshka, Arabic-STS-Matryoshka-V2, Arabic-STS-Matryoshka, artic_ft_midterm, ModernBERT-large-sts, Italian-ModernBERT-base-embed-mmarco-mnrl, multilingual-e5-large-ft-sts-spanish-matryoshka-768-64-5e, multilingual-e5-large-ft-sts-spanish-matryoshka-768-16-5e, modernbert-embed-base-ft-sts-spanish-matryoshka-768-64, distilroberta-base-ft-allnli-matryoshka-768-64-1e-256bs, distilroberta-base-ft-allnli-matryoshka-768-16-1e-128bs, distilbert-base-matryoshka-sts-v2, distilbert-base-matryoshka-sts, mpac-bge-large-v1.2, mpac-bge-large, hateBERT-cl-rlhf-5-epochs, hateBERT-cl-rlhf-10-epochs, hateBERT-cl-rlhf, bert-base-uncased-cl-rlhf-5-epochs, bert-base-uncased-cl-rlhf-10-epochs, bert-base-uncased-cl-rlhf, gte-base-law-matryoshka, multilingual-e5-large-ita, vietnamese-bi-encoder-for-SoICT-2024, vietnamese-bi-encoder-fine-tuning-for-law-chatbot, bge-base-custom-matryoshka, policy_gte_large_7, policy_gte_large, proba, legal-ft-arctic-l, legal-ft, bge-m3-spa-law-qa, modernbert-embed-ft-const-legal-matryoshka, modernbert-embed-base-legaltextai-matryoshka-legaldataset, fine-tuned-bge-base-raw_pdf-v1, fine-tune-embedding-bge-base-HrPolicy_vfinal, fine-tune-embedding-bge-base-HrPolicy, bge-base-raw_pdf_finetuned_vf1, mxbai-de-abat-matryoshka, mxbai-abat-matryoshka, bge-base-bioasq-matryoshka, all-mpnet-base-v2-bioasq-matryoshka, fine-tuned-matryoshka-500, fine-tuned-matryoshka-200, fine-tuned-matryoshka-1725, fine-tuned-matryoshka-1500, fine-tuned-matryoshka-1000, fine-tuned-matryoshka-100, fine-tuned-matryoshka, improve_vibi, improve_halong, snowflake-arctic-embed-xs_finetuned_aipolicy, snowflake_finetuned_semantic, snowflake_finetuned_recursive, mpnet_finetuned_semantic, mpnet_finetuned_recursive, finetuned_arctic-embedd-l, bge-m3-nvidia-ko-v1, snowflake-arctic-embed-l-v2.0-pits, bge-small-financial-matryoshka, bge-m3-financial-matryoshka, finetuned_paraphrase-multilingual_v3, finetuned_paraphrase-multilingual_v2, finetuned_paraphrase-multilingual_test, finetuned_paraphrase-multilingual_mpnet_try6, finetuned_paraphrase-multilingual_mpnet_try5, finetuned_paraphrase-multilingual_mpnet_try4, finetuned_paraphrase-multilingual_mpnet_try3, finetuned_paraphrase-multilingual_mpnet_try2, finetuned_paraphrase-multilingual_mpnet, finetuned_paraphrase-multilingual, chemembed-chemselfies, bge-base-financial-matryoshka-v1, bge-m3-uz-legal-matryoshka, bge-finetuned-insurance-matryoshka, bge-base-insurance-matryoshka, ModernBERT-base-nli-v3, ModernBERT-base-marco, bge-small-en-v1.5-esg-v2, bge-small-en-v1.5-esg, bge-micro-v2-esg-v2, bge-micro-v2-esg, bge-base-financial-matryoshka-testing, sentence-roberta-small, xlm-roberta-large-sts-matryoshka, nomic-embed-text-v1, bge-base-for_text2sql, bge-m3-es-legal-tmp-6, bge-m3-es-legal-tmp-5, bge-m3-es-legal-tmp-3, modernbert-embed-base-legal-matryoshka-2, slinger20241231-3, slinger20241231-2, slinger20241231-1, hi-di-hi, bge-base-patentmatch, deep-learning-for-embedding-model-ssilwal-qpham6_army_doc, deep-learning-for-embedding-model-ssilwal-qpham6, German-RAG-ModernBERT-Base-TRIPLES, acge_text_embedding, legal-ft-1, technographics-marketing-matryoshka, finetuned-gte-base, bge-base-matryoshka-aws-casestudies, bge-base-financial-matryoshka-anisha, bge-base-aws-case-studies, bge-base-financial-matryoshka-nvda-iter20, bge-base-financial-matryoshka-nvda, vietnamese-sbert-Financial-Matryoshka-5e-11k, vietnamese-sbert-Financial-Matryoshka-2e-11k, vietnamese-sbert-Financial-Matryoshka-1e-200k, vietnamese-bi-encoder-financial-matryoshka-5, vietnamese-bi-encoder-financial-matryoshka-2, vietnamese-bi-encoder-Matryoshka-2e-9k, vietnamese-bi-encoder-Matryoshka-1e-9k, vietnamese-bi-encoder-Financial-Matryoshka-5e-11k, vietnamese-bi-encoder-Financial-Matryoshka-3e-200k, vietnamese-bi-encoder-Financial-Matryoshka-2e-11k, vietnamese-bi-encoder-Financial-Matryoshka-1e-200k, vietnamese-bi-encoder-Financial-Matryoshka, multilingual-e5-base-Matryoshka-7e-11k, multilingual-e5-base-Matryoshka-5e-11k, multilingual-e5-base-Matryoshka-2e-11k, multilingual-e5-base-Matryoshka-1e-200k, halong_embedding-Financial-Matryoshka-2e-11k, halong_embedding-Financial-Matryoshka-1e-200k, halong-embedding-Financial-Matryoshka-5e-11k, gte-multilingual-base-Matryoshka-4e-9k, gte-multilingual-base-Matryoshka-3e-9k, gte-multilingual-base-Matryoshka-2e-9k, gte-multilingual-base-Matryoshka-1e-9k, gte-multilingual-base-Matryoshka-1e-11k, bert-base-multilingual-uncased-Financial-Matryoshka-8e-11k, bert-base-multilingual-uncased-Financial-Matryoshka-5e-11k, bert-base-multilingual-uncased-Financial-Matryoshka-2e-11k, bert-base-multilingual-Financial-Matryoshka-2-v2, bert-base-multilingual-Financial-Matryoshka, ModernBERT-base-3e-9k, indic-bert-nli-matryoshka, jina-semantic-bmf-matryoshka-1024-10epochs, jina-semantic-bmf-matryoshka, german-semantic-bmf-matryoshka-512-10epochs, german-semantic-bmf-matryoshka, bge-semantic-bmf-matryoshka, bge-base-financial-matryoshkafinetuning-tcz-webiste, bge-base-financial-matryoshka-finetuning-tcz-1, st-SIT-test, sqv-v3-10ep, sqv-v3, sqv-v2, sqv-5ep, sitgrsBAAIbge-m3-300824v2, sitgrsBAAIbge-m3-290824, sitges2608bai-4ep, sitges2608, sitges10242608-4ep-rerankv4-sp, sitges10242608-4ep-rerankv3-sp, sitges10242608-4ep-rerankv3, sitges10242608-4ep-rerankv2, sitges10242608-4ep-rerank, ST-tramits-sitges-006-5ep, ST-tramits-sitges-005-5ep, ST-tramits-sitges-003-5ep, ST-tramits-sitges-003-10ep, ST-tramits-sitges-002-5ep, ST-tramits-sitges-001-5ep, ST-tramits-VIL-001-5ep, ST-tramits-SQV-005-5ep, ST-tramits-SQV-005-10ep, ST-tramits-SQV-004-5ep, ST-tramits-SQV-004-10ep, ST-tramits-SITGES-007-5ep, ST-tramits-SB-003-5ep, ST-tramits-SB-001-5ep, ST-tramits-MONTGAT-001-5ep, SITGES-bge-FT1, SITGES-BAAI3, finetuned_arctic_ai_risk, bge-base-movie-matryoshka, midterm-finetuned-arctic, bge-base-automobile-matryoshka, Multilingual-base-soil-embedding, Multilingual-base-SWU-Matryoshka, tnt_v5_lega_new_tokens, bge_based_arg_minibio_matryoshka, votum-case-law-v1, votum-acts-v1, gte-base-legal-matryoshka-v1, gte-base-case-law-v2, bge-base-legal-matryoshka-v1, bge-base-case-law-v1, midterm-finetuned-embedding, modernbert-embed-base-bible, bge-base-bible-retrieval, BGE-Finetuned-FinBench, msmarco-distilbert-base-v4_1, bge-base-en-v1.5_v3, bge-base-en-v1.5_v2, bge-base-en-v1.5_v1, bge-base-en-v1.5, bge-base-financial-matryoshka_3, bge-base-financial-matryoshka_2, streetlight_sql_embedding2, bge-embedding-model2, bge-base-en-v1.5-41-keys-phase-2-v1, bge-base-en-41-keys-phase-2-v1, me5-large-construction-esp-cat, me5-large-construction-cat, bge-base-financial-matryoshka2, Marbert-all-nli-triplet-Matryoshka, E5-all-nli-triplet-Matryoshka, Arabic-mpnet-base-all-nli-triplet, Arabic-labse-Matryoshka, Arabic-all-nli-triplet-Matryoshka, Arabic-MiniLM-L12-v2-all-nli-triplet, Arabert-all-nli-triplet-Matryoshka, GO-Term-Embeddings-Snowflake-m-1.5, bge-base-finetuned-financial, bge-base-financial-matryoshka_test_4, bge-base-financial-matryoshka_test_3, bge-base-financial-matryoshka_test_1, bge-base-financial-matryoshka_test_0, my-bge-base-financial-matryoshka, bge-base-securiti-dataset-3-v23, bge-base-securiti-dataset-1-v9, bge-base-securiti-dataset-1-v8, bge-base-securiti-dataset-1-v7, bge-base-securiti-dataset-1-v6, bge-base-securiti-dataset-1-v5, bge-base-securiti-dataset-1-v4, bge-base-securiti-dataset-1-v3, bge-base-securiti-dataset-1-v22, bge-base-securiti-dataset-1-v20, bge-base-securiti-dataset-1-v2, bge-base-securiti-dataset-1-v19, bge-base-securiti-dataset-1-v18, bge-base-securiti-dataset-1-v17, bge-base-securiti-dataset-1-v16, bge-base-securiti-dataset-1-v14, bge-base-securiti-dataset-1-v13, bge-base-securiti-dataset-1-v12, bge-base-securiti-dataset-1-v11, bge-base-securiti-dataset-1-v10, bge-base-scidocs-dataset-10k-2k-e1, bge-base-climate_fever-dataset-10k-2k-v1, bge-base-climate_fever-dataset-10k-2k-e2, bge-base-citi-dataset-detailed-9k-1_5k-e1, bge-base-citi-dataset-detailed-6k-0_5k-e2, bge-base-citi-dataset-9k-1k-e1, bge-base-arguana-dataset-10k-2k-e1, sample-embedding, legal-ft-3, legal-ft-2, norsbert3-base-matryoshka, pubmedbert-base-embedding-Chatbot-Matryoshk, nomic-embed-text-v1.5-Chatbot-matryoshka, bge-large-Chatbot-matryoshka, legal-french-matroshka, RhetoriBERT, Noss, Niss, bge-base-financial-matryoshka_test_my, bge-base-financial-matryoshka_test, mixedbread-ai_mxbai-embed-large-v1_FareedKhan_prime_synthetic_data_2k_3_8, mixedbread-ai_deepset-mxbai-embed-de-large-v1_FareedKhan_prime_synthetic_data_2k_3_8, flax-sentence-embeddings_all_datasets_v4_MiniLM-L6_FareedKhan_prime_synthetic_data_2k_4_16, flax-sentence-embeddings_all_datasets_v4_MiniLM-L6_FareedKhan_prime_synthetic_data_2k_10_64, flax-sentence-embeddings_all_datasets_v4_MiniLM-L6_FareedKhan_prime_synthetic_data_2k_10_32, TaylorAI_bge-micro-v2_FareedKhan_prime_synthetic_data_2k_10_64, TaylorAI_bge-micro-v2_FareedKhan_prime_synthetic_data_2k_10_32, BAAI_bge-m3_FareedKhan_prime_synthetic_data_2k_2_4, Alibaba-NLP_gte-base-en-v1.5_FareedKhan_prime_synthetic_data_2k_10_32, FinguMv3, Fingu-M-v2, Fingu-M-v1, SciTopicNomicEmbed, snowflake-l-marketing-tuned, bge-base-financial-matryoshka-2, finetuned-arctic-sentence, finetuned-arctic, Morocco-Darija-Sentence-Embedding-v0.2, gte-multilingual-base-v2.1, gte-multilingual-base-v2.0, bge-base-financial-matryoshka-abhiram, qwen1k, Embedding-v2, Embedding-v1, Embedding-v0, modernbert-embed-quickb-video, modernbert-embed-quickb, ModernBERT-embed-base-legal-MRL, arabic_text_embedding_sts_arabertv02_arabicnlitriplet, Arabic_text_embedding_for_sts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>FlashAttention: Fast and Memory-Efficient Exact Attention with\\n  IO-Awareness</td>\n",
       "      <td>Transformers are slow and memory-hungry on long sequences, since the time and\\nmemory complexity of self-attention are quadratic in sequence length.\\nApproximate attention methods have attempted to address this problem by trading\\noff model quality to reduce the compute complexity, but often do not achieve\\nwall-clock speedup. We argue that a missing principle is making attention\\nalgorithms IO-aware -- accounting for reads and writes between levels of GPU\\nmemory. We propose FlashAttention, an IO-aware exact attention algorithm that\\nuses tiling to reduce the number of memory reads/writes between GPU high\\nbandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of\\nFlashAttention, showing that it requires fewer HBM accesses than standard\\nattention, and is optimal for a range of SRAM sizes. We also extend\\nFlashAttention to block-sparse attention, yielding an approximate attention\\nalgorithm that is faster than any existing approximate attention method.\\nFlashAttention trains Transformers faster than existing baselines: 15%\\nend-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the\\nMLPerf 1.1 training speed record, 3times speedup on GPT-2 (seq. length 1K),\\nand 2.4times speedup on long-range arena (seq. length 1K-4K). FlashAttention\\nand block-sparse FlashAttention enable longer context in Transformers, yielding\\nhigher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on\\nlong-document classification) and entirely new capabilities: the first\\nTransformers to achieve better-than-chance performance on the Path-X challenge\\n(seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1%\\naccuracy).</td>\n",
       "      <td>Samantha-1.11-CodeLlama-34b, Samantha-1.11-7b, Samantha-1.11-13b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Expanding Language-Image Pretrained Models for General Video Recognition</td>\n",
       "      <td>Contrastive language-image pretraining has shown great success in learning\\nvisual-textual joint representation from web-scale data, demonstrating\\nremarkable \"zero-shot\" generalization ability for various image tasks. However,\\nhow to effectively expand such new language-image pretraining methods to video\\ndomains is still an open problem. In this work, we present a simple yet\\neffective approach that adapts the pretrained language-image models to video\\nrecognition directly, instead of pretraining a new model from scratch. More\\nconcretely, to capture the long-range dependencies of frames along the temporal\\ndimension, we propose a cross-frame attention mechanism that explicitly\\nexchanges information across frames. Such module is lightweight and can be\\nplugged into pretrained language-image models seamlessly. Moreover, we propose\\na video-specific prompting scheme, which leverages video content information\\nfor generating discriminative textual prompts. Extensive experiments\\ndemonstrate that our approach is effective and can be generalized to different\\nvideo recognition scenarios. In particular, under fully-supervised settings,\\nour approach achieves a top-1 accuracy of 87.1% on Kinectics-400, while using\\n12 times fewer FLOPs compared with Swin-L and ViViT-H. In zero-shot\\nexperiments, our approach surpasses the current state-of-the-art methods by\\n+7.6% and +14.9% in terms of top-1 accuracy under two popular protocols. In\\nfew-shot scenarios, our approach outperforms previous best methods by +32.1%\\nand +23.1% when the labeled data is extremely limited. Code and models are\\navailable at https://aka.ms/X-CLIP</td>\n",
       "      <td>xclip-large-patch14-kinetics-600, xclip-large-patch14-16-frames, xclip-large-patch14, xclip-base-patch32-16-frames, xclip-base-patch16-ucf-8-shot, xclip-base-patch16-ucf-4-shot, xclip-base-patch16-ucf-2-shot, xclip-base-patch16-ucf-16-shot, xclip-base-patch16-kinetics-600-16-frames, xclip-base-patch16-kinetics-600, xclip-base-patch16-hmdb-8-shot, xclip-base-patch16-hmdb-4-shot, xclip-base-patch16-hmdb-2-shot, xclip-base-patch16-hmdb-16-shot, xclip-base-patch16-16-frames, xclip-base-patch16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Efficient Few-Shot Learning Without Prompts</td>\n",
       "      <td>Recent few-shot methods, such as parameter-efficient fine-tuning (PEFT) and\\npattern exploiting training (PET), have achieved impressive results in\\nlabel-scarce settings. However, they are difficult to employ since they are\\nsubject to high variability from manually crafted prompts, and typically\\nrequire billion-parameter language models to achieve high accuracy. To address\\nthese shortcomings, we propose SetFit (Sentence Transformer Fine-tuning), an\\nefficient and prompt-free framework for few-shot fine-tuning of Sentence\\nTransformers (ST). SetFit works by first fine-tuning a pretrained ST on a small\\nnumber of text pairs, in a contrastive Siamese manner. The resulting model is\\nthen used to generate rich text embeddings, which are used to train a\\nclassification head. This simple framework requires no prompts or verbalizers,\\nand achieves high accuracy with orders of magnitude less parameters than\\nexisting techniques. Our experiments show that SetFit obtains comparable\\nresults with PEFT and PET techniques, while being an order of magnitude faster\\nto train. We also show that SetFit can be applied in multilingual settings by\\nsimply switching the ST body. Our code is available at\\nhttps://github.com/huggingface/setfit and our datasets at\\nhttps://huggingface.co/setfit .</td>\n",
       "      <td>indo-setfit-absa-model-polarity, indo-setfit-absa-model-aspect, GIST-small-Embedding-v0-8-shot, maes, setfit-language-guess, setfit-rag-hybrid-search-query-router-test, setfit-rag-hybrid-search-query-router, setfit-bge-small-v1.5-sst2-nlapug-spelling, my-setfit-classifier_toxic, my-setfit-classifier, bge-small-en-v1.5-isarcasm, SetFit_sms_Analyzer5c95292, SetFit_sms_Analyzer1, setfit-paraphrase-mpnet-sst5_v2, setfit-paraphrase-mpnet-sst5, setfit-paraphrase-mpnet-emotionv, setfit-paraphrase-mpnet-emotion, setfit-paraphrase-mpnet-base-v2-emotion_comp, setfit-paraphrase-mpnet-base-v2, setfit-paraphrase-mpnet-amazoncf, setfit-paraphrase-mpnet-amazon_cf, setfit-paraphrase-mpnet-ag_news_v2, setfit-paraphrase-mpnet-ag_news, mpnetv2_setfit_finarg_finetuned, multi-qa-mpnet-base-cos-v1-test, multi-qa-mpnet-base-cos-v1-scon-poc, multi-qa-mpnet-base-cos-v1-scon-3epochs, multi-qa-mpnet-base-cos-v1-poc, multi-qa-mpnet-base-cos-v1-ocontrastive-3e-300samples-20iter, multi-qa-mpnet-base-cos-v1-contrastive-logistic-500s, multi-qa-mpnet-base-cos-v1-contrastive-logistic, multi-qa-mpnet-base-cos-v1-contrastive-3e-250samples-20iter, multi-qa-mpnet-base-cos-v1-contrastive-2e-1000samples, setfit_lobbying_classifier, setfit-oversample-labels-lobbying, news_cats_2, setfit-model-test, setfit-paraphrase-mpnet-base-v2-sst2-8-shot, setfit-all-MiniLM-L6-v2-sst2-32-shot, setfit-absa-paraphrase-mpnet-base-v2-restaurants-polarity, setfit-absa-paraphrase-mpnet-base-v2-restaurants-aspect, setfit-absa-bge-small-en-v1.5-restaurants-polarity, setfit-absa-bge-small-en-v1.5-restaurants-aspect, modernbert-embed-base-sst2, 725_tm-setfit-paraphrase-mpnet-base-v2, 725_tm-setfit-bge-base-en-v1.5, 725_test_model, fine_tuned_setfit_pydata_demo, setfit, setfit-paraphrase-mpnet-base-v2-twitter-sentiment-cleaned-73, setfit-paraphrase-mpnet-base-v2-twitter-sentiment-cleaned, setfit-paraphrase-mpnet-base-v2-twitter-sentiment, setfit-multilabel-one-vs-rest-feb-2024, setfit-multilabel-one-vs-rest-batchsize-128, setfit-multilabel-multioutput-batchsize-128, setfit-multilabel-example-classifier-chain-25iters, setfit-multilabel-example-classifier-chain, setfit-test, my-setfit-model-dataset-PG-OCR-3, my-setfit-model-dataset-PG-OCR-2, my-setfit-model-dataset-PG-OCR, my-setfit-model, setfit-bge-small-v1.5-sst2-8-shot-talk2loop, setfit-bge-small-v1.5-sst2-8-shot, empathy_model2, empathy_model, so_mpnet-base_question_classifier, sbert-questionclassifier, setfit-contracts-clauses, setfit-xlm-bank-tweets-processed-80, setfit-minilm-bank-tweets-processed-400, setfit-minilm-bank-tweets-processed-200, setfit-minilm-bank-tweets-processed-100, setfit-mental-bert-base-uncased-Suicidal-Topic-Check, setfit-FacebookAI-roberta-base-phatic, paraphrase-mpnet-base-setfit-testing, setfit-ll-MiniLM-L6-v2-email-fraud-2024-05-18, ft-intent-bank, FT-mDeBERTa-v3-base-mnli-xnli, setfit-paraphrase-mpnet-base-v2-type, setfit-paraphrase-mpnet-base-v2-business-type, metatext_models, paraphrase-MiniLM-L3-v2-93dataset-v3labels, paraphrase-MiniLM-L3-v2-93dataset-v2labels, paraphrase-MiniLM-L3-v2-93dataset, e5-base-v2-v3labels, bge-small-en-v1.5-93dataset, setfit-bert-uncased-fibe, setfit-paraphrase-mpnet-base-v2-sst2-model2, setfit-indo-resto-RM-ibu-imas-polarity, setfit-indo-resto-RM-ibu-imas-aspect, setfit-model-24-3, catastrophy8, catastrophy6, catastrophy5, catastrophy4, catastrophy, sentence_independancy_model, LLM_response_evaluator, setfit-absa-books-polarity, setfit-absa-books-aspect, setfit-paraphrase-mpnet-base-v2-surepath-chatgtp-dataset, setfit-me5-large-instruct-v3, amd-partial-v1, bge-small-en-v1.5-brahmaputra-iter-10, bge-large-en-v1.5-brahmaputra-qa-lookup-iter-1-2-epoch, bge-large-en-v1.5-brahmaputra-iter-9-2nd-1-epoch, bge-large-en-v1.5-brahmaputra-iter-9-1-epoch, bge-large-en-v1.5-brahmaputra-iter-8-2nd-1-epoch, bge-large-en-v1.5-brahmaputra-iter-8-2-epoch, GIST-all-MiniLM-L6-v2-Bankbot-Conversation, topic_temporary_setfitv2, bge-small-en-v1.5-SetFit-FSA, pr_ebsa_fr_tran_merged25_e1_beginning_offsets_10_v3, pr_ebsa_fr_tran_merged25_e1_beginning_offsets_10, pr_ebsa_fr_merged25_offsets_10_v3, pr_ebsa_fr_merged25_offsets_10, setfit-model-ESG-social, setfit-model-ESG-governance, setfit-model-ESG-environmental, master_main_item_top_bt, master_item_top_fd, master_item_top_el_flat, master_item_top_bt_flat, master_item_top_bt9, master_item_top_bt8, master_item_top_bt7, master_item_top_bt6, master_item_top_bt5_test_flat, master_item_top_bt5, master_item_top_bt4, master_item_top_bt3, master_item_top_bt2, master_item_top_bt13, master_item_top_bt12, master_item_top_bt11, master_item_top_bt10, master_item_top_bt1, master_item_lh, master_item_fd, master_item_el, master_item_bt_test_org_notcate, master_item_bt_test_org_gtcate, master_item_bt_test_org_allcate, master_item_bt_test_org, master_item_bt_test_flat_top_flat, master_item_bt_test_flat_top, master_item_bt_test_flat, master_item_bt_test, master_item_bt_setfit, master_item_ap, master_item_ac, master_domain, master_cate_top_fd5, master_cate_top_fd4, master_cate_top_fd3, master_cate_top_fd1, master_cate_top_fd0, master_cate_top_bt6_3_test_flat, master_cate_top_bt6_3, master_cate_top_bt5_4_test_flat, master_cate_top_bt5_4, master_cate_top_bt13_9_test_flat, master_cate_top_bt13_9, master_cate_top_bt13_3_test_flat, master_cate_top_bt13_3, master_cate_sl21, master_cate_lh9, master_cate_lh8, master_cate_lh7, master_cate_lh6, master_cate_lh5, master_cate_lh4, master_cate_lh3, master_cate_lh27, master_cate_lh26, master_cate_lh25, master_cate_lh24, master_cate_lh23, master_cate_lh22, master_cate_lh21, master_cate_lh20, master_cate_lh2, master_cate_lh19, master_cate_lh18, master_cate_lh17, master_cate_lh16, master_cate_lh15, master_cate_lh14, master_cate_lh13, master_cate_lh12, master_cate_lh11, master_cate_lh10, master_cate_lh1, master_cate_lh0, master_cate_fd9, master_cate_fd8, master_cate_fd7, master_cate_fd6, master_cate_fd5, master_cate_fd4, master_cate_fd3, master_cate_fd21, master_cate_fd20, master_cate_fd2, master_cate_fd18, master_cate_fd17, master_cate_fd16, master_cate_fd15, master_cate_fd14, master_cate_fd13, master_cate_fd12, master_cate_fd11, master_cate_fd10, master_cate_fd1, master_cate_fd0, master_cate_el8, master_cate_el7, master_cate_el5, master_cate_el4, master_cate_el3, master_cate_el25, master_cate_el23, master_cate_el22, master_cate_el20, master_cate_el2, master_cate_el19, master_cate_el18, master_cate_el17, master_cate_el16, master_cate_el15, master_cate_el14, master_cate_el13, master_cate_el12, master_cate_el11, master_cate_el10, master_cate_el1, master_cate_el0, master_cate_bt_top9_test, master_cate_bt_top8_test, master_cate_bt_top7_test, master_cate_bt_top6_test, master_cate_bt_top5_test, master_cate_bt_top4_test, master_cate_bt_top3_test, master_cate_bt_top2_test, master_cate_bt_top1_test, master_cate_bt_top13_test, master_cate_bt_top12_test, master_cate_bt_top11_test, master_cate_bt_top10_test, master_cate_bt9_test, master_cate_bt8_test_flat_top_cate, master_cate_bt8_test, master_cate_bt7_test_flat_top_cate, master_cate_bt7_test, master_cate_bt6_test_flat_top_cate, master_cate_bt6_test, master_cate_bt5_test_flat_top_cate, master_cate_bt5_test, master_cate_bt4_test_flat_top_cate, master_cate_bt4_test, master_cate_bt3_test_flat_top_cate, master_cate_bt3_test, master_cate_bt2_test_flat_top_cate, master_cate_bt2_test, master_cate_bt1_test_flat_top_cate, master_cate_bt1_test, master_cate_bt12_test, master_cate_bt11_test, master_cate_bt10_test, master_cate_bt0_test_flat_top_cate, master_cate_bt0_test, master_cate_bc7, master_cate_bc5, master_cate_bc12, master_cate_ap3, master_cate_ap2, master_cate_ap1, master_cate_ap0, master_cate_ac9, master_cate_ac8, master_cate_ac7, master_cate_ac6, master_cate_ac5, master_cate_ac4, master_cate_ac3, master_cate_ac2, master_cate_ac16, master_cate_ac15, master_cate_ac14, master_cate_ac13, master_cate_ac12, master_cate_ac11, master_cate_ac10, master_cate_ac1, master_cate_ac0, setfit-mltclss, all-mpnet-base-v2-absa-polarity, all-MiniLM-L6-v2-absa-aspect, BCMPIIRAB_MiniLM_HTTest, medicare_idrak, kotodama-multilingual-v4, kotodama-multilingual-v3, temporalInformation_classifier, mentalizing-class, lifestyle_disease_classifier, informational_content_classifier, PAG-annotation, setfit-multilabel, modelo_racismo_setfit_5jan24, unfreeze_body_head_setfit-paraphrase-mpnet-base-v2-ccbuzz, setfitdeepset_gelectra-base, setfit-paraphrase-mpnet-base-v2-sst2_SentEval-C, setfit-paraphrase-mpnet-base-v2-ccbuzz, hyperparameter_tuned_setfit-paraphrase-mpnet-base-v2-ccbuzz, hyperparameter_tuned_setfit-deepset-gelectra-base-ccbuzz, roberta-large-setfit-ReqORNot, setfitmkrt2, setfitmkrt, setfit-messages-multilabel-example, setfit-messages-label-v2, setfit-messages-generated-v2, setfit-messages-generated, setfit-ethos-multilabel-example, harris, setfit_test_twitter_news_syn, setfit_test_twitter_news, setfit_test_toxic_chat_syn, setfit_test_toxic_chat, setfit_test_imdb, setfit_test_arxiv_classification_syn, setfit_test_ag_news_syn, setfit_test_ag_news, anyclassifier_setfit_demo, albert-zwnj-wnli-mean-tokens, Setfit_bert-zwnj-wnli-mean-tokens, kaustubh_setfit_1iteration, kaustubh_setfit, is_organizational_model, crypto_organization_infer_model_setfit, crypto_individual_infer_model_setfit, mtg-coloridentity-multilabel-classification, LiderzyAI-homestyle-reklamacje, 725_model_v5, 725_model_v4, 725_model_v3, 725_model_v2, 725_32batch_150_sample, setfit-proj4-label, setfit-multilabel-test, proj8-lab1, setfit-model-intent-classification-insurance, paraphrase-multilingual-mpnet-base-v2_setfit-lemonde-french, gbert-large-stance-multiculturalism, hin-v001-trainer, setfit-indo-absa-restaurant-polarity, setfit-indo-absa-restaurant-aspect, indo-setfit-bert-base-p3, indo-setfit-bert-base-p2, indo-setfit-bert-base-p1, test-trainer-alternate, setfit-paraphrase-multilingual-MiniLM-L12-v2-ig-fa, phantom-dispatch-02, transactional_model, risk_model, modality_model, cluster-ntrials-400, setfit_classifier, fairness_model, bert-leg-al-setfit, fewshot-model, emotionSample, setfit-model, bge-small-en-v1.5_setfit-sst2-english, setfit-jnj-relevancy, setfit_finetuned_iaf_98, paraphrase-multilingual-mpnet-klimacoder_v0.8, klimacoder_speedlimit_v0.1, klimacoder_protest_v0.1, klimacoder2_v0.1, distiluse-base-multilingual-cased-klimacoder_v0.9, all-mpnet-base-klimacoder_v0.7, MiniLM-klimacoder_v0.5, MiniLM-klimacoder_v0.2, MiniLM-klimacoder_v0.1, MiniLM-ispolitical-zeroshot, MiniLM-ispolitical-german-zeroshot_v0.1, SentimentClassifierDune64shot, SentimentClassifierDune-8shot, SentimentClassifierBarbieDune-8shot, journey, setfit-multilabel-example, eyeR-category2-multilabel, eyeR-category1-multilabel, setfit-paraphrase-mpnet-base-v2-sst2, setfit-bge-small-fibe-v3, setfit-all-minilm-l6-v2-malay_en_cn_sentiment_analysis, wolo-app-categories-setfit-model, setfitabsa-polarity, setfitabsa-aspect, pc_components_classifier, setfit_reuters21578_reducedto15, setfit_mpnet_reuters21578_reducedto15, improve-G3-setfit-model, doubt_repetition_with_noPropaganda_with_3_zeros_SetFit, doubt_repetition_with_noPropaganda_multiclass_SetFit, doubt_repetition_with_noPropaganda_SetFit, appeal-to-authority-setfit-model, Roberta-large-G3-setfit-model, G3-setfit-model, G2_replace_Whata_repetition_with_noPropaganda_SetFit, G2-with-noPropaganda-multilabel-setfit-model, G2-multilabel-setfit-model, G1-setfit-model, setfit-categorization, test-model-label2-MiniLMVERSION2, test-model-label1-MiniLMVERSION2, test-model-label1-MiniLM, setfit_ps, setfit_ar_ubc_hs, setfit_ar_sst2, setfit_ar_hs_mb, setfit_ar_hs, setfit_ar_100k_reviews, st-mpnet-v2-amazon-mi, stsitgesreranking, greetings-v1, fs_setfit_hybrid2, setfit-paraphrase-modernbert-embed-base-sst2, dimension3_setfit_BAAI, dimension3_setfit, dimension2_wo_thesis_setfit_BAAI, dimension2_wo_thesis_setfit, dimension2_w_thesis_setfit, dimension1_setfit, L1-classifier, BCG-classifier, pn_experiment_v02, modelo_setfit_politica_BA, setfit-indosentencebert-indonlusmsa-8-shot, setfit-indosentencebert-indonlusmsa-32-shot, setfit-indosentencebert-indonlusmsa-16-shot, classifier_woog_plum_dual, classifier_woog_plum, classifier_woog_hkv, classifier_woog_firstbud_updated, classifier_woog_firstbud, classifier_woog, setfit-bge-small-v1.5-sst2-10-shot, Setfit_subj_all-mpnet-base-v2, Setfit_subj_SVC, gte-large-train-test-4, gte-large-train-test-3, SetFit_Cyberaviation, setfitmodel, my-awesome-setfit-model, setfit_e1_bz16_ni0_sz25000, setfit_baai_wix_qa_gpt-4o_improved-cot_chat_few_shot_remove_final_evaluation_e1_one_o, setfit_baai_wix_qa_gpt-4o_improved-cot-instructions_two_reasoning_only_reasoning_1726, setfit_baai_wix_qa_gpt-4o_improved-cot-instructions_chat_few_shot_generated_only_reas, setfit_baai_wikisum_gpt-4o_improved-cot_chat_few_shot_remove_final_evaluation_e1_larg, setfit_baai_wikisum_gpt-4o_improved-cot_chat_few_shot_remove_final_evaluation_e1_1726, setfit_baai_wikisum_gpt-4o_improved-cot-instructions_two_reasoning_remove_final_evalu, setfit_baai_wikisum_gpt-4o_improved-cot-instructions_two_reasoning_only_reasoning_172, setfit_baai_wikisum_gpt-4o_improved-cot-instructions_chat_few_shot_remove_final_evalu, setfit_baai_wikisum_gpt-4o_cot-instructions_remove_final_evaluation_e1_larger_train_1, setfit_baai_wikisum_gpt-4o_cot-few_shot_remove_final_evaluation_e1_larger_train_17270, setfit_baai_wikisum_gpt-4o_cot-few_shot_remove_final_evaluation_e1_1726757655.403782, setfit_baai_wikisum_gpt-4o_cot-few_shot-instructions_remove_final_evaluation_e1_large, setfit_baai_squad_gpt-4o_improved-cot_chat_few_shot_remove_final_evaluation_e1_larger, setfit_baai_squad_gpt-4o_improved-cot-instructions_two_reasoning_only_reasoning_17267, setfit_baai_squad_gpt-4o_improved-cot-instructions_chat_few_shot_remove_final_evaluat, setfit_baai_squad_gpt-4o_improved-cot-instructions_chat_few_shot_generated_remove_fin, setfit_baai_squad_gpt-4o_cot-instructions_remove_final_evaluation_e1_larger_train_172, setfit_baai_squad_gpt-4o_cot-few_shot_remove_final_evaluation_e1_larger_train_1727019, setfit_baai_oversampling_2k, setfit_baai_newrelic_gpt-4o_improved-cot_chat_few_shot_only_reasoning_1726751494.1082, setfit_baai_newrelic_gpt-4o_improved-cot-instructions_two_reasoning_remove_final_eval, setfit_baai_newrelic_gpt-4o_improved-cot-instructions_chat_few_shot_remove_final_eval, setfit_baai_newrelic_gpt-4o_improved-cot-instructions_chat_few_shot_only_reasoning_17, setfit_baai_newrelic_gpt-4o_improved-cot-instructions_chat_few_shot_generated_remove_, setfit_baai_newrelic_gpt-4o_improved-cot-instructions_chat_few_shot_generated_only_re, setfit_baai_cybereason_gpt-4o_improved-cot-instructions_two_reasoning_remove_final_ev, setfit_baai_cybereason_gpt-4o_improved-cot-instructions_two_reasoning_only_reasoning_, setfit_baai_cybereason_gpt-4o_improved-cot-instructions_chat_few_shot_generated_remov, setfit_baai_cybereason_gpt-4o_improved-cot-instructions_chat_few_shot_generated_only_, setfit_baai_cybereason_gpt-4o_cot-instructions_remove_final_evaluation_e2_one_out_172, setfit_baai_cybereason_gpt-4o_cot-instructions_remove_final_evaluation_e2_larger_trai, setfit_baai_cybereason_gpt-4o_cot-few_shot_only_reasoning_1726751740.333197, setfit_baai_cybereason_gpt-4o_cot-few_shot-instructions_remove_final_evaluation_e1_on, setfit-MiniLM-mpnet-absa-tesla-tweet-polarity, setfit-MiniLM-mpnet-absa-tesla-tweet-aspect, mams-ds-setfit-MiniLM-mpnet-absa-tesla-tweet-polarity, mams-ds-setfit-MiniLM-mpnet-absa-tesla-tweet-aspect, text2sql_encoder, sroberta-embedding, setfit-generated-data-test-on-specific-cases, SetFit-few-shot-classification-sst2, fine-tuned-gist, ABSA_Polarity_IT, ABSA_Polarity_EN, ABSA_Aspect_IT, ABSA_Aspect_EN, automated-essay-scoring-setfit, setfit-newsapi, setfit-hub-report, setfit-hub-multilabel-example, setfit-bbc-news, setfit-bge-small-v1.5-sst2-50-shot, Unifeed, fabric_model_1, fabric_model, all_mpnetcric-setfit-model, sigma-cls, Swag-multi-class-8, Swag-multi-class-6, Swag-multi-class-4, Swag-multi-class-20, Swag-multi-class-10, SemEval-multi-label-v2, SemEval-multi-label-v1, SemEval-multi-class-8, SemEval-multi-class-6, SemEval-multi-class-4, SemEval-multi-class-20, BEA2019-multi-class-8, BEA2019-multi-class-6, BEA2019-multi-class-4, BEA2019-multi-class-20, BEA2019-multi-class-10, gte-large-setfit-train-test2, gte-large-setfit-train-b77-test3, setfit-particular-transaction-solon-embeddings-labels-large-v4, SERVICE_LARGE_MODEL_ZEON, setfit-paraphrase-multilingual-MiniLM-L12-v2-ed-fr, setfit-paraphrase-multilingual-MiniLM-L12-v2-ed-balanced-fr-AI4ED, TARGET-VULNERABILITY-multiclass-mpnet, ABSA_mpnet_MiniLM-L6-polarity, ABSA_mpnet_MiniLM-L6-aspect, ABSA_indo-sentence-bert-large_MiniLM-L6-polarity, ABSA_bert-base_MiniLM-L6-polarity, ABSA_Roberta-large_MiniLM-L6-polarity, LaBSE-based-Arabic-News-Classifier, setfit-demoModel, setfit-tryv1, SciGenSetfit5, SciGenSetfit3, SciGenSetfit24Binary, SciGenSetfit24, SciGenSetfit, SciFunctions, emb_classifier_model, physics-classifier-model, setfit-bankintents-model, GIST-small-Embedding-v0-checkthat-fitset-256, GIST-small-Embedding-v0-checkthat-fitset-128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>CCC-wav2vec 2.0: Clustering aided Cross Contrastive Self-supervised\\n  learning of speech representations</td>\n",
       "      <td>While Self-Supervised Learning has helped reap the benefit of the scale from\\nthe available unlabeled data, the learning paradigms are continuously being\\nbettered. We present a new pre-training strategy named ccc-wav2vec 2.0, which\\nuses clustering and an augmentation-based cross-contrastive loss as its\\nself-supervised objective. Through the clustering module, we scale down the\\ninfluence of those negative examples that are highly similar to the positive.\\nThe Cross-Contrastive loss is computed between the encoder output of the\\noriginal sample and the quantizer output of its augmentation and vice-versa,\\nbringing robustness to the pre-training strategy. ccc-wav2vec 2.0 achieves up\\nto 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on\\nthe test-clean and test-other sets, respectively, of LibriSpeech, without the\\nuse of any language model. The proposed method also achieves up to 14.9%\\nrelative WER improvement over the baseline wav2vec 2.0 when fine-tuned on\\nSwitchboard data. We make all our codes publicly available on GitHub.</td>\n",
       "      <td>wav2vec2-360h-base-ft-100h, ccc-wav2vec2-base-100h, ccc-wav2vec2-360h-base-ft-100h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>\n",
       "      <td>While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io</td>\n",
       "      <td>Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.005-128K-code-COT, Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Generative Language Models for Paragraph-Level Question Generation</td>\n",
       "      <td>Powerful generative models have led to recent progress in question generation\\n(QG). However, it is difficult to measure advances in QG research since there\\nare no standardized resources that allow a uniform comparison among approaches.\\nIn this paper, we introduce QG-Bench, a multilingual and multidomain benchmark\\nfor QG that unifies existing question answering datasets by converting them to\\na standard QG setting. It includes general-purpose datasets such as SQuAD for\\nEnglish, datasets from ten domains and two styles, as well as datasets in eight\\ndifferent languages. Using QG-Bench as a reference, we perform an extensive\\nanalysis of the capabilities of language models for the task. First, we propose\\nrobust QG baselines based on fine-tuning generative language models. Then, we\\ncomplement automatic evaluation based on standard metrics with an extensive\\nmanual evaluation, which in turn sheds light on the difficulty of evaluating QG\\nmodels. Finally, we analyse both the domain adaptability of these models as\\nwell as the effectiveness of multilingual models in languages other than\\nEnglish. QG-Bench is released along with the fine-tuned models presented in the\\npaper https://github.com/asahi417/lm-question-generation, which are also\\navailable as a demo https://autoqg.net/.</td>\n",
       "      <td>mt5-small-trimmed-ru-ruquad-qg, mt5-small-trimmed-ru-ruquad-qa, mt5-small-trimmed-ru-90000-ruquad-qg, mt5-small-trimmed-ru-90000-ruquad-qa, mt5-small-trimmed-ru-60000-ruquad-qg, mt5-small-trimmed-ru-60000-ruquad-qa, mt5-small-trimmed-ru-5000-ruquad-qg, mt5-small-trimmed-ru-5000-ruquad-qa, mt5-small-trimmed-ru-30000-ruquad-qg, mt5-small-trimmed-ru-30000-ruquad-qa, mt5-small-trimmed-ru-15000-ruquad-qg, mt5-small-trimmed-ru-15000-ruquad-qa, mt5-small-trimmed-ru-120000-ruquad-qg, mt5-small-trimmed-ru-120000-ruquad-qa, mt5-small-trimmed-ru-10000-ruquad-qg, mt5-small-trimmed-ru-10000-ruquad-qa, mt5-small-trimmed-ko-koquad-qg, mt5-small-trimmed-ko-koquad-qa, mt5-small-trimmed-ko-60000-koquad-qg, mt5-small-trimmed-ko-60000-koquad-qa, mt5-small-trimmed-ko-5000-koquad-qg, mt5-small-trimmed-ko-5000-koquad-qa, mt5-small-trimmed-ko-30000-koquad-qg, mt5-small-trimmed-ko-30000-koquad-qa, mt5-small-trimmed-ko-15000-koquad-qg, mt5-small-trimmed-ko-15000-koquad-qa, mt5-small-trimmed-ko-10000-koquad-qg, mt5-small-trimmed-ko-10000-koquad-qa, mt5-small-trimmed-ja-jaquad-qg, mt5-small-trimmed-ja-jaquad-qa, mt5-small-trimmed-ja-90000-jaquad-qg, mt5-small-trimmed-ja-90000-jaquad-qa, mt5-small-trimmed-ja-60000-jaquad-qg, mt5-small-trimmed-ja-60000-jaquad-qa, mt5-small-trimmed-ja-5000-jaquad-qg, mt5-small-trimmed-ja-5000-jaquad-qa, mt5-small-trimmed-ja-30000-jaquad-qg, mt5-small-trimmed-ja-30000-jaquad-qa, mt5-small-trimmed-ja-15000-jaquad-qg, mt5-small-trimmed-ja-15000-jaquad-qa, mt5-small-trimmed-ja-120000-jaquad-qg, mt5-small-trimmed-ja-120000-jaquad-qa, mt5-small-trimmed-ja-10000-jaquad-qg, mt5-small-trimmed-ja-10000-jaquad-qa, mt5-small-trimmed-it-itquad-qg, mt5-small-trimmed-it-itquad-qa, mt5-small-trimmed-it-90000-itquad-qg, mt5-small-trimmed-it-90000-itquad-qa, mt5-small-trimmed-it-60000-itquad-qg, mt5-small-trimmed-it-60000-itquad-qa, mt5-small-trimmed-it-5000-itquad-qg, mt5-small-trimmed-it-5000-itquad-qa, mt5-small-trimmed-it-30000-itquad-qg, mt5-small-trimmed-it-30000-itquad-qa, mt5-small-trimmed-it-15000-itquad-qg, mt5-small-trimmed-it-15000-itquad-qa, mt5-small-trimmed-it-10000-itquad-qg, mt5-small-trimmed-it-10000-itquad-qa, mt5-small-trimmed-fr-frquad-qg, mt5-small-trimmed-fr-frquad-qa, mt5-small-trimmed-fr-90000-frquad-qg, mt5-small-trimmed-fr-90000-frquad-qa, mt5-small-trimmed-fr-60000-frquad-qg, mt5-small-trimmed-fr-60000-frquad-qa, mt5-small-trimmed-fr-5000-frquad-qg, mt5-small-trimmed-fr-5000-frquad-qa, mt5-small-trimmed-fr-30000-frquad-qg, mt5-small-trimmed-fr-30000-frquad-qa, mt5-small-trimmed-fr-15000-frquad-qg, mt5-small-trimmed-fr-15000-frquad-qa, mt5-small-trimmed-fr-120000-frquad-qg, mt5-small-trimmed-fr-120000-frquad-qa, mt5-small-trimmed-fr-10000-frquad-qg, mt5-small-trimmed-fr-10000-frquad-qa, mt5-small-trimmed-es-esquad-qg, mt5-small-trimmed-es-esquad-qa, mt5-small-trimmed-es-90000-esquad-qg, mt5-small-trimmed-es-90000-esquad-qa, mt5-small-trimmed-es-60000-esquad-qg, mt5-small-trimmed-es-60000-esquad-qa, mt5-small-trimmed-es-5000-esquad-qg, mt5-small-trimmed-es-5000-esquad-qa, mt5-small-trimmed-es-30000-esquad-qg, mt5-small-trimmed-es-30000-esquad-qa, mt5-small-trimmed-es-15000-esquad-qg, mt5-small-trimmed-es-15000-esquad-qa, mt5-small-trimmed-es-120000-esquad-qg, mt5-small-trimmed-es-120000-esquad-qa, mt5-small-trimmed-es-10000-esquad-qg, mt5-small-trimmed-es-10000-esquad-qa, mt5-small-trimmed-en-squad-qg, mt5-small-trimmed-en-squad-qa, mt5-small-trimmed-en-enquad-qg, mt5-small-trimmed-en-90000-squad-qg, mt5-small-trimmed-en-90000-squad-qa, mt5-small-trimmed-en-60000-squad-qg, mt5-small-trimmed-en-60000-squad-qa, mt5-small-trimmed-en-5000-squad-qg, mt5-small-trimmed-en-5000-squad-qa, mt5-small-trimmed-en-30000-squad-qg, mt5-small-trimmed-en-30000-squad-qa, mt5-small-trimmed-en-15000-squad-qg, mt5-small-trimmed-en-15000-squad-qa, mt5-small-trimmed-en-120000-squad-qg, mt5-small-trimmed-en-120000-squad-qa, mt5-small-trimmed-en-10000-squad-qg, mt5-small-trimmed-en-10000-squad-qa, mbart-large-cc25-trimmed-ru-ruquad-qg, mbart-large-cc25-trimmed-ru-ruquad-qa, mbart-large-cc25-trimmed-ko-koquad-qg, mbart-large-cc25-trimmed-ko-koquad-qa, mbart-large-cc25-trimmed-ja-jaquad-qg, mbart-large-cc25-trimmed-ja-jaquad-qa, mbart-large-cc25-trimmed-it-itquad-qg, mbart-large-cc25-trimmed-it-itquad-qa, mbart-large-cc25-trimmed-fr-frquad-qg, mbart-large-cc25-trimmed-fr-frquad-qa, mbart-large-cc25-trimmed-es-esquad-qg, mbart-large-cc25-trimmed-es-esquad-qa, mbart-large-cc25-trimmed-en-squad-qg, mbart-large-cc25-trimmed-en-squad-qa, t5-small-tweetqa-qag-np, t5-small-subjqa-vanilla-tripadvisor-qg, t5-small-subjqa-vanilla-restaurants-qg, t5-small-subjqa-vanilla-movies-qg, t5-small-subjqa-vanilla-grocery-qg, t5-small-subjqa-vanilla-electronics-qg, t5-small-subjqa-vanilla-books-qg, t5-small-subjqa-tripadvisor-qg, t5-small-subjqa-restaurants-qg, t5-small-subjqa-movies-qg, t5-small-subjqa-grocery-qg, t5-small-subjqa-electronics-qg, t5-small-subjqa-books-qg, t5-small-squadshifts-vanilla-reddit-qg, t5-small-squadshifts-vanilla-nyt-qg, t5-small-squadshifts-vanilla-new_wiki-qg, t5-small-squadshifts-vanilla-amazon-qg, t5-small-squadshifts-reddit-qg, t5-small-squadshifts-nyt-qg, t5-small-squadshifts-new_wiki-qg, t5-small-squadshifts-amazon-qg, t5-small-squad-qg-no-paragraph, t5-small-squad-qg-no-answer, t5-small-squad-qg-default, t5-large-tweetqa-qag-np, t5-large-subjqa-vanilla-tripadvisor-qg, t5-large-subjqa-vanilla-restaurants-qg, t5-large-subjqa-vanilla-movies-qg, t5-large-subjqa-vanilla-grocery-qg, t5-large-subjqa-vanilla-electronics-qg, t5-large-subjqa-vanilla-books-qg, t5-large-subjqa-tripadvisor-qg, t5-large-subjqa-restaurants-qg, t5-large-subjqa-movies-qg, t5-large-subjqa-grocery-qg, t5-large-subjqa-electronics-qg, t5-large-subjqa-books-qg, t5-large-squadshifts-vanilla-reddit-qg, t5-large-squadshifts-vanilla-nyt-qg, t5-large-squadshifts-vanilla-new_wiki-qg, t5-large-squadshifts-vanilla-amazon-qg, t5-large-squadshifts-reddit-qg, t5-large-squadshifts-nyt-qg, t5-large-squadshifts-new_wiki-qg, t5-large-squadshifts-amazon-qg, t5-large-squad-qg-no-paragraph, t5-large-squad-qg-no-answer, t5-large-squad-qg-default, t5-base-tweetqa-qag-np, t5-base-subjqa-vanilla-tripadvisor-qg, t5-base-subjqa-vanilla-restaurants-qg, t5-base-subjqa-vanilla-movies-qg, t5-base-subjqa-vanilla-grocery-qg, t5-base-subjqa-vanilla-electronics-qg, t5-base-subjqa-vanilla-books-qg, t5-base-subjqa-tripadvisor-qg, t5-base-subjqa-restaurants-qg, t5-base-subjqa-movies-qg, t5-base-subjqa-grocery-qg, t5-base-subjqa-electronics-qg, t5-base-subjqa-books-qg, t5-base-squadshifts-vanilla-reddit-qg, t5-base-squadshifts-vanilla-nyt-qg, t5-base-squadshifts-vanilla-new_wiki-qg, t5-base-squadshifts-vanilla-amazon-qg, t5-base-squadshifts-reddit-qg, t5-base-squadshifts-nyt-qg, t5-base-squadshifts-new_wiki-qg, t5-base-squadshifts-amazon-qg, t5-base-squad-qg-no-paragraph, t5-base-squad-qg-no-answer, t5-base-squad-qg-default, mt5-base-trimmed-ko-15000-koquad-qg, mt5-base-trimmed-de-15000-dequad-qg, mbart-large-cc25-squad-qg, mbart-large-cc25-squad-qa, mbart-large-cc25-ruquad-qg-ae, mbart-large-cc25-ruquad-qg, mbart-large-cc25-ruquad-qag, mbart-large-cc25-ruquad-qa, mbart-large-cc25-ruquad-ae, mbart-large-cc25-koquad-qg-ae, mbart-large-cc25-koquad-qg, mbart-large-cc25-koquad-qag, mbart-large-cc25-koquad-qa, mbart-large-cc25-koquad-ae, mbart-large-cc25-jaquad-qg-ae, mbart-large-cc25-jaquad-qg, mbart-large-cc25-jaquad-qag, mbart-large-cc25-jaquad-qa, mbart-large-cc25-jaquad-ae, mbart-large-cc25-itquad-qg-ae, mbart-large-cc25-itquad-qg, mbart-large-cc25-itquad-qag, mbart-large-cc25-itquad-qa, mbart-large-cc25-itquad-ae, mbart-large-cc25-frquad-qg-ae, mbart-large-cc25-frquad-qg, mbart-large-cc25-frquad-qag, mbart-large-cc25-frquad-qa, mbart-large-cc25-frquad-ae, mbart-large-cc25-esquad-qg-ae, mbart-large-cc25-esquad-qg, mbart-large-cc25-esquad-qag, mbart-large-cc25-esquad-qa, mbart-large-cc25-esquad-ae, mbart-large-cc25-dequad-qg-ae, mbart-large-cc25-dequad-qg, mbart-large-cc25-dequad-qag, mbart-large-cc25-dequad-qa, mbart-large-cc25-dequad-ae, bart-large-subjqa-vanilla-tripadvisor-qg, bart-large-subjqa-vanilla-restaurants-qg, bart-large-subjqa-vanilla-movies-qg, bart-large-subjqa-vanilla-grocery-qg, bart-large-subjqa-vanilla-electronics-qg, bart-large-subjqa-vanilla-books-qg, bart-large-subjqa-tripadvisor-qg, bart-large-subjqa-restaurants-qg, bart-large-subjqa-movies-qg, bart-large-subjqa-grocery-qg, bart-large-subjqa-electronics-qg, bart-large-subjqa-books-qg, bart-large-squadshifts-vanilla-reddit-qg, bart-large-squadshifts-vanilla-nyt-qg, bart-large-squadshifts-vanilla-new_wiki-qg, bart-large-squadshifts-vanilla-amazon-qg, bart-large-squadshifts-reddit-qg, bart-large-squadshifts-nyt-qg, bart-large-squadshifts-new_wiki-qg, bart-large-squadshifts-amazon-qg, bart-large-squad-qg-no-answer, bart-large-squad-qg-default, bart-base-subjqa-vanilla-tripadvisor-qg, bart-base-subjqa-vanilla-restaurants-qg, bart-base-subjqa-vanilla-movies-qg, bart-base-subjqa-vanilla-grocery-qg, bart-base-subjqa-vanilla-electronics-qg, bart-base-subjqa-vanilla-books-qg, bart-base-subjqa-tripadvisor-qg, bart-base-subjqa-restaurants-qg, bart-base-subjqa-movies-qg, bart-base-subjqa-grocery-qg, bart-base-subjqa-electronics-qg, bart-base-subjqa-books-qg, bart-base-squadshifts-vanilla-reddit-qg, bart-base-squadshifts-vanilla-nyt-qg, bart-base-squadshifts-vanilla-new_wiki-qg, bart-base-squadshifts-vanilla-amazon-qg, bart-base-squadshifts-reddit-qg, bart-base-squadshifts-nyt-qg, bart-base-squadshifts-new_wiki-qg, bart-base-squadshifts-amazon-qg, bart-base-squad-qg-no-answer, bart-base-squad-qg-default, t5-small-tweetqa-qag, t5-small-tweetqa-qa, t5-small-squad-qg-ae, t5-small-squad-qg, t5-small-squad-qag, t5-small-squad-qa, t5-small-squad-ae, t5-large-tweetqa-qag, t5-large-tweetqa-qa, t5-large-squad-qg-ae, t5-large-squad-qg, t5-large-squad-qag, t5-large-squad-ae, t5-base-tweetqa-qag, t5-base-tweetqa-qa, t5-base-squad-qg-ae, t5-base-squad-qg, t5-base-squad-qag, t5-base-squad-ae, mt5-small-zhquad-qg-ae, mt5-small-zhquad-qg, mt5-small-zhquad-qag, mt5-small-zhquad-ae, mt5-small-squad-qg, mt5-small-squad-qa, mt5-small-ruquad-qg-ae, mt5-small-ruquad-qg, mt5-small-ruquad-qag, mt5-small-ruquad-qa, mt5-small-ruquad-ae, mt5-small-koquad-qg-ae, mt5-small-koquad-qg, mt5-small-koquad-qag, mt5-small-koquad-qa, mt5-small-koquad-ae, mt5-small-jaquad-qg-ae, mt5-small-jaquad-qg, mt5-small-jaquad-qag, mt5-small-jaquad-qa, mt5-small-jaquad-ae, mt5-small-itquad-qg-ae, mt5-small-itquad-qg, mt5-small-itquad-qag, mt5-small-itquad-qa, mt5-small-itquad-ae, mt5-small-frquad-qg-ae, mt5-small-frquad-qg, mt5-small-frquad-qag, mt5-small-frquad-qa, mt5-small-frquad-ae, mt5-small-esquad-qg-ae, mt5-small-esquad-qg, mt5-small-esquad-qag, mt5-small-esquad-qa, mt5-small-esquad-ae, mt5-small-dequad-qg-ae, mt5-small-dequad-qg, mt5-small-dequad-qag, mt5-small-dequad-qa, mt5-small-dequad-ae, mt5-base-zhquad-qg-ae, mt5-base-zhquad-qg, mt5-base-zhquad-qag, mt5-base-zhquad-ae, mt5-base-squad-qg, mt5-base-ruquad-qg-ae, mt5-base-ruquad-qg, mt5-base-ruquad-qag, mt5-base-ruquad-ae, mt5-base-koquad-qg-ae, mt5-base-koquad-qg, mt5-base-koquad-qag, mt5-base-koquad-ae, mt5-base-jaquad-qg-ae, mt5-base-jaquad-qg, mt5-base-jaquad-qag, mt5-base-jaquad-ae, mt5-base-itquad-qg-ae, mt5-base-itquad-qg, mt5-base-itquad-qag, mt5-base-itquad-ae, mt5-base-frquad-qg-ae, mt5-base-frquad-qg, mt5-base-frquad-qag, mt5-base-frquad-ae, mt5-base-esquad-qg-ae, mt5-base-esquad-qg, mt5-base-esquad-qag, mt5-base-esquad-ae, mt5-base-dequad-qg-ae, mt5-base-dequad-qg, mt5-base-dequad-qag, mt5-base-dequad-ae, flan-t5-small-squad-qg-ae, flan-t5-small-squad-qg, flan-t5-small-squad-qag, flan-t5-small-squad-ae, flan-t5-large-squad-qg-ae, flan-t5-large-squad-qg, flan-t5-large-squad-qag, flan-t5-large-squad-ae, flan-t5-base-squad-qg-ae, flan-t5-base-squad-qg, flan-t5-base-squad-qag, flan-t5-base-squad-ae, bart-large-tweetqa-qag, bart-large-tweetqa-qa, bart-large-squad-qg-ae, bart-large-squad-qg, bart-large-squad-qag, bart-large-squad-ae, bart-base-tweetqa-qag, bart-base-tweetqa-qa, bart-base-squad-qg-ae, bart-base-squad-qg, bart-base-squad-qag, bart-base-squad-ae, t5-small-squad-qag-recreated, t5-small-squad-newsqa-qag-trained, t5-small-newsqa-qag-trained, t5-small-newsqa-qag-finetuned, t5-small-newsqa-modified-qag-finetuned, t5-base-newsqa-qag-trained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>MTEB: Massive Text Embedding Benchmark</td>\n",
       "      <td>Text embeddings are commonly evaluated on a small set of datasets from a\\nsingle task not covering their possible applications to other tasks. It is\\nunclear whether state-of-the-art embeddings on semantic textual similarity\\n(STS) can be equally well applied to other tasks like clustering or reranking.\\nThis makes progress in the field difficult to track, as various models are\\nconstantly being proposed without proper evaluation. To solve this problem, we\\nintroduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding\\ntasks covering a total of 58 datasets and 112 languages. Through the\\nbenchmarking of 33 models on MTEB, we establish the most comprehensive\\nbenchmark of text embeddings to date. We find that no particular text embedding\\nmethod dominates across all tasks. This suggests that the field has yet to\\nconverge on a universal text embedding method and scale it up sufficiently to\\nprovide state-of-the-art results on all embedding tasks. MTEB comes with\\nopen-source code and a public leaderboard at\\nhttps://github.com/embeddings-benchmark/mteb.</td>\n",
       "      <td>NV-Embed-v1, Linq-Embed-Mistral, speed-embedding-7b-instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Characterizing Verbatim Short-Term Memory in Neural Language Models</td>\n",
       "      <td>When a language model is trained to predict natural language sequences, its\\nprediction at each moment depends on a representation of prior context. What\\nkind of information about the prior context can language models retrieve? We\\ntested whether language models could retrieve the exact words that occurred\\npreviously in a text. In our paradigm, language models (transformers and an\\nLSTM) processed English text in which a list of nouns occurred twice. We\\noperationalized retrieval as the reduction in surprisal from the first to the\\nsecond list. We found that the transformers retrieved both the identity and\\nordering of nouns from the first list. Further, the transformers' retrieval was\\nmarkedly enhanced when they were trained on a larger corpus and with greater\\nmodel depth. Lastly, their ability to index prior tokens was dependent on\\nlearned attention patterns. In contrast, the LSTM exhibited less precise\\nretrieval, which was limited to list-initial tokens and to short intervening\\ntexts. The LSTM's retrieval was not sensitive to the order of nouns and it\\nimproved when the list was semantically coherent. We conclude that transformers\\nimplemented something akin to a working memory system that could flexibly\\nretrieve individual token representations across arbitrary delays; conversely,\\nthe LSTM maintained a coarser and more rapidly-decaying semantic gist of prior\\ntokens, weighted toward the earliest items.</td>\n",
       "      <td>gpt2_wt103_12-layer, gpt2_wt103-40m_12-layer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Iterative pseudo-forced alignment by acoustic CTC loss for\\n  self-supervised ASR domain adaptation</td>\n",
       "      <td>High-quality data labeling from specific domains is costly and human\\ntime-consuming. In this work, we propose a self-supervised domain adaptation\\nmethod, based upon an iterative pseudo-forced alignment algorithm. The produced\\nalignments are employed to customize an end-to-end Automatic Speech Recognition\\n(ASR) and iteratively refined. The algorithm is fed with frame-wise character\\nposteriors produced by a seed ASR, trained with out-of-domain data, and\\noptimized throughout a Connectionist Temporal Classification (CTC) loss. The\\nalignments are computed iteratively upon a corpus of broadcast TV. The process\\nis repeated by reducing the quantity of text to be aligned or expanding the\\nalignment window until finding the best possible audio-text alignment. The\\nstarting timestamps, or temporal anchors, are produced uniquely based on the\\nconfidence score of the last aligned utterance. This score is computed with the\\npaths of the CTC-alignment matrix. With this methodology, no human-revised text\\nreferences are required. Alignments from long audio files with low-quality\\ntranscriptions, like TV captions, are filtered out by confidence score and\\nready for further ASR adaptation. The obtained results, on both the Spanish\\nRTVE2022 and CommonVoice databases, underpin the feasibility of using CTC-based\\nsystems to perform: highly accurate audio-text alignments, domain adaptation\\nand semi-supervised training of end-to-end ASR.</td>\n",
       "      <td>asr-wav2vec2-commonvoice-es-finetuned-rtve, asr-wav2vec2-commonvoice-es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>GPTQ: Accurate Post-Training Quantization for Generative Pre-trained\\n  Transformers</td>\n",
       "      <td>Generative Pre-trained Transformer models, known as GPT or OPT, set\\nthemselves apart through breakthrough performance across complex language\\nmodelling tasks, but also by their extremely high computational and storage\\ncosts. Specifically, due to their massive size, even inference for large,\\nhighly-accurate GPT models may require multiple performant GPUs, which limits\\nthe usability of such models. While there is emerging work on relieving this\\npressure via model compression, the applicability and performance of existing\\ncompression techniques is limited by the scale and complexity of GPT models. In\\nthis paper, we address this challenge, and propose GPTQ, a new one-shot weight\\nquantization method based on approximate second-order information, that is both\\nhighly-accurate and highly-efficient. Specifically, GPTQ can quantize GPT\\nmodels with 175 billion parameters in approximately four GPU hours, reducing\\nthe bitwidth down to 3 or 4 bits per weight, with negligible accuracy\\ndegradation relative to the uncompressed baseline. Our method more than doubles\\nthe compression gains relative to previously-proposed one-shot quantization\\nmethods, preserving accuracy, allowing us for the first time to execute an 175\\nbillion-parameter model inside a single GPU for generative inference. Moreover,\\nwe also show that our method can still provide reasonable accuracy in the\\nextreme quantization regime, in which weights are quantized to 2-bit or even\\nternary quantization levels. We show experimentally that these improvements can\\nbe leveraged for end-to-end inference speedups over FP16, of around 3.25x when\\nusing high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones\\n(NVIDIA A6000). The implementation is available at\\nhttps://github.com/IST-DASLab/gptq.</td>\n",
       "      <td>starcoder2-7b-quantized.w8a8, starcoder2-7b-quantized.w8a16, starcoder2-3b-quantized.w8a8, starcoder2-3b-quantized.w8a16, starcoder2-15b-quantized.w8a8, starcoder2-15b-quantized.w8a16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Crosslingual Generalization through Multitask Finetuning</td>\n",
       "      <td>Multitask prompted finetuning (MTF) has been shown to help large language\\nmodels generalize to new tasks in a zero-shot setting, but so far explorations\\nof MTF have focused on English data and models. We apply MTF to the pretrained\\nmultilingual BLOOM and mT5 model families to produce finetuned variants called\\nBLOOMZ and mT0. We find finetuning large multilingual language models on\\nEnglish tasks with English prompts allows for task generalization to\\nnon-English languages that appear only in the pretraining corpus. Finetuning on\\nmultilingual tasks with English prompts further improves performance on English\\nand non-English tasks leading to various state-of-the-art zero-shot results. We\\nalso investigate finetuning on multilingual tasks with prompts that have been\\nmachine-translated from English to match the language of each dataset. We find\\ntraining on these machine-translated prompts leads to better performance on\\nhuman-written prompts in the respective languages. Surprisingly, we find models\\nare capable of zero-shot generalization to tasks in languages they have never\\nintentionally seen. We conjecture that the models are learning higher-level\\ncapabilities that are both task- and language-agnostic. In addition, we\\nintroduce xP3, a composite of supervised datasets in 46 languages with English\\nand machine-translated prompts. Our code, datasets and models are publicly\\navailable at https://github.com/bigscience-workshop/xmtf.</td>\n",
       "      <td>bloomz-7b1-500m-ru, bloomz-7b1-4b-xp3ru, bloomz-7b1-4b-ru, mt0-xxl-p3, mt0-small, mt0-large, mt0-base, bloomz-p3, bloomz-7b1-p3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>ATCO2 corpus: A Large-Scale Dataset for Research on Automatic Speech\\n  Recognition and Natural Language Understanding of Air Traffic Control\\n  Communications</td>\n",
       "      <td>Personal assistants, automatic speech recognizers and dialogue understanding\\nsystems are becoming more critical in our interconnected digital world. A clear\\nexample is air traffic control (ATC) communications. ATC aims at guiding\\naircraft and controlling the airspace in a safe and optimal manner. These\\nvoice-based dialogues are carried between an air traffic controller (ATCO) and\\npilots via very-high frequency radio channels. In order to incorporate these\\nnovel technologies into ATC (low-resource domain), large-scale annotated\\ndatasets are required to develop the data-driven AI systems. Two examples are\\nautomatic speech recognition (ASR) and natural language understanding (NLU). In\\nthis paper, we introduce the ATCO2 corpus, a dataset that aims at fostering\\nresearch on the challenging ATC field, which has lagged behind due to lack of\\nannotated data. The ATCO2 corpus covers 1) data collection and pre-processing,\\n2) pseudo-annotations of speech data, and 3) extraction of ATC-related named\\nentities. The ATCO2 corpus is split into three subsets. 1) ATCO2-test-set\\ncorpus contains 4 hours of ATC speech with manual transcripts and a subset with\\ngold annotations for named-entity recognition (callsign, command, value). 2)\\nThe ATCO2-PL-set corpus consists of 5281 hours of unlabeled ATC data enriched\\nwith automatic transcripts from an in-domain speech recognizer, contextual\\ninformation, speaker turn information, signal-to-noise ratio estimate and\\nEnglish language detection score per sample. Both available for purchase\\nthrough ELDA at http://catalog.elra.info/en-us/repository/browse/ELRA-S0484. 3)\\nThe ATCO2-test-set-1h corpus is a one-hour subset from the original test set\\ncorpus, that we are offering for free at https://www.atco2.org/data. We expect\\nthe ATCO2 corpus will foster research on robust ASR and NLU not only in the\\nfield of ATC communications but also in the general research community.</td>\n",
       "      <td>whisper-small-atc, wav2vec2-xls-r-300m-en-atc-uwb-atcc-and-atcosim, wav2vec2-xls-r-300m-en-atc-uwb-atcc, wav2vec2-xls-r-300m-en-atc-atcosim, wav2vec2-large-960h-lv60-self-en-atc-uwb-atcc-and-atcosim, wav2vec2-large-960h-lv60-self-en-atc-uwb-atcc, wav2vec2-large-960h-lv60-self-en-atc-atcosim, bert-base-token-classification-for-atc-en-uwb-atcc, bert-base-ner-atc-en-atco2-1h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Editing Models with Task Arithmetic</td>\n",
       "      <td>Changing how pre-trained models behave -- e.g., improving their performance\\non a downstream task or mitigating biases learned during pre-training -- is a\\ncommon practice when developing machine learning systems. In this work, we\\npropose a new paradigm for steering the behavior of neural networks, centered\\naround task vectors. A task vector specifies a direction in the weight\\nspace of a pre-trained model, such that movement in that direction improves\\nperformance on the task. We build task vectors by subtracting the weights of a\\npre-trained model from the weights of the same model after fine-tuning on a\\ntask. We show that these task vectors can be modified and combined together\\nthrough arithmetic operations such as negation and addition, and the behavior\\nof the resulting model is steered accordingly. Negating a task vector decreases\\nperformance on the target task, with little change in model behavior on control\\ntasks. Moreover, adding task vectors together can improve performance on\\nmultiple tasks at once. Finally, when tasks are linked by an analogy\\nrelationship of the form ``A is to B as C is to D\", combining task vectors from\\nthree of the tasks can improve performance on the fourth, even when no data\\nfrom the fourth task is used for training. Overall, our experiments with\\nseveral models, modalities and tasks show that task arithmetic is a simple,\\nefficient and effective way of editing models.</td>\n",
       "      <td>my-first-blend, Lelantos-low-tune, Hermes-low-tune-3.1, Hermes-low-tune-2, Mahou-1.5-mistral-nemo-12B-lorablated, Llama-3.1-Nemotron-lorablated-70B, llama-3-Nephilim-v2.1-8B, llama-3-Nephilim-v2-8B, DeepSauerHuatuoSkywork-R1-o1-Llama-3.1-8B, Open_Hermes_Orca_Mistral-7B, Ph3task3-14B, Ph3task2-14B, Ph3task1-14B, LlamaExecutor-8B-3.0.5, SeQwence-14Bv2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Robust Speech Recognition via Large-Scale Weak Supervision</td>\n",
       "      <td>We study the capabilities of speech processing systems trained simply to\\npredict large amounts of transcripts of audio on the internet. When scaled to\\n680,000 hours of multilingual and multitask supervision, the resulting models\\ngeneralize well to standard benchmarks and are often competitive with prior\\nfully supervised results but in a zero-shot transfer setting without the need\\nfor any fine-tuning. When compared to humans, the models approach their\\naccuracy and robustness. We are releasing models and inference code to serve as\\na foundation for further work on robust speech processing.</td>\n",
       "      <td>whisper-base.en, whisper-uk2en-speech-translation, kotoba-whisper-v2.0, kotoba-whisper-v1.0, whisper-small-amet, whisper-medium-v2-amet, whisper-large-v2-amet, whisper-large-v3-french-distil-dec8, whisper-large-v3-french-distil-dec4, whisper-large-v3-french-distil-dec2, whisper-large-v3-french, whisper-nl-noise, whisper-tiny-finetune-hindi-fleurs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting</td>\n",
       "      <td>The BLOOM model is a large open-source multilingual language model capable of\\nzero-shot learning, but its pretraining was limited to 46 languages. To improve\\nits zero-shot performance on unseen languages, it is desirable to adapt BLOOM,\\nbut previous works have only explored adapting small language models. In this\\nwork, we apply existing language adaptation strategies to BLOOM and benchmark\\nits zero-shot prompting performance on eight new languages. We find language\\nadaptation to be effective at improving zero-shot performance in new languages.\\nSurprisingly, adapter-based finetuning is more effective than continued\\npretraining for large models. In addition, we discover that prompting\\nperformance is not significantly affected by language specifics, such as the\\nwriting system. It is primarily determined by the size of the language\\nadaptation data. We also add new languages to BLOOMZ, which is a multitask\\nfinetuned version of BLOOM capable of following task instructions zero-shot. We\\nfind including a new language in the multitask fine-tuning mixture to be the\\nmost effective method to teach BLOOMZ a new language. We conclude that with\\nsufficient training data language adaptation can generalize well to diverse\\nlanguages. Our code is available at\\nhttps://github.com/bigscience-workshop/multilingual-modeling/.</td>\n",
       "      <td>bloomz-7b1-500m-ru, bloomz-7b1-4b-xp3ru, bloomz-7b1-4b-ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>One Embedder, Any Task: Instruction-Finetuned Text Embeddings</td>\n",
       "      <td>We introduce INSTRUCTOR, a new method for computing text embeddings given\\ntask instructions: every text input is embedded together with instructions\\nexplaining the use case (e.g., task and domain descriptions). Unlike encoders\\nfrom prior work that are more specialized, INSTRUCTOR is a single embedder that\\ncan generate text embeddings tailored to different downstream tasks and\\ndomains, without any further training. We first annotate instructions for 330\\ndiverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive\\nloss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are\\nunseen during training), ranging from classification and information retrieval\\nto semantic textual similarity and text generation evaluation. INSTRUCTOR,\\nwhile having an order of magnitude fewer parameters than the previous best\\nmodel, achieves state-of-the-art performance, with an average improvement of\\n3.4% compared to the previous best results on the 70 diverse datasets. Our\\nanalysis suggests that INSTRUCTOR is robust to changes in instructions, and\\nthat instruction finetuning mitigates the challenge of training a single model\\non diverse datasets. Our model, code, and data are available at\\nhttps://instructor-embedding.github.io.</td>\n",
       "      <td>GIST-large-Embedding-v0, GIST-Embedding-v0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>LEVER: Learning to Verify Language-to-Code Generation with Execution</td>\n",
       "      <td>The advent of pre-trained code language models (CodeLMs) has lead to\\nsignificant progress in language-to-code generation. State-of-the-art\\napproaches in this area combine CodeLM decoding with sample pruning and\\nreranking using test cases or heuristics based on the execution results.\\nHowever, it is challenging to obtain test cases for many real-world\\nlanguage-to-code applications, and heuristics cannot well capture the semantic\\nfeatures of the execution results, such as data type and value range, which\\noften indicates the correctness of the program. In this work, we propose LEVER,\\na simple approach to improve language-to-code generation by learning to verify\\nthe generated programs with their execution results. Specifically, we train\\nverifiers to determine whether a program sampled from the CodeLM is correct or\\nnot based on the natural language input, the program itself and its execution\\nresults. The sampled programs are reranked by combining the verification score\\nwith the CodeLM generation probability, and marginalizing over programs with\\nthe same execution results. On four datasets across the domains of table QA,\\nmath QA and basic Python programming, LEVER consistently improves over the base\\nCodeLMs (4.6% to 10.9% with code-davinci-002) and achieves new state-of-the-art\\nresults on all of them.</td>\n",
       "      <td>lever-wikitq-codex, lever-spider-codex, lever-mbpp-codex, lever-gsm8k-codex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Exploring the Potential of Machine Translation for Generating Named\\n  Entity Datasets: A Case Study between Persian and English</td>\n",
       "      <td>This study focuses on the generation of Persian named entity datasets through\\nthe application of machine translation on English datasets. The generated\\ndatasets were evaluated by experimenting with one monolingual and one\\nmultilingual transformer model. Notably, the CoNLL 2003 dataset has achieved\\nthe highest F1 score of 85.11%. In contrast, the WNUT 2017 dataset yielded the\\nlowest F1 score of 40.02%. The results of this study highlight the potential of\\nmachine translation in creating high-quality named entity recognition datasets\\nfor low-resource languages like Persian. The study compares the performance of\\nthese generated datasets with English named entity recognition systems and\\nprovides insights into the effectiveness of machine translation for this task.\\nAdditionally, this approach could be used to augment data in low-resource\\nlanguage or create noisy data to make named entity systems more robust and\\nimprove them.</td>\n",
       "      <td>xlm-roberta-base-wnut2017-en, xlm-roberta-base-ncbi_disease-en, xlm-roberta-base-conll2003-en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>LLaMA: Open and Efficient Foundation Language Models</td>\n",
       "      <td>We introduce LLaMA, a collection of foundation language models ranging from\\n7B to 65B parameters. We train our models on trillions of tokens, and show that\\nit is possible to train state-of-the-art models using publicly available\\ndatasets exclusively, without resorting to proprietary and inaccessible\\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\\nPaLM-540B. We release all our models to the research community.</td>\n",
       "      <td>orca_mini_v3_7b, orca_mini_v2_7b, orca_mini_v2_13b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>GPT-4 Technical Report</td>\n",
       "      <td>We report the development of GPT-4, a large-scale, multimodal model which can\\naccept image and text inputs and produce text outputs. While less capable than\\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\\nvarious professional and academic benchmarks, including passing a simulated bar\\nexam with a score around the top 10% of test takers. GPT-4 is a\\nTransformer-based model pre-trained to predict the next token in a document.\\nThe post-training alignment process results in improved performance on measures\\nof factuality and adherence to desired behavior. A core component of this\\nproject was developing infrastructure and optimization methods that behave\\npredictably across a wide range of scales. This allowed us to accurately\\npredict some aspects of GPT-4's performance based on models trained with no\\nmore than 1/1,000th the compute of GPT-4.</td>\n",
       "      <td>WizardCoder3b-gguf, openchat-3.5-0106-32k, openchat-3.5-0106-128k, openchat-3.5-0106-11b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Configurable EBEN: Extreme Bandwidth Extension Network to enhance\\n  body-conducted speech capture</td>\n",
       "      <td>This paper presents a configurable version of Extreme Bandwidth Extension\\nNetwork (EBEN), a Generative Adversarial Network (GAN) designed to improve\\naudio captured with body-conduction microphones. We show that although these\\nmicrophones significantly reduce environmental noise, this insensitivity to\\nambient noise happens at the expense of the bandwidth of the speech signal\\nacquired by the wearer of the devices. The obtained captured signals therefore\\nrequire the use of signal enhancement techniques to recover the full-bandwidth\\nspeech. EBEN leverages a configurable multiband decomposition of the raw\\ncaptured signal. This decomposition allows the data time domain dimensions to\\nbe reduced and the full band signal to be better controlled. The multiband\\nrepresentation of the captured signal is processed through a U-Net-like model,\\nwhich combines feature and adversarial losses to generate an enhanced speech\\nsignal. We also benefit from this original representation in the proposed\\nconfigurable discriminators architecture. The configurable EBEN approach can\\nachieve state-of-the-art enhancement results on synthetic data with a\\nlightweight generator that allows real-time processing.</td>\n",
       "      <td>EBEN_throat_microphone, EBEN_temple_vibration_pickup, EBEN_soft_in_ear_microphone, EBEN_rigid_in_ear_microphone, EBEN_reverse_throat_microphone, EBEN_reverse_temple_vibration_pickup, EBEN_reverse_soft_in_ear_microphone, EBEN_reverse_rigid_in_ear_microphone, EBEN_reverse_forehead_accelerometer, EBEN_forehead_accelerometer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>GreekBART: The First Pretrained Greek Sequence-to-Sequence Model</td>\n",
       "      <td>The era of transfer learning has revolutionized the fields of Computer Vision\\nand Natural Language Processing, bringing powerful pretrained models with\\nexceptional performance across a variety of tasks. Specifically, Natural\\nLanguage Processing tasks have been dominated by transformer-based language\\nmodels. In Natural Language Inference and Natural Language Generation tasks,\\nthe BERT model and its variants, as well as the GPT model and its successors,\\ndemonstrated exemplary performance. However, the majority of these models are\\npretrained and assessed primarily for the English language or on a multilingual\\ncorpus. In this paper, we introduce GreekBART, the first Seq2Seq model based on\\nBART-base architecture and pretrained on a large-scale Greek corpus. We\\nevaluate and compare GreekBART against BART-random, Greek-BERT, and XLM-R on a\\nvariety of discriminative tasks. In addition, we examine its performance on two\\nNLG tasks from GreekSUM, a newly introduced summarization dataset for the Greek\\nlanguage. The model, the code, and the new summarization dataset will be\\npublicly available.</td>\n",
       "      <td>GreekT5-umt5-small-greeksum, GreekT5-umt5-base-greeksum, GreekT5-mt5-small-greeksum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Efficient Sequence Transduction by Jointly Predicting Tokens and\\n  Durations</td>\n",
       "      <td>This paper introduces a novel Token-and-Duration Transducer (TDT)\\narchitecture for sequence-to-sequence tasks. TDT extends conventional\\nRNN-Transducer architectures by jointly predicting both a token and its\\nduration, i.e. the number of input frames covered by the emitted token. This is\\nachieved by using a joint network with two outputs which are independently\\nnormalized to generate distributions over tokens and durations. During\\ninference, TDT models can skip input frames guided by the predicted duration\\noutput, which makes them significantly faster than conventional Transducers\\nwhich process the encoder output frame by frame. TDT models achieve both better\\naccuracy and significantly faster inference than conventional Transducers on\\ndifferent sequence transduction tasks. TDT models for Speech Recognition\\nachieve better accuracy and up to 2.82X faster inference than conventional\\nTransducers. TDT models for Speech Translation achieve an absolute gain of over\\n1 BLEU on the MUST-C test compared with conventional Transducers, and its\\ninference is 2.27X faster. In Speech Intent Classification and Slot Filling\\ntasks, TDT models improve the intent accuracy by up to over 1% (absolute) over\\nconventional Transducers, while running up to 1.28X faster. Our implementation\\nof the TDT model will be open-sourced with the NeMo\\n(https://github.com/NVIDIA/NeMo) toolkit.</td>\n",
       "      <td>parakeet-tdt_ctc-110m, parakeet-tdt_ctc-1.1b, parakeet-tdt_ctc-0.6b-ja, parakeet-tdt-1.1b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>WizardLM: Empowering Large Language Models to Follow Complex\\n  Instructions</td>\n",
       "      <td>Training large language models (LLM) with open-domain instruction following\\ndata brings colossal success. However, manually creating such instruction data\\nis very time-consuming and labor-intensive. Moreover, humans may struggle to\\nproduce high-complexity instructions. In this paper, we show an avenue for\\ncreating large amounts of instruction data with varying levels of complexity\\nusing LLM instead of humans. Starting with an initial set of instructions, we\\nuse our proposed Evol-Instruct to rewrite them step by step into more complex\\ninstructions. Then, we mix all generated instruction data to fine-tune LLaMA.\\nWe call the resulting model WizardLM. Human evaluations on a\\ncomplexity-balanced test bed show that instructions from Evol-Instruct are\\nsuperior to human-created ones. By analyzing the human evaluation results of\\nthe high complexity part, we demonstrate that outputs from our WizardLM model\\nare preferred to outputs from OpenAI ChatGPT. Even though WizardLM still lags\\nbehind ChatGPT in some aspects, our findings suggest that fine-tuning with\\nAI-evolved instructions is a promising direction for enhancing large language\\nmodels. Our codes and generated data are public at\\nhttps://github.com/nlpxucan/WizardLM</td>\n",
       "      <td>gemma-2-2b-it-chinese-kyara-dpo, WizardCoder3b-gguf, orca_mini_v2_7b, orca_mini_v2_13b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Fast Conformer with Linearly Scalable Attention for Efficient Speech\\n  Recognition</td>\n",
       "      <td>Conformer-based models have become the most dominant end-to-end architecture\\nfor speech processing tasks. In this work, we propose a carefully redesigned\\nConformer with a new down-sampling schema. The proposed model, named Fast\\nConformer, is 2.8x faster than original Conformer, while preserving\\nstate-of-the-art accuracy on Automatic Speech Recognition benchmarks. Also we\\nreplace the original Conformer global attention with limited context attention\\npost-training to enable transcription of an hour-long audio. We further improve\\nlong-form speech transcription by adding a global token. Fast Conformer\\ncombined with a Transformer decoder also outperforms the original Conformer in\\naccuracy and in speed for Speech Translation and Spoken Language Understanding.</td>\n",
       "      <td>stt_uz_fastconformer_hybrid_large_pc, stt_ua_fastconformer_hybrid_large_pc, stt_ru_fastconformer_hybrid_large_pc, stt_pl_fastconformer_hybrid_large_pc, stt_nl_fastconformer_hybrid_large_pc, stt_kk_ru_fastconformer_hybrid_large, stt_ka_fastconformer_hybrid_transducer_ctc_large_streaming_80ms_pc, stt_ka_fastconformer_hybrid_large_pc, stt_it_fastconformer_hybrid_large_pc, stt_hr_fastconformer_hybrid_large_pc, stt_fr_fastconformer_hybrid_large_pc, stt_fa_fastconformer_hybrid_large, stt_es_fastconformer_hybrid_large_pc, stt_en_fastconformer_transducer_xxlarge, stt_en_fastconformer_transducer_xlarge, stt_en_fastconformer_transducer_large, stt_en_fastconformer_hybrid_large_pc, stt_en_fastconformer_ctc_xxlarge, stt_en_fastconformer_ctc_xlarge, stt_en_fastconformer_ctc_large, stt_de_fastconformer_hybrid_large_pc, stt_be_fastconformer_hybrid_large_pc, parakeet-tdt_ctc-110m, parakeet-tdt_ctc-1.1b, parakeet-tdt_ctc-0.6b-ja, parakeet-tdt-1.1b, parakeet-rnnt-1.1b, parakeet-rnnt-0.6b, parakeet-ctc-1.1b, parakeet-ctc-0.6b, diar_sortformer_4spk-v1, canary-1b, stt_fr_fastconformer_hybrid_large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>LDM3D: Latent Diffusion Model for 3D</td>\n",
       "      <td>This research paper proposes a Latent Diffusion Model for 3D (LDM3D) that\\ngenerates both image and depth map data from a given text prompt, allowing\\nusers to generate RGBD images from text prompts. The LDM3D model is fine-tuned\\non a dataset of tuples containing an RGB image, depth map and caption, and\\nvalidated through extensive experiments. We also develop an application called\\nDepthFusion, which uses the generated RGB images and depth maps to create\\nimmersive and interactive 360-degree-view experiences using TouchDesigner. This\\ntechnology has the potential to transform a wide range of industries, from\\nentertainment and gaming to architecture and design. Overall, this paper\\npresents a significant contribution to the field of generative AI and computer\\nvision, and showcases the potential of LDM3D and DepthFusion to revolutionize\\ncontent creation and digital experiences. A short video summarizing the\\napproach can be found at https://t.ly/tdi2.</td>\n",
       "      <td>ldm3d-pano, ldm3d-4c, ldm3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Model-Generated Pretraining Signals Improves Zero-Shot Generalization of\\n  Text-to-Text Transformers</td>\n",
       "      <td>This paper explores the effectiveness of model-generated signals in improving\\nzero-shot generalization of text-to-text Transformers such as T5. We study\\nvarious designs to pretrain T5 using an auxiliary model to construct more\\nchallenging token replacements for the main model to denoise. Key aspects under\\nstudy include the decoding target, the location of the RTD head, and the\\nmasking pattern. Based on these studies, we develop a new model, METRO-T0,\\nwhich is pretrained using the redesigned ELECTRA-Style pretraining strategies\\nand then prompt-finetuned on a mixture of NLP tasks. METRO-T0 outperforms all\\nsimilar-sized baselines on prompted NLP benchmarks, such as T0 Eval and MMLU,\\nand rivals the state-of-the-art T0-11B model with only 8% of its parameters.\\nOur analysis on model's neural activation and parameter sensitivity reveals\\nthat the effectiveness of METRO-T0 stems from more balanced contribution of\\nparameters and better utilization of their capacity. The code and model\\ncheckpoints are available at https://github.com/gonglinyuan/metro_t0.</td>\n",
       "      <td>metro_t0pp_largepp, metro_t0pp_basepp, metro_t0pp_base, metro_t0p_largepp, metro_t0p_basepp, metro_t0p_base, metro_t0_largepp, metro_t0_basepp, metro_t0_base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>GQA: Training Generalized Multi-Query Transformer Models from Multi-Head\\n  Checkpoints</td>\n",
       "      <td>Multi-query attention (MQA), which only uses a single key-value head,\\ndrastically speeds up decoder inference. However, MQA can lead to quality\\ndegradation, and moreover it may not be desirable to train a separate model\\njust for faster inference. We (1) propose a recipe for uptraining existing\\nmulti-head language model checkpoints into models with MQA using 5% of original\\npre-training compute, and (2) introduce grouped-query attention (GQA), a\\ngeneralization of multi-query attention which uses an intermediate (more than\\none, less than number of query heads) number of key-value heads. We show that\\nuptrained GQA achieves quality close to multi-head attention with comparable\\nspeed to MQA.</td>\n",
       "      <td>Llama-2-70B-Instruct-v0.1, DeciLM-6b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>QLoRA: Efficient Finetuning of Quantized LLMs</td>\n",
       "      <td>We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.</td>\n",
       "      <td>Falcon-40B-Chat-v0.1, Samantha-1.11-CodeLlama-34b, Samantha-1.11-7b, Samantha-1.11-13b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Resolving Interference When Merging Models</td>\n",
       "      <td>Transfer learning - i.e., further fine-tuning a pre-trained model on a\\ndownstream task - can confer significant advantages, including improved\\ndownstream performance, faster convergence, and better sample efficiency. These\\nadvantages have led to a proliferation of task-specific fine-tuned models,\\nwhich typically can only perform a single task and do not benefit from one\\nanother. Recently, model merging techniques have emerged as a solution to\\ncombine multiple task-specific models into a single multitask model without\\nperforming additional training. However, existing merging methods often ignore\\nthe interference between parameters of different models, resulting in large\\nperformance drops when merging multiple models. In this paper, we demonstrate\\nthat prior merging techniques inadvertently lose valuable information due to\\ntwo major sources of interference: (a) interference due to redundant parameter\\nvalues and (b) disagreement on the sign of a given parameter's values across\\nmodels. To address this, we propose our method, TrIm, Elect Sign &amp; Merge\\n(TIES-Merging), which introduces three novel steps when merging models: (1)\\nresetting parameters that only changed a small amount during fine-tuning, (2)\\nresolving sign conflicts, and (3) merging only the parameters that are in\\nalignment with the final agreed-upon sign. We find that TIES-Merging\\noutperforms several existing methods in diverse settings covering a range of\\nmodalities, domains, number of tasks, model sizes, architectures, and\\nfine-tuning settings. We further analyze the impact of different types of\\ninterference on model parameters, highlight the importance of resolving sign\\ninterference. Our code is available at\\nhttps://github.com/prateeky2806/ties-merging</td>\n",
       "      <td>Llama3-8B-SuperNova-Spectrum-dare_ties, SmolLM2-360M-Merged, SmolLM2-135M-Merged, WestKunai-Hermes-7b, West-Hermes-7B, Top-Western-Maid-7B, Kuno-Lake-7B, Fett-Eris-Mix-7B, Qwen2.5-Ultimate-14B-Instruct, NeuralZephyr-Beagle-7B, EEVE-Instruct-Math-10.8B, Moza-7B-v1.0, IceSakeV6RP-7b, IceCocoaRP-7b, WestMaid_HermesMonarchv0.1, WestLake_Noromaid_OpenHermes_neural-chatv0.1, Open_Neural_Monarch_Maidv0.1, Open_Maid_Samantha_Hermes_Orca_dare_tiesv0.1, Gemma-2-gemmama-9b, TEST-L3.2-ReWish-3B-ties-w-base, Qwen2.5-3B-RP-Thinker, Llama-3.2-3B-Mix-Skill, phi4-qwq-sky-t1, VICIOUS_MESH-12B-NEMO, VICIOUS_MESH-12B-GAMMA, Macaroni-7b-Tied, CosmicBun-8B, Qwenslerp4-14B, Qwen2.5-Dyanka-7B-Preview, RomboHermes3-R1-Llama3.2-3b, Robo-Gutenberg_V1.0, Q2.5-14B-Instruct-1M-Harmony, Pantheon_ChatWaifu_V0.2, Pans_Gutenbergum_V0.2, Pans_Gutenbergum_V0.1, Mistral-Redemption-Arc, Minerva-7b, Minerva-1.5b_V0.2, L3.1-8B-Dusky-Ink, Herodotos-14B_V0.1, Hermes3-L3.1-DirtyHarry-8B, Hermes-Llama-3.2-CoT-Summary, Distilled-Whiskey-8b, Distilled-DarkPlanet-Allades-8B_TIES, Distilled-DarkPlanet-Allades-8B, Dark-Chivalry_V1.0, Chronos-Prism_V1.0, Chatty-Harry_V3.0, Chatty-Harry_V2.0, BigTalker-Lite-8B, qwen-carpmuscle-r-v0.3, ZEUS-8B-V9, ZEUS-8B-V8, ZEUS-8B-V7, ZEUS-8B-V6, ZEUS-8B-V29, ZEUS-8B-V28, ZEUS-8B-V27, ZEUS-8B-V26, ZEUS-8B-V25, ZEUS-8B-V24, ZEUS-8B-V23, ZEUS-8B-V22, ZEUS-8B-V20, ZEUS-8B-V2, ZEUS-8B-V19, ZEUS-8B-V18, ZEUS-8B-V17-abliterated-V4, ZEUS-8B-V17, ZEUS-8B-V16, ZEUS-8B-V14, ZEUS-8B-V13-abliterated, ZEUS-8B-V13, ZEUS-8B-V12, ZEUS-8B-V11, ZEUS-8B-V10, Meta-Llama-3.1-8B-Instruct-TIES, Llama-3.1-8B-Instruct-Zeus, KRONOS-8B-V9, KRONOS-8B-V6, KRONOS-8B-V5, KRONOS-8B-V3, KRONOS-8B-V2, Etheria-55b-v0.1, ultiima-72B, model-3, FineTome-v1.5-Llama3.2-3B-1007, FineTome-v1.5-Llama3.2-1B-1007, Kunocchini-1.2-7b-longtext-broken, StarDust-12b-v1, threebird-7B, Smart-Lemon-Cookie-7B, TriFusionNexus-7b, SeQwence-14Bv3, Qwestion-14B, huihui-ai-abliterated-Qwen2.5-32B-Inst-BaseMerge-TIES, Rombos-Qwen2.5-7B-Inst-BaseMerge-TIES, This_is_fine_7B, Prima-LelantaclesV5-7b, Cookie_7B, L3-RP_io</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Orca: Progressive Learning from Complex Explanation Traces of GPT-4</td>\n",
       "      <td>Recent research has focused on enhancing the capability of smaller models\\nthrough imitation learning, drawing on the outputs generated by large\\nfoundation models (LFMs). A number of issues impact the quality of these\\nmodels, ranging from limited imitation signals from shallow LFM outputs; small\\nscale homogeneous training data; and most notably a lack of rigorous evaluation\\nresulting in overestimating the small model's capability as they tend to learn\\nto imitate the style, but not the reasoning process of LFMs. To address these\\nchallenges, we develop Orca (We are working with our legal team to publicly\\nrelease a diff of the model weights in accordance with LLaMA's release policy\\nto be published at https://aka.ms/orca-lm), a 13-billion parameter model that\\nlearns to imitate the reasoning process of LFMs. Orca learns from rich signals\\nfrom GPT-4 including explanation traces; step-by-step thought processes; and\\nother complex instructions, guided by teacher assistance from ChatGPT. To\\npromote this progressive learning, we tap into large-scale and diverse\\nimitation data with judicious sampling and selection. Orca surpasses\\nconventional state-of-the-art instruction-tuned models such as Vicuna-13B by\\nmore than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard\\n(BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH\\nbenchmark and shows competitive performance (4 pts gap with optimized system\\nmessage) in professional and academic examinations like the SAT, LSAT, GRE, and\\nGMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our\\nresearch indicates that learning from step-by-step explanations, whether these\\nare generated by humans or more advanced AI models, is a promising direction to\\nimprove model capabilities and skills.</td>\n",
       "      <td>orca_mini_v3_7b, orca_mini_v3_70b, orca_mini_v3_13b, orca_mini_v2_7b, orca_mini_v2_13b, orca_mini_7b, orca_mini_13b, model_51, model_420, model_101, model_009, model_007_preview, model_007_13b_v2, model_007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Weakly-Supervised Conditional Embedding for Referred Visual Search</td>\n",
       "      <td>This paper presents a new approach to image similarity search in the context\\nof fashion, a domain with inherent ambiguity due to the multiple ways in which\\nimages can be considered similar. We introduce the concept of Referred Visual\\nSearch (RVS), where users provide additional information to define the desired\\nsimilarity. We present a new dataset, LAION-RVS-Fashion, consisting of 272K\\nfashion products with 842K images extracted from LAION, designed explicitly for\\nthis task. We then propose an innovative method for learning conditional\\nembeddings using weakly-supervised training, achieving a 6% increase in Recall\\nat one (R@1) against a gallery with 2M distractors, compared to classical\\napproaches based on explicit attention and filtering. The proposed method\\ndemonstrates robustness, maintaining similar R@1 when dealing with 2.5 times as\\nmany distractors as the baseline methods. We believe this is a step forward in\\nthe emerging field of Referred Visual Search both in terms of accessible data\\nand approach. Code, data and models are available at\\nhttps://www.github.com/Simon-Lepage/CondViT-LRVSF .</td>\n",
       "      <td>CondViT-B16-txt, CondViT-B16-cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Judging LLM-as-a-judge with MT-Bench and Chatbot Arena</td>\n",
       "      <td>Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, such as position and verbosity biases\\nand limited reasoning ability, and propose solutions to migrate some of them.\\nWe then verify the agreement between LLM judges and human preferences by\\nintroducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot\\nArena, a crowdsourced battle platform. Our results reveal that strong LLM\\njudges like GPT-4 can match both controlled and crowdsourced human preferences\\nwell, achieving over 80\\% agreement, the same level of agreement between\\nhumans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate\\nhuman preferences, which are otherwise very expensive to obtain. Additionally,\\nwe show our benchmark and traditional benchmarks complement each other by\\nevaluating several variants of LLaMA/Vicuna. We will publicly release 80\\nMT-bench questions, 3K expert votes, and 30K conversations with human\\npreferences from Chatbot Arena.</td>\n",
       "      <td>WestMaid_HermesMonarchv0.1, WestLake_Noromaid_OpenHermes_neural-chatv0.1, radiantloom-mixtral-8x7b-fusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Boosting Norwegian Automatic Speech Recognition</td>\n",
       "      <td>In this paper, we present several baselines for automatic speech recognition\\n(ASR) models for the two official written languages in Norway: Bokm{\\aa}l and\\nNynorsk. We compare the performance of models of varying sizes and pre-training\\napproaches on multiple Norwegian speech datasets. Additionally, we measure the\\nperformance of these models against previous state-of-the-art ASR models, as\\nwell as on out-of-domain datasets. We improve the state of the art on the\\nNorwegian Parliamentary Speech Corpus (NPSC) from a word error rate (WER) of\\n17.10\\% to 7.60\\%, with models achieving 5.81\\% for Bokm{\\aa}l and 11.54\\% for\\nNynorsk. We also discuss the challenges and potential solutions for further\\nimproving ASR models for Norwegian.</td>\n",
       "      <td>nb-wav2vec2-300m-nynorsk, nb-wav2vec2-300m-bokmaal, nb-wav2vec2-1b-nynorsk, nb-wav2vec2-1b-bokmaal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Llama 2: Open Foundation and Fine-Tuned Chat Models</td>\n",
       "      <td>In this work, we develop and release Llama 2, a collection of pretrained and\\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\\ndialogue use cases. Our models outperform open-source chat models on most\\nbenchmarks we tested, and based on our human evaluations for helpfulness and\\nsafety, may be a suitable substitute for closed-source models. We provide a\\ndetailed description of our approach to fine-tuning and safety improvements of\\nLlama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.</td>\n",
       "      <td>canarim-7b, yayi2-30b-llama, latxa-70b-v1.2, latxa-70b-v1.1, latxa-13b-v1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Towards General Text Embeddings with Multi-stage Contrastive Learning</td>\n",
       "      <td>We present GTE, a general-purpose text embedding model trained with\\nmulti-stage contrastive learning. In line with recent advancements in unifying\\nvarious NLP tasks into a single format, we train a unified text embedding model\\nby employing contrastive learning over a diverse mixture of datasets from\\nmultiple sources. By significantly increasing the number of training data\\nduring both unsupervised pre-training and supervised fine-tuning stages, we\\nachieve substantial performance gains over existing embedding models. Notably,\\neven with a relatively modest parameter count of 110M, GTE_base\\noutperforms the black-box embedding API provided by OpenAI and even surpasses\\n10x larger text embedding models on the massive text embedding benchmark.\\nFurthermore, without additional fine-tuning on each programming language\\nindividually, our model outperforms previous best code retrievers of similar\\nsize by treating code as text. In summary, our model achieves impressive\\nresults by effectively harnessing multi-stage contrastive learning, offering a\\npowerful and efficient text embedding model with broad applicability across\\nvarious NLP and code-related tasks.</td>\n",
       "      <td>gte-large-zh, gte-base-zh, french-document-embedding, gte-Qwen1.5-7B-instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>OctoPack: Instruction Tuning Code Large Language Models</td>\n",
       "      <td>Finetuning large language models (LLMs) on instructions leads to vast\\nperformance improvements on natural language tasks. We apply instruction tuning\\nusing code, leveraging the natural structure of Git commits, which pair code\\nchanges with human instructions. We compile CommitPack: 4 terabytes of Git\\ncommits across 350 programming languages. We benchmark CommitPack against other\\nnatural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B\\nparameter StarCoder model, and achieve state-of-the-art performance among\\nmodels not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2%\\npass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark\\nto a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis)\\nacross 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models,\\nOctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among\\nall permissive models, demonstrating CommitPack's benefits in generalizing to a\\nwider set of languages and natural coding tasks. Code, models and data are\\nfreely available at https://github.com/bigcode-project/octopack.</td>\n",
       "      <td>santacoderpack, octogeex, octocoder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122\\n  Language Variants</td>\n",
       "      <td>We present Belebele, a multiple-choice machine reading comprehension (MRC)\\ndataset spanning 122 language variants. Significantly expanding the language\\ncoverage of natural language understanding (NLU) benchmarks, this dataset\\nenables the evaluation of text models in high-, medium-, and low-resource\\nlanguages. Each question is based on a short passage from the Flores-200\\ndataset and has four multiple-choice answers. The questions were carefully\\ncurated to discriminate between models with different levels of general\\nlanguage comprehension. The English dataset on its own proves difficult enough\\nto challenge state-of-the-art language models. Being fully parallel, this\\ndataset enables direct comparison of model performance across all languages. We\\nuse this dataset to evaluate the capabilities of multilingual masked language\\nmodels (MLMs) and large language models (LLMs). We present extensive results\\nand find that despite significant cross-lingual transfer in English-centric\\nLLMs, much smaller MLMs pretrained on balanced multilingual data still\\nunderstand far more languages. We also observe that larger vocabulary size and\\nconscious vocabulary construction correlate with better performance on\\nlow-resource languages. Overall, Belebele opens up new avenues for evaluating\\nand analyzing the multilingual capabilities of NLP systems.</td>\n",
       "      <td>latxa-70b-v1.2, latxa-70b-v1.1, latxa-13b-v1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>YaRN: Efficient Context Window Extension of Large Language Models</td>\n",
       "      <td>Rotary Position Embeddings (RoPE) have been shown to effectively encode\\npositional information in transformer-based language models. However, these\\nmodels fail to generalize past the sequence length they were trained on. We\\npresent YaRN (Yet another RoPE extensioN method), a compute-efficient method to\\nextend the context window of such models, requiring 10x less tokens and 2.5x\\nless training steps than previous methods. Using YaRN, we show that LLaMA\\nmodels can effectively utilize and extrapolate to context lengths much longer\\nthan their original pre-training would allow, while also surpassing previous\\nthe state-of-the-art at context window extension. In addition, we demonstrate\\nthat YaRN exhibits the capability to extrapolate beyond the limited context of\\na fine-tuning dataset. We publish the checkpoints of Llama 2 7B/13B fine-tuned\\nusing YaRN with 64k and 128k context windows at\\nhttps://github.com/jquesnelle/yarn</td>\n",
       "      <td>orca_mini_v7_72b, openthaigpt1.5-7b-instruct, openthaigpt1.5-72b-instruct, openthaigpt1.5-14b-instruct, Josiefied-Qwen2.5-7B-Instruct-abliterated-GGUF, Josiefied-Qwen2.5-14B-Instruct-abliterated-v4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>C-Pack: Packaged Resources To Advance General Chinese Embedding</td>\n",
       "      <td>We introduce C-Pack, a package of resources that significantly advance the\\nfield of general Chinese embeddings. C-Pack includes three critical resources.\\n1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6\\ntasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated\\nfrom labeled and unlabeled Chinese corpora for training embedding models. 3)\\nC-TEM is a family of embedding models covering multiple sizes. Our models\\noutperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the\\ntime of the release. We also integrate and optimize the entire suite of\\ntraining methods for C-TEM. Along with our resources on general Chinese\\nembedding, we release our data and models for English text embeddings. The\\nEnglish models achieve state-of-the-art performance on MTEB benchmark;\\nmeanwhile, our released English data is 2 times larger than the Chinese data.\\nAll these resources are made publicly available at\\nhttps://github.com/FlagOpen/FlagEmbedding.</td>\n",
       "      <td>BAAI-bge-reranker-large, bge-multilingual-gemma2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>OpenChat: Advancing Open-source Language Models with Mixed-Quality Data</td>\n",
       "      <td>Nowadays, open-source large language models like LLaMA have emerged. Recent\\ndevelopments have incorporated supervised fine-tuning (SFT) and reinforcement\\nlearning fine-tuning (RLFT) to align these models with human goals. However,\\nSFT methods treat all training data with mixed quality equally, while RLFT\\nmethods require high-quality pairwise or ranking-based preference data. In this\\nstudy, we present a novel framework, named OpenChat, to advance open-source\\nlanguage models with mixed-quality data. Specifically, we consider the general\\nSFT training data, consisting of a small amount of expert data mixed with a\\nlarge proportion of sub-optimal data, without any preference labels. We propose\\nthe C(onditioned)-RLFT, which regards different data sources as coarse-grained\\nreward labels and learns a class-conditioned policy to leverage complementary\\ndata quality information. Interestingly, the optimal policy in C-RLFT can be\\neasily solved through single-stage, RL-free supervised learning, which is\\nlightweight and avoids costly human preference labeling. Through extensive\\nexperiments on three standard benchmarks, our openchat-13b fine-tuned with\\nC-RLFT achieves the highest average performance among all 13b open-source\\nlanguage models. Moreover, we use AGIEval to validate the model generalization\\nperformance, in which only openchat-13b surpasses the base model. Finally, we\\nconduct a series of analyses to shed light on the effectiveness and robustness\\nof OpenChat. Our code, data, and models are publicly available at\\nhttps://github.com/imoneoi/openchat.</td>\n",
       "      <td>openchat-3.5-0106-32k, openchat-3.5-0106-128k, openchat-3.5-0106-11b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>AnglE-optimized Text Embeddings</td>\n",
       "      <td>High-quality text embedding is pivotal in improving semantic textual\\nsimilarity (STS) tasks, which are crucial components in Large Language Model\\n(LLM) applications. However, a common challenge existing text embedding models\\nface is the problem of vanishing gradients, primarily due to their reliance on\\nthe cosine function in the optimization objective, which has saturation zones.\\nTo address this issue, this paper proposes a novel angle-optimized text\\nembedding model called AnglE. The core idea of AnglE is to introduce angle\\noptimization in a complex space. This novel approach effectively mitigates the\\nadverse effects of the saturation zone in the cosine function, which can impede\\ngradient and hinder optimization processes. To set up a comprehensive STS\\nevaluation, we experimented on existing short-text STS datasets and a newly\\ncollected long-text STS dataset from GitHub Issues. Furthermore, we examine\\ndomain-specific STS scenarios with limited labeled data and explore how AnglE\\nworks with LLM-annotated data. Extensive experiments were conducted on various\\ntasks including short-text STS, long-text STS, and domain-specific STS tasks.\\nThe results show that AnglE outperforms the state-of-the-art (SOTA) STS models\\nthat ignore the cosine saturation zone. These findings demonstrate the ability\\nof AnglE to generate high-quality text embeddings and the usefulness of angle\\noptimization in STS.</td>\n",
       "      <td>Angle_BERT, mxbai-embed-xsmall-v1, multiling-e5-large-instruct-claim-matching, STS-multilingual-mpnet-base-v2, bge-large-en-v1.5-AngleLoss-25-Epochs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Mistral 7B</td>\n",
       "      <td>We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention (GQA) for faster\\ninference, coupled with sliding window attention (SWA) to effectively handle\\nsequences of arbitrary length with a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released under the Apache 2.0 license.</td>\n",
       "      <td>Mixtral-Instruct-v0.2-2x7B-AWQ, Mistral-7B-Instruct-v0.2-attention-sparsity-30, Mistral-7B-Instruct-v0.2-attention-sparsity-10-v0.1, Mixtral-8x7B-Instruct-v0.1, Mistral-7B-Instruct-v0.2, Kant-Test-0.1-Mistral-7B, Linq-Embed-Mistral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Language Models are Universal Embedders</td>\n",
       "      <td>In the large language model (LLM) revolution, embedding is a key component of\\nvarious systems. For example, it is used to retrieve knowledge or memories for\\nLLMs, to build content moderation filters, etc. As such cases span from English\\nto other natural or programming languages, from retrieval to classification and\\nbeyond, it is desirable to build a unified embedding model rather than\\ndedicated ones for each scenario. In this work, we make an initial step towards\\nthis goal, demonstrating that multiple languages (both natural and programming)\\npre-trained transformer decoders can embed universally when finetuned on\\nlimited English data. We provide a comprehensive practice with thorough\\nevaluations. On English MTEB, our models achieve competitive performance on\\ndifferent embedding tasks by minimal training data. On other benchmarks, such\\nas multilingual classification and code search, our models (without any\\nsupervision) perform comparably to, or even surpass heavily supervised\\nbaselines and/or APIs. These results provide evidence of a promising path\\ntowards building powerful unified embedders that can be applied across tasks\\nand languages.</td>\n",
       "      <td>udever-bloom-7b1, udever-bloom-560m, udever-bloom-1b1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V</td>\n",
       "      <td>We present Set-of-Mark (SoM), a new visual prompting method, to unleash the\\nvisual grounding abilities of large multimodal models (LMMs), such as GPT-4V.\\nAs illustrated in Fig. 1 (right), we employ off-the-shelf interactive\\nsegmentation models, such as SAM, to partition an image into regions at\\ndifferent levels of granularity, and overlay these regions with a set of marks\\ne.g., alphanumerics, masks, boxes. Using the marked image as input, GPT-4V can\\nanswer the questions that require visual grounding. We perform a comprehensive\\nempirical study to validate the effectiveness of SoM on a wide range of\\nfine-grained vision and multimodal tasks. For example, our experiments show\\nthat GPT-4V with SoM outperforms the state-of-the-art fully-finetuned referring\\nsegmentation model on RefCOCOg in a zero-shot setting.</td>\n",
       "      <td>LLaVA-Next-Inst-It-Vicuna-7B, LLaVA-Next-Inst-It-Qwen2-7B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Back Transcription as a Method for Evaluating Robustness of Natural\\n  Language Understanding Models to Speech Recognition Errors</td>\n",
       "      <td>In a spoken dialogue system, an NLU model is preceded by a speech recognition\\nsystem that can deteriorate the performance of natural language understanding.\\nThis paper proposes a method for investigating the impact of speech recognition\\nerrors on the performance of natural language understanding models. The\\nproposed method combines the back transcription procedure with a fine-grained\\ntechnique for categorizing the errors that affect the performance of NLU\\nmodels. The method relies on the usage of synthesized speech for NLU\\nevaluation. We show that the use of synthesized speech in place of audio\\nrecording does not change the outcomes of the presented technique in a\\nsignificant way.</td>\n",
       "      <td>xlm-r-base-amazon-massive-slot, xlm-r-base-amazon-massive-intent, xlm-r-base-amazon-massive-domain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Zephyr: Direct Distillation of LM Alignment</td>\n",
       "      <td>We aim to produce a smaller language model that is aligned to user intent.\\nPrevious research has shown that applying distilled supervised fine-tuning\\n(dSFT) on larger models significantly improves task accuracy; however, these\\nmodels are unaligned, i.e. they do not respond well to natural prompts. To\\ndistill this property, we experiment with the use of preference data from AI\\nFeedback (AIF). Starting from a dataset of outputs ranked by a teacher model,\\nwe apply distilled direct preference optimization (dDPO) to learn a chat model\\nwith significantly improved intent alignment. The approach requires only a few\\nhours of training without any additional sampling during fine-tuning. The final\\nresult, Zephyr-7B, sets the state-of-the-art on chat benchmarks for 7B\\nparameter models, and requires no human annotation. In particular, results on\\nMT-Bench show that Zephyr-7B surpasses Llama2-Chat-70B, the best open-access\\nRLHF-based model. Code, models, data, and tutorials for the system are\\navailable at https://github.com/huggingface/alignment-handbook.</td>\n",
       "      <td>juanako-7b-UNA, zephyr-7b-gemma-v0.1, zephyr-7b-beta-128k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Language Models are Super Mario: Absorbing Abilities from Homologous\\n  Models as a Free Lunch</td>\n",
       "      <td>In this paper, we uncover that Language Models (LMs), either encoder- or\\ndecoder-based, can obtain new capabilities by assimilating the parameters of\\nhomologous models without retraining or GPUs. Typically, new abilities of LMs\\ncan be imparted by Supervised Fine-Tuning (SFT), reflected in the disparity\\nbetween fine-tuned and pre-trained parameters (i.e., delta parameters). We\\ninitially observe that by introducing a novel operation called DARE (Drop And\\nREscale), most delta parameters can be directly set to zeros without affecting\\nthe capabilities of SFT LMs and larger models can tolerate a higher proportion\\nof discarded parameters. Based on this observation, we further sparsify delta\\nparameters of multiple SFT homologous models with DARE and subsequently merge\\nthem into a single model by parameter averaging. We conduct experiments on\\neight datasets from the GLUE benchmark with BERT and RoBERTa. We also merge\\nWizardLM, WizardMath, and Code Alpaca based on Llama 2. Experimental results\\nshow that: (1) The delta parameter value ranges for SFT models are typically\\nsmall, often within 0.005, and DARE can eliminate 99% of them effortlessly.\\nHowever, once the models are continuously pre-trained, the value ranges can\\ngrow to around 0.03, making DARE impractical. We have also tried to remove\\nfine-tuned instead of delta parameters and find that a 10% reduction can lead\\nto drastically decreased performance (even to 0). This highlights that SFT\\nmerely stimulates the abilities via delta parameters rather than injecting new\\nabilities into LMs; (2) DARE can merge multiple task-specific LMs into one LM\\nwith diverse abilities. For instance, the merger of WizardLM and WizardMath\\nimproves the GSM8K zero-shot accuracy of WizardLM from 2.2 to 66.3, retaining\\nits instruction-following ability while surpassing WizardMath's original 64.2\\nperformance. Codes are available at https://github.com/yule-BUAA/MergeLM.</td>\n",
       "      <td>Llama3-8B-SuperNova-Spectrum-dare_ties, WestKunai-Hermes-7b, West-Hermes-7B, Top-Western-Maid-7B, Kuno-Lake-7B, Fett-Eris-Mix-7B, NeuralZephyr-Beagle-7B, EEVE-Instruct-Math-10.8B, Moza-7B-v1.0, WestMaid_HermesMonarchv0.1, WestLake_Noromaid_OpenHermes_neural-chatv0.1, Open_Neural_Monarch_Maidv0.1, Open_Maid_Samantha_Hermes_Orca_dare_tiesv0.1, Gemma-2-gemmama-9b, TEST-L3.2-ReWish-3B, CosmicBun-8B, Qwenslerp4-14B, ZEUS-8B-V9, ZEUS-8B-V8, ZEUS-8B-V7, ZEUS-8B-V6, ZEUS-8B-V29, ZEUS-8B-V28, ZEUS-8B-V27, ZEUS-8B-V26, ZEUS-8B-V25, ZEUS-8B-V24, ZEUS-8B-V23, ZEUS-8B-V22, ZEUS-8B-V20, ZEUS-8B-V2, ZEUS-8B-V19, ZEUS-8B-V18, ZEUS-8B-V17-abliterated-V4, ZEUS-8B-V17, ZEUS-8B-V16, ZEUS-8B-V14, ZEUS-8B-V13-abliterated, ZEUS-8B-V13, ZEUS-8B-V12, ZEUS-8B-V11, ZEUS-8B-V10, Llama-3.1-8B-Instruct-Zeus, Etheria-55b-v0.1, Kunocchini-1.2-7b-longtext-broken, StarDust-12b-v1, SeQwence-14Bv3, Qwestion-14B, This_is_fine_7B, Prima-LelantaclesV5-7b, Cookie_7B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>LDM3D-VR: Latent Diffusion Model for 3D VR</td>\n",
       "      <td>Latent diffusion models have proven to be state-of-the-art in the creation\\nand manipulation of visual outputs. However, as far as we know, the generation\\nof depth maps jointly with RGB is still limited. We introduce LDM3D-VR, a suite\\nof diffusion models targeting virtual reality development that includes\\nLDM3D-pano and LDM3D-SR. These models enable the generation of panoramic RGBD\\nbased on textual prompts and the upscaling of low-resolution inputs to\\nhigh-resolution RGBD, respectively. Our models are fine-tuned from existing\\npretrained models on datasets containing panoramic/high-resolution RGB images,\\ndepth maps and captions. Both models are evaluated in comparison to existing\\nrelated methods.</td>\n",
       "      <td>ldm3d-sr, ldm3d-pano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Tamil-Llama: A New Tamil Language Model Based on Llama 2</td>\n",
       "      <td>Language modeling has witnessed remarkable advancements in recent years, with\\nLarge Language Models (LLMs) like ChatGPT setting unparalleled benchmarks in\\nhuman-like text generation. However, a prevailing limitation is the\\nunderrepresentation of languages like Tamil in these cutting-edge models,\\nleading to suboptimal performance in diverse linguistic contexts. This paper\\naddresses this lacuna, enhancing the open-source LLaMA model with an addition\\nof 16,000 Tamil tokens, aiming to achieve superior text generation and\\ncomprehension in the Tamil language. We strategically employ the LoRA\\nmethodology for efficient model training on a comprehensive Tamil corpus,\\nensuring computational feasibility and model robustness. Moreover, we introduce\\na Tamil-translated version of the Alpaca dataset and a subset of the OpenOrca\\ndataset tailored for instruction fine-tuning. Our results showcase significant\\nperformance improvements in Tamil text generation, with potential implications\\nfor the broader landscape of LLMs in Indian languages. We further underscore\\nour commitment to open research by making our models, datasets, and code\\npublicly accessible, fostering further innovations in language modeling.</td>\n",
       "      <td>tamil-llama-7b-instruct-v0.1, tamil-llama-13b-base-v0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Towards the Law of Capacity Gap in Distilling Language Models</td>\n",
       "      <td>Language model (LM) distillation is a trending area that aims to distil the\\nknowledge resided in a large teacher LM to a small student one. While various\\nmethods have been proposed to push the distillation to its limits, it is still\\na pain distilling LMs when a large capacity gap is exhibited between the\\nteacher and the student LMs. The pain is mainly resulted by the curse of\\ncapacity gap, which describes that a larger teacher LM cannot always lead to a\\nbetter student LM than one distilled from a smaller teacher LM due to the\\naffect of capacity gap increment. That is, there is likely an optimal point\\nyielding the best student LM along the scaling course of the teacher LM. Even\\nworse, the curse of capacity gap can be only partly yet not fully lifted as\\nindicated in previous studies.\\n  However, the tale is not ever one-sided. Although a larger teacher LM has\\nbetter performance than a smaller teacher LM, it is much more\\nresource-demanding especially in the context of recent large LMs (LLMs).\\nConsequently, instead of sticking to lifting the curse, leaving the curse as is\\nshould be arguably fine. Even better, in this paper, we reveal that the optimal\\ncapacity gap is almost consistent across different student scales and\\narchitectures, fortunately turning the curse into the law of capacity gap. The\\nlaw later guides us to distil a 3B student LM (termed MiniMA) from a 7B teacher\\nLM (adapted LLaMA2-7B). MiniMA is demonstrated to yield a new\\ncompute-performance pareto frontier among existing 3B LMs on commonly used\\nbenchmarks, and its instruction-tuned version (termed MiniChat) outperforms a\\nwide range of 3B competitors in GPT4 evaluation and could even compete with\\nseveral 7B chat models.</td>\n",
       "      <td>MiniMA-2-3B, MiniChat-2-3B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>calamanCy: A Tagalog Natural Language Processing Toolkit</td>\n",
       "      <td>We introduce calamanCy, an open-source toolkit for constructing natural\\nlanguage processing (NLP) pipelines for Tagalog. It is built on top of spaCy,\\nenabling easy experimentation and integration with other frameworks. calamanCy\\naddresses the development gap by providing a consistent API for building NLP\\napplications and offering general-purpose multitask models with out-of-the-box\\nsupport for dependency parsing, parts-of-speech (POS) tagging, and named entity\\nrecognition (NER). calamanCy aims to accelerate the progress of Tagalog NLP by\\nconsolidating disjointed resources in a unified framework. The calamanCy\\ntoolkit is available on GitHub: https://github.com/ljvmiranda921/calamanCy.</td>\n",
       "      <td>tl_calamancy_trf-0.1.0, tl_calamancy_lg-0.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>GreekT5: A Series of Greek Sequence-to-Sequence Models for News\\n  Summarization</td>\n",
       "      <td>Text summarization (TS) is a natural language processing (NLP) subtask\\npertaining to the automatic formulation of a concise and coherent summary that\\ncovers the major concepts and topics from one or multiple documents. Recent\\nadvancements in deep learning have led to the development of abstractive\\nsummarization transformer-based models, which outperform classical approaches.\\nIn any case, research in this field focuses on high resource languages such as\\nEnglish, while the corresponding work for low resource languages is still\\nunderdeveloped. Taking the above into account, this paper proposes a series of\\nnovel TS models for Greek news articles. The proposed models were thoroughly\\nevaluated on the same dataset against GreekBART, which is the state-of-the-art\\nmodel in Greek abstractive news summarization. Our evaluation results reveal\\nthat most of the proposed models significantly outperform GreekBART on various\\nevaluation metrics. We make our evaluation code public, aiming to increase the\\nreproducibility of this work and facilitate future research in the field.</td>\n",
       "      <td>GreekT5-umt5-small-greeksum, GreekT5-umt5-base-greeksum, GreekT5-mt5-small-greeksum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Digital Socrates: Evaluating LLMs through explanation critiques</td>\n",
       "      <td>While LLMs can provide reasoned explanations along with their answers, the\\nnature and quality of those explanations are still poorly understood. In\\nresponse, our goal is to define a detailed way of characterizing the\\nexplanation capabilities of modern models and to create a nuanced,\\ninterpretable explanation evaluation tool that can generate such\\ncharacterizations automatically, without relying on expensive API calls or\\nhuman annotations. Our approach is to (a) define the new task of explanation\\ncritiquing - identifying and categorizing any main flaw in an explanation and\\nproviding suggestions to address the flaw, (b) create a sizeable,\\nhuman-verified dataset for this task, and (c) train an open-source, automatic\\ncritiquing model (called Digital Socrates) using this data. Through\\nquantitative and qualitative analysis, we demonstrate how Digital Socrates is\\nuseful for revealing insights about student models by examining their reasoning\\nchains, and how it can provide high-quality, nuanced, automatic evaluation of\\nthose model explanations for the first time. Digital Socrates thus fills an\\nimportant gap in evaluation tools for understanding and improving the\\nexplanation behavior of models.</td>\n",
       "      <td>digital-socrates-7b, digital-socrates-13b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models</td>\n",
       "      <td>We introduce EQ-Bench, a novel benchmark designed to evaluate aspects of\\nemotional intelligence in Large Language Models (LLMs). We assess the ability\\nof LLMs to understand complex emotions and social interactions by asking them\\nto predict the intensity of emotional states of characters in a dialogue. The\\nbenchmark is able to discriminate effectively between a wide range of models.\\nWe find that EQ-Bench correlates strongly with comprehensive multi-domain\\nbenchmarks like MMLU (Hendrycks et al., 2020) (r=0.97), indicating that we may\\nbe capturing similar aspects of broad intelligence. Our benchmark produces\\nhighly repeatable results using a set of 60 English-language questions. We also\\nprovide open-source code for an automated benchmarking pipeline at\\nhttps://github.com/EQ-bench/EQ-Bench and a leaderboard at https://eqbench.com</td>\n",
       "      <td>WestMaid_HermesMonarchv0.1, WestLake_Noromaid_OpenHermes_neural-chatv0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Improving Text Embeddings with Large Language Models</td>\n",
       "      <td>In this paper, we introduce a novel and simple method for obtaining\\nhigh-quality text embeddings using only synthetic data and less than 1k\\ntraining steps. Unlike existing methods that often depend on multi-stage\\nintermediate pre-training with billions of weakly-supervised text pairs,\\nfollowed by fine-tuning with a few labeled datasets, our method does not\\nrequire building complex training pipelines or relying on manually collected\\ndatasets that are often constrained by task diversity and language coverage. We\\nleverage proprietary LLMs to generate diverse synthetic data for hundreds of\\nthousands of text embedding tasks across nearly 100 languages. We then\\nfine-tune open-source decoder-only LLMs on the synthetic data using standard\\ncontrastive loss. Experiments demonstrate that our method achieves strong\\nperformance on highly competitive text embedding benchmarks without using any\\nlabeled data. Furthermore, when fine-tuned with a mixture of synthetic and\\nlabeled data, our model sets new state-of-the-art results on the BEIR and MTEB\\nbenchmarks.</td>\n",
       "      <td>NV-Retriever-v1, Linq-Embed-Mistral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>LLaMA Pro: Progressive LLaMA with Block Expansion</td>\n",
       "      <td>Humans generally acquire new skills without compromising the old; however,\\nthe opposite holds for Large Language Models (LLMs), e.g., from LLaMA to\\nCodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with\\nan expansion of Transformer blocks. We tune the expanded blocks using only new\\ncorpus, efficiently and effectively improving the model's knowledge without\\ncatastrophic forgetting. In this paper, we experiment on the corpus of code and\\nmath, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from\\nLLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro\\nand its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced\\nperformance among various benchmarks, demonstrating superiority over existing\\nopen models in the LLaMA family and the immense potential of reasoning and\\naddressing diverse tasks as an intelligent agent. Our findings provide valuable\\ninsights into integrating natural and programming languages, laying a solid\\nfoundation for developing advanced language agents that operate effectively in\\nvarious environments.</td>\n",
       "      <td>OpenChat-3.5-0106_8.99B_40Layers-Interleaved, OpenChat-3.5-0106_8.11B_36Layers-Interleaved, OpenChat-3.5-0106_8.11B_36Layers-Appended, OpenChat-3.5-0106_10.7B_48Layers-Interleaved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Introducing Bode: A Fine-Tuned Large Language Model for Portuguese\\n  Prompt-Based Task</td>\n",
       "      <td>Large Language Models (LLMs) are increasingly bringing advances to Natural\\nLanguage Processing. However, low-resource languages, those lacking extensive\\nprominence in datasets for various NLP tasks, or where existing datasets are\\nnot as substantial, such as Portuguese, already obtain several benefits from\\nLLMs, but not to the same extent. LLMs trained on multilingual datasets\\nnormally struggle to respond to prompts in Portuguese satisfactorily,\\npresenting, for example, code switching in their responses. This work proposes\\na fine-tuned LLaMA 2-based model for Portuguese prompts named Bode in two\\nversions: 7B and 13B. We evaluate the performance of this model in\\nclassification tasks using the zero-shot approach with in-context learning, and\\ncompare it with other LLMs. Our main contribution is to bring an LLM with\\nsatisfactory results in the Portuguese language, as well as to provide a model\\nthat is free for research or commercial purposes.</td>\n",
       "      <td>bode-7b-alpaca-pt-br, bode-13b-alpaca-pt-br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Nomic Embed: Training a Reproducible Long Context Text Embedder</td>\n",
       "      <td>This technical report describes the training of nomic-embed-text-v1, the\\nfirst fully reproducible, open-source, open-weights, open-data, 8192 context\\nlength English text embedding model that outperforms both OpenAI Ada-002 and\\nOpenAI text-embedding-3-small on short and long-context tasks. We release the\\ntraining code and model weights under an Apache 2 license. In contrast with\\nother open-source models, we release a training data loader with 235 million\\ncurated text pairs that allows for the full replication of nomic-embed-text-v1.\\nYou can find code and data to replicate the model at\\nhttps://github.com/nomic-ai/contrastors</td>\n",
       "      <td>nomic-embed-text-v1-unsupervised, nomic-embed-text-v1-ablated, modernbert-embed-base-unsupervised, modernbert-embed-base, modernbert-embed-large-unsupervised, modernbert-embed-large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity\\n  Text Embeddings Through Self-Knowledge Distillation</td>\n",
       "      <td>In this paper, we present a new embedding model, called M3-Embedding, which\\nis distinguished for its versatility in Multi-Linguality, Multi-Functionality,\\nand Multi-Granularity. It can support more than 100 working languages, leading\\nto new state-of-the-art performances on multi-lingual and cross-lingual\\nretrieval tasks. It can simultaneously perform the three common retrieval\\nfunctionalities of embedding model: dense retrieval, multi-vector retrieval,\\nand sparse retrieval, which provides a unified model foundation for real-world\\nIR applications. It is able to process inputs of different granularities,\\nspanning from short sentences to long documents of up to 8192 tokens. The\\neffective training of M3-Embedding involves the following technical\\ncontributions. We propose a novel self-knowledge distillation approach, where\\nthe relevance scores from different retrieval functionalities can be integrated\\nas the teacher signal to enhance the training quality. We also optimize the\\nbatching strategy, enabling a large batch size and high training throughput to\\nensure the discriminativeness of embeddings. To the best of our knowledge,\\nM3-Embedding is the first embedding model which realizes such a strong\\nversatility. The model and code will be publicly available at\\nhttps://github.com/FlagOpen/FlagEmbedding.</td>\n",
       "      <td>bge-m3-distill-8l, bilingual-document-embedding, bge-multilingual-gemma2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Multilingual E5 Text Embeddings: A Technical Report</td>\n",
       "      <td>This technical report presents the training methodology and evaluation\\nresults of the open-source multilingual E5 text embedding models, released in\\nmid-2023. Three embedding models of different sizes (small / base / large) are\\nprovided, offering a balance between the inference efficiency and embedding\\nquality. The training procedure adheres to the English E5 model recipe,\\ninvolving contrastive pre-training on 1 billion multilingual text pairs,\\nfollowed by fine-tuning on a combination of labeled datasets. Additionally, we\\nintroduce a new instruction-tuned embedding model, whose performance is on par\\nwith state-of-the-art, English-only models of similar sizes. Information\\nregarding the model release can be found at\\nhttps://github.com/microsoft/unilm/tree/master/e5 .</td>\n",
       "      <td>e5-small-korean, e5-base-korean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2D Matryoshka Sentence Embeddings</td>\n",
       "      <td>Common approaches rely on fixed-length embedding vectors from language models\\nas sentence embeddings for downstream tasks such as semantic textual similarity\\n(STS). Such methods are limited in their flexibility due to unknown\\ncomputational constraints and budgets across various applications. Matryoshka\\nRepresentation Learning (MRL) (Kusupati et al., 2022) encodes information at\\nfiner granularities, i.e., with lower embedding dimensions, to adaptively\\naccommodate ad hoc tasks. Similar accuracy can be achieved with a smaller\\nembedding size, leading to speedups in downstream tasks. Despite its improved\\nefficiency, MRL still requires traversing all Transformer layers before\\nobtaining the embedding, which remains the dominant factor in time and memory\\nconsumption. This prompts consideration of whether the fixed number of\\nTransformer layers affects representation quality and whether using\\nintermediate layers for sentence representation is feasible. In this paper, we\\nintroduce a novel sentence embedding model called Two-dimensional Matryoshka\\nSentence Embedding (2DMSE). It supports elastic settings for both embedding\\nsizes and Transformer layers, offering greater flexibility and efficiency than\\nMRL. We conduct extensive experiments on STS tasks and downstream applications.\\nThe experimental results demonstrate the effectiveness of our proposed model in\\ndynamically supporting different embedding sizes and Transformer layers,\\nallowing it to be highly adaptable to various scenarios.</td>\n",
       "      <td>distilroberta-base-nli-adaptive-layer, distilroberta-base-nli-2d-matryoshka, distilbert-base-uncased-sts-adaptive-layer, distilbert-base-uncased-sts-2d-matryoshka, mxbai-embed-xsmall-v1, mxbai-embed-2d-large-v1, french-document-embedding, DeBERTaV3-small-SentenceTransformer-AdaptiveLayerBaseline, DeBERTaV3-small-SentenceTransformer-AdaptiveLayerAll, DeBERTaV3-small-SenTra-AdaptiveLayers-AllSoft-LowTemp, DeBERTaV3-small-SenTra-AdaptiveLayers-AllSoft-HighTemp, DeBERTaV3-small-ST-AdaptiveLayers-ep2, DeBERTaV3-small-ST-AdaptiveLayerAllNormalized, DeBERTaV3-small-ST-AdaptiveLayer-Norm-ep2, DeBERTaV3-small-ST-AdaptiveLayer-3L-ep2, DeBERTaV3-small-GeneralSentenceTransformer-v2-AllSoft, DeBERTaV3-TR-AllSoft-HT, DeBERTa-ST-AllLayers-v3.1bis, DeBERTa-ST-AllLayers-v3.1, DeBERTa-ST-AllLayers-testing, me5-large-construction-esp-cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data</td>\n",
       "      <td>Large Language Models (LLMs) have shown impressive abilities in data\\nannotation, opening the way for new approaches to solve classic NLP problems.\\nIn this paper, we show how to use LLMs to create NuNER, a compact language\\nrepresentation model specialized in the Named Entity Recognition (NER) task.\\nNuNER can be fine-tuned to solve downstream NER problems in a data-efficient\\nway, outperforming similar-sized foundation models in the few-shot regime and\\ncompeting with much larger LLMs. We find that the size and entity-type\\ndiversity of the pre-training dataset are key to achieving good performance. We\\nview NuNER as a member of the broader family of task-specific foundation\\nmodels, recently unlocked by LLMs.</td>\n",
       "      <td>nuner-v2_fewnerd_fine_super, nuner-v1_orgs, nuner-v1_ontonotes5, nuner-v1_fewnerd_fine_super, nuner-v1_fewnerd_coarse_super</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>FuseChat: Knowledge Fusion of Chat Models</td>\n",
       "      <td>While training large language models (LLMs) from scratch can indeed lead to\\nmodels with distinct capabilities and strengths, this approach incurs\\nsubstantial costs and may lead to potential redundancy in competencies. An\\nalternative strategy is to combine existing LLMs into a more robust LLM,\\nthereby diminishing the necessity for expensive pre-training. However, due to\\nthe diverse architectures of LLMs, direct parameter blending proves to be\\nunfeasible. Recently, FuseLLM introduced the concept of knowledge\\nfusion to transfer the collective knowledge of multiple structurally varied\\nLLMs into a target LLM through lightweight continual training. In this report,\\nwe extend the scalability and flexibility of the FuseLLM framework to\\nrealize the fusion of chat LLMs, resulting in FuseChat.\\nFuseChat comprises two main stages. Firstly, we undertake knowledge\\nfusion for structurally and scale-varied source LLMs to derive multiple target\\nLLMs of identical structure and size via lightweight fine-tuning. Then, these\\ntarget LLMs are merged within the parameter space, wherein we propose a novel\\nmethod for determining the merging weights based on the variation ratio of\\nparameter matrices before and after fine-tuning. We validate our approach using\\nthree prominent chat LLMs with diverse architectures and scales, namely\\nNH2-Mixtral-8x7B, NH2-Solar-10.7B, and\\nOpenChat-3.5-7B. Experimental results spanning various chat domains\\ndemonstrate the superiority of \\textsc{FuseChat-7B} across a broad\\nspectrum of chat LLMs at 7B and 34B scales, even surpassing GPT-3.5\\n(March) and approaching Mixtral-8x7B-Instruct. Our code, model\\nweights, and data are openly accessible at\\nhttps://github.com/fanqiwan/FuseLLM.</td>\n",
       "      <td>OpenChat-3.5-7B-Mixtral, FuseChat-7B-TA, FuseChat-7B-Slerp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>GISTEmbed: Guided In-sample Selection of Training Negatives for Text\\n  Embedding Fine-tuning</td>\n",
       "      <td>Embedding models are integral to AI applications like semantic search,\\npersonalized recommendations, and retrieval augmented generation for LLMs,\\nnecessitating high-quality training data. However, the limited scalability of\\nmanual data curation prompts the need for automated methods to ensure data\\nintegrity. Traditional unsupervised triplet mining automates training data\\ngeneration, crucial for embedding model training, yet inadvertently injects\\nbiases and noise, thereby degrading model performance. Addressing this, we\\nintroduce GISTEmbed, a novel strategy that enhances in-batch negative selection\\nduring contrastive training through a guide model. This approach departs from\\nreliance on random sampling and equal utility assumption of batch negatives,\\nsignificantly reducing noise from data quality issues and improving model\\nfine-tuning. Benchmarked against the Massive Text Embedding Benchmark (MTEB),\\nGISTEmbed showcases consistent performance improvements across various model\\nsizes and achieves state-of-the-art results in select categories. This\\nframework enables significant enhancements for smaller models by leveraging the\\ncapabilities of powerful yet resource-intensive large models. GISTEmbed can\\npotentially revolutionize the creation of highly efficient, smaller models,\\ndemocratizing access to advanced AI technologies. Making these technologies\\nmore accessible and cost-effective, especially for applications constrained by\\nresources, significantly expands the impact and accessibility of\\nstate-of-the-art AI solutions across diverse sectors.</td>\n",
       "      <td>distilroberta-base-nli-v3, embedding-finetuned, bge_pairs, phobert_GISTEmbedLoss, bert-large-cantonese-nli, french-document-embedding, DeBERTaV3-small-SenTra-AdaptiveLayers-AllSoft-LowTemp, DeBERTaV3-small-SenTra-AdaptiveLayers-AllSoft-HighTemp, DeBERTaV3-small-GeneralSentenceTransformer-v2-checkpoints-tmp, DeBERTaV3-small-GeneralSentenceTransformer-v2-AllSoft, DeBERTaV3-small-GeneralSentenceTransformer, DeBERTaV3-TR-AllSoft-HT, DeBERTa3-s-CustomPoolin-toytest3-step1, DeBERTa3-s-CustomPoolin-toytest2-step1, DeBERTa3-s-CustomPoolin-toytest-step1, DeBERTa-ST-AllLayers-v3.1bis, DeBERTa-ST-AllLayers-v3.1, DeBERTa-ST-AllLayers-testing, GIST-large-Embedding-v0, GIST-Embedding-v0, embedding_finetuned_test, embedding_finetuned, mpnet-base-GISTEmbedLoss-MSEE_Evaluator-salestax-docs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings</td>\n",
       "      <td>We introduce a novel suite of state-of-the-art bilingual text embedding\\nmodels that are designed to support English and another target language. These\\nmodels are capable of processing lengthy text inputs with up to 8192 tokens,\\nmaking them highly versatile for a range of natural language processing tasks\\nsuch as text retrieval, clustering, and semantic textual similarity (STS)\\ncalculations.\\n  By focusing on bilingual models and introducing a unique multi-task learning\\nobjective, we have significantly improved the model performance on STS tasks,\\nwhich outperforms the capabilities of existing multilingual models in both\\ntarget language understanding and cross-lingual evaluation tasks. Moreover, our\\nbilingual models are more efficient, requiring fewer parameters and less memory\\ndue to their smaller vocabulary needs. Furthermore, we have expanded the\\nMassive Text Embedding Benchmark (MTEB) to include benchmarks for German and\\nSpanish embedding models. This integration aims to stimulate further research\\nand advancement in text embedding technologies for these languages.</td>\n",
       "      <td>jina-embeddings-v2-base-es, jina-embeddings-v2-base-de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>MathScale: Scaling Instruction Tuning for Mathematical Reasoning</td>\n",
       "      <td>Large language models (LLMs) have demonstrated remarkable capabilities in\\nproblem-solving. However, their proficiency in solving mathematical problems\\nremains inadequate. We propose MathScale, a simple and scalable method to\\ncreate high-quality mathematical reasoning data using frontier LLMs (e.g., {\\tt\\nGPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning,\\nit first extracts topics and knowledge points from seed math questions and then\\nbuild a concept graph, which is subsequently used to generate new math\\nquestions. MathScale exhibits effective scalability along the size axis of the\\nmath dataset that we generate. As a result, we create a mathematical reasoning\\ndataset (MathScaleQA) containing two million math question-answer pairs. To\\nevaluate mathematical reasoning abilities of LLMs comprehensively, we construct\\n{\\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten\\ndatasets (including GSM8K and MATH) covering K-12, college, and competition\\nlevel math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g.,\\nLLaMA-2 and Mistral), resulting in significantly improved capabilities in\\nmathematical reasoning. Evaluated on {\\sc MwpBench}, MathScale-7B achieves\\nstate-of-the-art performance across all datasets, surpassing its best peers of\\nequivalent size by 42.9\\% in micro average accuracy and 43.7\\% in macro average\\naccuracy, respectively.</td>\n",
       "      <td>dart-math-mistral-7b-uniform, dart-math-mistral-7b-prop2diff, dart-math-llama3-8b-uniform, dart-math-llama3-70b-uniform, dart-math-llama3-70b-prop2diff, dart-math-dsmath-7b-uniform, dart-math-dsmath-7b-prop2diff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Common 7B Language Models Already Possess Strong Math Capabilities</td>\n",
       "      <td>Mathematical capabilities were previously believed to emerge in common\\nlanguage models only at a very large scale or require extensive math-related\\npre-training. This paper shows that the LLaMA-2 7B model with common\\npre-training already exhibits strong mathematical abilities, as evidenced by\\nits impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks,\\nrespectively, when selecting the best response from 256 random generations. The\\nprimary issue with the current base model is the difficulty in consistently\\neliciting its inherent mathematical capabilities. Notably, the accuracy for the\\nfirst answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks,\\nrespectively. We find that simply scaling up the SFT data can significantly\\nenhance the reliability of generating correct answers. However, the potential\\nfor extensive scaling is constrained by the scarcity of publicly available math\\nquestions. To overcome this limitation, we employ synthetic data, which proves\\nto be nearly as effective as real data and shows no clear saturation when\\nscaled up to approximately one million samples. This straightforward approach\\nachieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7B\\nmodels, surpassing previous models by 14.2% and 20.8%, respectively. We also\\nprovide insights into scaling behaviors across different reasoning complexities\\nand error types.</td>\n",
       "      <td>dart-math-mistral-7b-uniform, dart-math-mistral-7b-prop2diff, dart-math-llama3-8b-uniform, dart-math-llama3-70b-uniform, dart-math-llama3-70b-prop2diff, dart-math-dsmath-7b-uniform, dart-math-dsmath-7b-prop2diff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in\\n  Low-Resource Languages</td>\n",
       "      <td>Large pre-trained language models (PLMs) are at the forefront of advances in\\nNatural Language Processing. One widespread use case of PLMs is \"prompting\" -\\nor in-context learning - where a user provides a description of a task and some\\ncompleted examples of the task to a PLM as context before prompting the PLM to\\nperform the task on a new example. Only the largest, most capable PLMs are able\\nto perform in-context learning effectively, and these models are typically\\ntrained with a predominantly English corpus, leaving all other languages\\nbehind. The data limitations in most languages preclude the training of\\nlanguage-specific PLMs capable of prompting. Albeit the surge in work of\\nprompting settings, it is still unclear how PLMs should be adapted\\ncross-lingually specifically for prompting. We evaluate the possible methods to\\nadapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for\\nprompting in low-resource languages, namely for Kinyarwanda, Hausa, and\\nLuganda. We consider three methods: few-shot prompting (prompt),\\nlanguage-adaptive fine-tuning (LAFT), and neural machine translation\\n(translate), and evaluate on abstractive summarization, multi-class topic\\nclassification, and named-entity recognition. Although LAFT carries the\\ngreatest compute cost and intuitively should lead to the best results, our\\nexperiments exhibit that LAFT is only occasionally the optimal choice for\\nadapting PLMs for prompting. Rather, the translate and prompt settings are a\\ncompute-efficient and cost-effective method of few-shot prompting for the\\nselected low-resource languages. We find that the results are task and language\\ndependent but find that the prompting method is the best on average across all\\ntasks and languages. Results show that the prompt setting performs better than\\nboth translating and LAFT with statistical significance for all shots when\\naggregated across all tasks and languages.</td>\n",
       "      <td>llama_luganda_LAFT, llama_kinyarwanda_LAFT, llama_hausa_LAFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Model Stock: All we need is just a few fine-tuned models</td>\n",
       "      <td>This paper introduces an efficient fine-tuning method for large pre-trained\\nmodels, offering strong in-distribution (ID) and out-of-distribution (OOD)\\nperformance. Breaking away from traditional practices that need a multitude of\\nfine-tuned models for averaging, our approach employs significantly fewer\\nmodels to achieve final weights yet yield superior accuracy. Drawing from key\\ninsights in the weight space of fine-tuned weights, we uncover a strong link\\nbetween the performance and proximity to the center of weight space. Based on\\nthis, we introduce a method that approximates a center-close weight using only\\ntwo fine-tuned models, applicable during or after training. Our innovative\\nlayer-wise weight averaging technique surpasses state-of-the-art model methods\\nsuch as Model Soup, utilizing only two fine-tuned models. This strategy can be\\naptly coined Model Stock, highlighting its reliance on selecting a minimal\\nnumber of models to draw a more optimized-averaged model. We demonstrate the\\nefficacy of Model Stock with fine-tuned models based upon pre-trained CLIP\\narchitectures, achieving remarkable performance on both ID and OOD tasks on the\\nstandard benchmarks, all while barely bringing extra computational demands. Our\\ncode and pre-trained models are available at\\nhttps://github.com/naver-ai/model-stock.</td>\n",
       "      <td>Clarus-7B-v0.2, miscii-14b-0218, Llamaverse-3.1-8B-Instruct, SOVLish-Devil-8B-L3, SOVL-Mega-Mash-L3-8B, Megatron-Opus-14B-Stock, Calcium-Opus-14B-Merge, Calcium-Opus-14B-Elite-Stock, StableProse, llama-3-stinky-v2-8B, llama-3-spicy-abliterated-stella-8B, Stella-mistral-nemo-12B-v2, Gutensuppe-mistral-nemo-12B, llama-3-NeuralMahou-8b, IceDrinkNameNotFoundRP-7b-Model_Stock, Ice0.17-03.10-RP, flammen26-mistral-7B, MN-Chinofun-12B-4, MN-Chinofun-12B-3, MN-Chinofun-12B-2, MN-Chinofun, L3.1-ForStHS, G2-BigGSHT-27B-2, BuddyGlass_v0.3_Xortron7MethedUpSwitchedUp, Qwen2.5-7B-MixStock-V0.1, Qwen2.5-3B-Model-Stock-v3.2, Qwen2.5-3B-Model-Stock-v3.1, Qwen-2.5-7B-R1-Stock, Qwen-2.5-7B-Deep-Stock-v4, Qwen-2.5-7B-Deep-Stock-v1, Phi-4-RStock-v0.1, Phi-4-Model-Stock-v4, Phi-4-Model-Stock-v3, Phi-4-Model-Stock-v2, Phi-4-Model-Stock, Llama-3.1-8B-TitanFusion-v3, Llama-3.1-8B-TitanFusion-Mix, MISCHIEVOUS-12B, QwenStock3-14B, QwenStock2-14B, QwenStock1-14B, GemmaStock1-27B, Llama3.1-8B-drill, Qwen2.5-14B-1M-YOYO-V3, Set-70b, L3.1-8B-Dusky-Ink_v0.r1, Gemmadevi-Stock-10B, Annunaki-12b, KRONOS-8B-V8, KRONOS-8B-V4, Llama_3.2_3b_Kermes_v2.1, Llama_3.2_3b_Kermes_v1, test24, Zelus-8B-Model_Stock, ONeil-model_stock-8B, LemonP-8B-Model_Stock, Derivative-8B-Model_Stock, Aspire-8B-model_stock, Qwen2.5-14B-Wernicke-SFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Latxa: An Open Language Model and Evaluation Suite for Basque</td>\n",
       "      <td>We introduce Latxa, a family of large language models for Basque ranging from\\n7 to 70 billion parameters. Latxa is based on Llama 2, which we continue\\npretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens.\\nAddressing the scarcity of high-quality benchmarks for Basque, we further\\nintroduce 4 multiple choice evaluation datasets: EusProficiency, comprising\\n5,169 questions from official language proficiency exams; EusReading,\\ncomprising 352 reading comprehension questions; EusTrivia, comprising 1,715\\ntrivia questions from 5 knowledge areas; and EusExams, comprising 16,774\\nquestions from public examinations. In our extensive evaluation, Latxa\\noutperforms all previous open models we compare to by a large margin. In\\naddition, it is competitive with GPT-4 Turbo in language proficiency and\\nunderstanding, despite lagging behind in reading comprehension and\\nknowledge-intensive tasks. Both the Latxa family of models, as well as our new\\npretraining corpora and evaluation datasets, are publicly available under open\\nlicenses at https://github.com/hitz-zentroa/latxa. Our suite enables\\nreproducible research on methods to build LLMs for low-resource languages.</td>\n",
       "      <td>latxa-70b-v1.2, latxa-70b-v1.1, latxa-13b-v1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders</td>\n",
       "      <td>Large decoder-only language models (LLMs) are the state-of-the-art models on\\nmost of today's NLP tasks and benchmarks. Yet, the community is only slowly\\nadopting these models for text embedding tasks, which require rich\\ncontextualized representations. In this work, we introduce LLM2Vec, a simple\\nunsupervised approach that can transform any decoder-only LLM into a strong\\ntext encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional\\nattention, 2) masked next token prediction, and 3) unsupervised contrastive\\nlearning. We demonstrate the effectiveness of LLM2Vec by applying it to 3\\npopular LLMs ranging from 1.3B to 7B parameters and evaluate the transformed\\nmodels on English word- and sequence-level tasks. We outperform encoder-only\\nmodels by a large margin on word-level tasks and reach a new unsupervised\\nstate-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB).\\nMoreover, when combining LLM2Vec with supervised contrastive learning, we\\nachieve state-of-the-art performance on MTEB among models that train only on\\npublicly available data. Our strong empirical results and extensive analysis\\ndemonstrate that LLMs can be effectively transformed into universal text\\nencoders in a parameter-efficient manner without the need for expensive\\nadaptation or synthetic GPT-4 generated data.</td>\n",
       "      <td>LLM2Vec-Sheared-LLaMA-mntp-unsup-simcse, LLM2Vec-Sheared-LLaMA-mntp-supervised, LLM2Vec-Mistral-7B-Instruct-v2-mntp-unsup-simcse, LLM2Vec-Mistral-7B-Instruct-v2-mntp-supervised, LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-unsup-simcse, LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised, LLM2Vec-Llama-2-7b-chat-hf-mntp-unsup-simcse, LLM2Vec-Llama-2-7b-chat-hf-mntp-supervised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Towards Large-Scale Training of Pathology Foundation Models</td>\n",
       "      <td>Driven by the recent advances in deep learning methods and, in particular, by\\nthe development of modern self-supervised learning algorithms, increased\\ninterest and efforts have been devoted to build foundation models (FMs) for\\nmedical images. In this work, we present our scalable training pipeline for\\nlarge pathology imaging data, and a comprehensive analysis of various\\nhyperparameter choices and training techniques for building pathology FMs. We\\nrelease and make publicly available the first batch of our pathology FMs\\n(https://github.com/kaiko-ai/towards_large_pathology_fms) trained on\\nopen-access TCGA whole slide images, a commonly used collection of pathology\\nimages. The experimental evaluation shows that our models reach\\nstate-of-the-art performance on various patch-level downstream tasks, ranging\\nfrom breast cancer subtyping to colorectal nuclear segmentation. Finally, to\\nunify the evaluation approaches used in the field and to simplify future\\ncomparisons of different FMs, we present an open-source framework\\n(https://github.com/kaiko-ai/eva) designed for the consistent evaluation of\\npathology FMs across various downstream tasks.</td>\n",
       "      <td>vit_small_patch8_224.kaiko_ai_towards_large_pathology_fms, vit_small_patch16_224.kaiko_ai_towards_large_pathology_fms, vit_large_patch14_reg4_224.kaiko_ai_towards_large_pathology_fms, vit_base_patch8_224.kaiko_ai_towards_large_pathology_fms, vit_base_patch16_224.kaiko_ai_towards_large_pathology_fms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Granite Code Models: A Family of Open Foundation Models for Code\\n  Intelligence</td>\n",
       "      <td>Large Language Models (LLMs) trained on code are revolutionizing the software\\ndevelopment process. Increasingly, code LLMs are being integrated into software\\ndevelopment environments to improve the productivity of human programmers, and\\nLLM-based agents are beginning to show promise for handling complex tasks\\nautonomously. Realizing the full potential of code LLMs requires a wide range\\nof capabilities, including code generation, fixing bugs, explaining and\\ndocumenting code, maintaining repositories, and more. In this work, we\\nintroduce the Granite series of decoder-only code models for code generative\\ntasks, trained with code written in 116 programming languages. The Granite Code\\nmodels family consists of models ranging in size from 3 to 34 billion\\nparameters, suitable for applications ranging from complex application\\nmodernization tasks to on-device memory-constrained use cases. Evaluation on a\\ncomprehensive set of tasks demonstrates that Granite Code models consistently\\nreaches state-of-the-art performance among available open-source code LLMs. The\\nGranite Code model family was optimized for enterprise software development\\nworkflows and performs well across a range of coding tasks (e.g. code\\ngeneration, fixing and explanation), making it a versatile all around code\\nmodel. We release all our Granite Code models under an Apache 2.0 license for\\nboth research and commercial use.</td>\n",
       "      <td>granite-8b-code-base-128k, granite-3b-code-instruct-128k, granite-20b-code-instruct-r1.1, granite-20b-code-base-r1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models</td>\n",
       "      <td>This report describes the training dataset creation and recipe behind the\\nfamily of arctic-embed text embedding models (a set of five models\\nranging from 22 to 334 million parameters with weights open-sourced under an\\nApache-2 license). At the time of their release, each model achieved\\nstate-of-the-art retrieval accuracy for models of their size on the MTEB\\nRetrieval leaderboard, with the largest model, arctic-embed-l outperforming\\nclosed source embedding models such as Cohere's embed-v3 and Open AI's\\ntext-embed-3-large. In addition to the details of our training recipe, we have\\nprovided several informative ablation studies, which we believe are the cause\\nof our model performance.</td>\n",
       "      <td>snowflake-arctic-embed-xs, snowflake-arctic-embed-s, snowflake-arctic-embed-m-long, snowflake-arctic-embed-m, snowflake-arctic-embed-l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Qiskit Code Assistant: Training LLMs for generating Quantum Computing\\n  Code</td>\n",
       "      <td>Code Large Language Models (Code LLMs) have emerged as powerful tools,\\nrevolutionizing the software development landscape by automating the coding\\nprocess and reducing time and effort required to build applications. This paper\\nfocuses on training Code LLMs to specialize in the field of quantum computing.\\nWe begin by discussing the unique needs of quantum computing programming, which\\ndiffer significantly from classical programming approaches or languages. A Code\\nLLM specializing in quantum computing requires a foundational understanding of\\nquantum computing and quantum information theory. However, the scarcity of\\navailable quantum code examples and the rapidly evolving field, which\\nnecessitates continuous dataset updates, present significant challenges.\\nMoreover, we discuss our work on training Code LLMs to produce high-quality\\nquantum code using the Qiskit library. This work includes an examination of the\\nvarious aspects of the LLMs used for training and the specific training\\nconditions, as well as the results obtained with our current models. To\\nevaluate our models, we have developed a custom benchmark, similar to\\nHumanEval, which includes a set of tests specifically designed for the field of\\nquantum computing programming using Qiskit. Our findings indicate that our\\nmodel outperforms existing state-of-the-art models in quantum computing tasks.\\nWe also provide examples of code suggestions, comparing our model to other\\nrelevant code LLMs. Finally, we introduce a discussion on the potential\\nbenefits of Code LLMs for quantum computing computational scientists,\\nresearchers, and practitioners. We also explore various features and future\\nwork that could be relevant in this context.</td>\n",
       "      <td>granite-8b-qiskit-rc-0.10, granite-8b-qiskit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>DELLA-Merging: Reducing Interference in Model Merging through\\n  Magnitude-Based Sampling</td>\n",
       "      <td>With the proliferation of domain-specific models, model merging has emerged\\nas a set of techniques that combine the capabilities of multiple models into\\none that can multitask without the cost of additional training. In this paper,\\nwe propose a new model merging technique, Drop and rEscaLe via sampLing with\\nmAgnitude (DELLA-Merging), that employs a novel pruning technique, MAGPRUNE,\\nwhich shows significant advantages over DARE and TIES. MAGPRUNE first ranks the\\nparameters in order of their magnitude and assigns higher dropout probabilities\\n(p) to parameters with lower ranks corresponding to lower magnitudes. To\\napproximate the original embeddings, MAGPRUNE employs a rescaling operation on\\nthe parameters that survive the random dropping by 1/(1 - p). On three\\ndifferent expert models considered for merging (LM, Math, Code) and\\ncorresponding benchmark datasets (AlpacaEval, GSM8K, MBPP), DELLA shows an\\naverage improvement of 2.4 points over baseline methods employing delta\\nparameter pruning (an improvement of 3.6 points over TIES, 1.2 points over\\nDARE), and 11.1 points over the no-pruning baseline (TA). We release the source\\ncode at: https://github.com/declare-lab/della.</td>\n",
       "      <td>Qwen2.5-14B-1M-YOYO-V3, Qwen2.5-THREADRIPPER-Small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Qiskit HumanEval: An Evaluation Benchmark For Quantum Code Generative\\n  Models</td>\n",
       "      <td>Quantum programs are typically developed using quantum Software Development\\nKits (SDKs). The rapid advancement of quantum computing necessitates new tools\\nto streamline this development process, and one such tool could be Generative\\nArtificial intelligence (GenAI). In this study, we introduce and use the Qiskit\\nHumanEval dataset, a hand-curated collection of tasks designed to benchmark the\\nability of Large Language Models (LLMs) to produce quantum code using Qiskit -\\na quantum SDK. This dataset consists of more than 100 quantum computing tasks,\\neach accompanied by a prompt, a canonical solution, a comprehensive test case,\\nand a difficulty scale to evaluate the correctness of the generated solutions.\\nWe systematically assess the performance of a set of LLMs against the Qiskit\\nHumanEval dataset's tasks and focus on the models ability in producing\\nexecutable quantum code. Our findings not only demonstrate the feasibility of\\nusing LLMs for generating quantum code but also establish a new benchmark for\\nongoing advancements in the field and encourage further exploration and\\ndevelopment of GenAI-driven tools for quantum code generation.</td>\n",
       "      <td>granite-8b-qiskit-rc-0.10, granite-8b-qiskit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>\"Vorbeti Romnete?\" A Recipe to Train Powerful Romanian LLMs\\n  with English Instructions</td>\n",
       "      <td>In recent years, Large Language Models (LLMs) have achieved almost human-like\\nperformance on various tasks. While some LLMs have been trained on multilingual\\ndata, most of the training data is in English; hence, their performance in\\nEnglish greatly exceeds other languages. To our knowledge, we are the first to\\ncollect and translate a large collection of texts, instructions, and benchmarks\\nand train, evaluate, and release open-source LLMs tailored for Romanian. We\\nevaluate our methods on four different categories, including academic\\nbenchmarks, MT-Bench (manually translated), and a professionally built\\nhistorical, cultural, and social benchmark adapted to Romanian. We argue for\\nthe usefulness and high performance of RoLLMs by obtaining state-of-the-art\\nresults across the board. We publicly release all resources (i.e., data,\\ntraining and evaluation code, models) to support and encourage research on\\nRomanian LLMs while concurrently creating a generalizable recipe, adequate for\\nother low or less-resourced languages.</td>\n",
       "      <td>RoMistral-7b-Instruct-2024-05-17, RoLlama3-8b-Instruct-2024-06-28, RoLlama2-7b-Instruct-2024-05-14, RoGemma-7b-Instruct-2024-06-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Beyond Benchmarks: Evaluating Embedding Model Similarity for Retrieval\\n  Augmented Generation Systems</td>\n",
       "      <td>The choice of embedding model is a crucial step in the design of Retrieval\\nAugmented Generation (RAG) systems. Given the sheer volume of available\\noptions, identifying clusters of similar models streamlines this model\\nselection process. Relying solely on benchmark performance scores only allows\\nfor a weak assessment of model similarity. Thus, in this study, we evaluate the\\nsimilarity of embedding models within the context of RAG systems. Our\\nassessment is two-fold: We use Centered Kernel Alignment to compare embeddings\\non a pair-wise level. Additionally, as it is especially pertinent to RAG\\nsystems, we evaluate the similarity of retrieval results between these models\\nusing Jaccard and rank similarity. We compare different families of embedding\\nmodels, including proprietary ones, across five datasets from the popular\\nBenchmark Information Retrieval (BEIR). Through our experiments we identify\\nclusters of models corresponding to model families, but interestingly, also\\nsome inter-family clusters. Furthermore, our analysis of top-k retrieval\\nsimilarity reveals high-variance at low k values. We also identify possible\\nopen-source alternatives to proprietary models, with Mistral exhibiting the\\nhighest similarity to OpenAI models.</td>\n",
       "      <td>arctic-s-bge-small, arctic-m-bge-small, arctic-l-bge-small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Qwen2 Technical Report</td>\n",
       "      <td>This report introduces the Qwen2 series, the latest addition to our large\\nlanguage models and large multimodal models. We release a comprehensive suite\\nof foundational and instruction-tuned language models, encompassing a parameter\\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\\nQwen1.5, and exhibits competitive performance relative to proprietary models\\nacross diverse benchmarks on language understanding, generation, multilingual\\nproficiency, coding, mathematics, and reasoning.\\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\\ndemonstrates robust multilingual capabilities, proficient in approximately 30\\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\\nglobal reach.\\n  To foster community innovation and accessibility, we have made the Qwen2\\nmodel weights openly available on Hugging Face1 and ModelScope2, and the\\nsupplementary materials including example code on GitHub3. These platforms also\\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\\nwide range of applications and research endeavors.</td>\n",
       "      <td>Josiefied-Qwen2.5-7B-Instruct-abliterated-GGUF, Josiefied-Qwen2.5-14B-Instruct-abliterated-v4, huihui-ai-abliterated-Qwen2.5-32B-Inst-BaseMerge-TIES, Rombos-Qwen2.5-7B-Inst-BaseMerge-TIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Vibravox: A Dataset of French Speech Captured with Body-conduction Audio\\n  Sensors</td>\n",
       "      <td>Vibravox is a dataset compliant with the General Data Protection Regulation\\n(GDPR) containing audio recordings using five different body-conduction audio\\nsensors : two in-ear microphones, two bone conduction vibration pickups and a\\nlaryngophone. The data set also includes audio data from an airborne microphone\\nused as a reference. The Vibravox corpus contains 38 hours of speech samples\\nand physiological sounds recorded by 188 participants under different acoustic\\nconditions imposed by an high order ambisonics 3D spatializer. Annotations\\nabout the recording conditions and linguistic transcriptions are also included\\nin the corpus. We conducted a series of experiments on various speech-related\\ntasks, including speech recognition, speech enhancement and speaker\\nverification. These experiments were carried out using state-of-the-art models\\nto evaluate and compare their performances on signals captured by the different\\naudio sensors offered by the Vibravox dataset, with the aim of gaining a better\\ngrasp of their individual characteristics.</td>\n",
       "      <td>phonemizer_throat_microphone, phonemizer_temple_vibration_pickup, phonemizer_soft_in_ear_microphone, phonemizer_rigid_in_ear_microphone, phonemizer_headset_microphone, phonemizer_forehead_accelerometer, EBEN_throat_microphone, EBEN_temple_vibration_pickup, EBEN_soft_in_ear_microphone, EBEN_rigid_in_ear_microphone, EBEN_reverse_throat_microphone, EBEN_reverse_temple_vibration_pickup, EBEN_reverse_soft_in_ear_microphone, EBEN_reverse_rigid_in_ear_microphone, EBEN_reverse_forehead_accelerometer, EBEN_forehead_accelerometer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Embedding And Clustering Your Data Can Improve Contrastive Pretraining</td>\n",
       "      <td>Recent studies of large-scale contrastive pretraining in the text embedding\\ndomain show that using single-source minibatches, rather than mixed-source\\nminibatches, can substantially improve overall model accuracy. In this work, we\\nexplore extending training data stratification beyond source granularity by\\nleveraging a pretrained text embedding model and the classic k-means clustering\\nalgorithm to further split training data apart by the semantic clusters within\\neach source. Experimentally, we observe a notable increase in NDCG@10 when\\npretraining a BERT-based text embedding model on query-passage pairs from the\\nMSMARCO passage retrieval dataset. Additionally, we conceptually connect our\\nclustering approach to both the Topic Aware Sampling (TAS) aspect of the TAS-B\\nmethodology and the nearest-neighbor-based hard-negative mining aspect of the\\nANCE methodology and discuss how this unified view motivates future lines of\\nresearch on the organization of contrastive pretraining data.</td>\n",
       "      <td>snowflake-arctic-embed-xs, snowflake-arctic-embed-s, snowflake-arctic-embed-m-long, snowflake-arctic-embed-m, snowflake-arctic-embed-l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>mGTE: Generalized Long-Context Text Representation and Reranking Models\\n  for Multilingual Text Retrieval</td>\n",
       "      <td>We present systematic efforts in building long-context multilingual text\\nrepresentation model (TRM) and reranker from scratch for text retrieval. We\\nfirst introduce a text encoder (base size) enhanced with RoPE and unpadding,\\npre-trained in a native 8192-token context (longer than 512 of previous\\nmultilingual encoders). Then we construct a hybrid TRM and a cross-encoder\\nreranker by contrastive learning. Evaluations show that our text encoder\\noutperforms the same-sized previous state-of-the-art XLM-R. Meanwhile, our TRM\\nand reranker match the performance of large-sized state-of-the-art BGE-M3\\nmodels and achieve better results on long-context retrieval benchmarks. Further\\nanalysis demonstrate that our proposed models exhibit higher efficiency during\\nboth training and inference. We believe their efficiency and effectiveness\\ncould benefit various researches and industrial applications.</td>\n",
       "      <td>gte-base-korean, french-document-embedding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Enhancing Semantic Similarity Understanding in Arabic NLP with Nested\\n  Embedding Learning</td>\n",
       "      <td>This work presents a novel framework for training Arabic nested embedding\\nmodels through Matryoshka Embedding Learning, leveraging multilingual,\\nArabic-specific, and English-based models, to highlight the power of nested\\nembeddings models in various Arabic NLP downstream tasks. Our innovative\\ncontribution includes the translation of various sentence similarity datasets\\ninto Arabic, enabling a comprehensive evaluation framework to compare these\\nmodels across different dimensions. We trained several nested embedding models\\non the Arabic Natural Language Inference triplet dataset and assessed their\\nperformance using multiple evaluation metrics, including Pearson and Spearman\\ncorrelations for cosine similarity, Manhattan distance, Euclidean distance, and\\ndot product similarity. The results demonstrate the superior performance of the\\nMatryoshka embedding models, particularly in capturing semantic nuances unique\\nto the Arabic language. Results demonstrated that Arabic Matryoshka embedding\\nmodels have superior performance in capturing semantic nuances unique to the\\nArabic language, significantly outperforming traditional models by up to\\n20-25\\% across various similarity metrics. These results underscore the\\neffectiveness of language-specific training and highlight the potential of\\nMatryoshka models in enhancing semantic textual similarity tasks for Arabic\\nNLP.</td>\n",
       "      <td>Marbert-all-nli-triplet-Matryoshka, Arabic-mpnet-base-all-nli-triplet, Arabic-labse-Matryoshka, Arabic-all-nli-triplet-Matryoshka, Arabic-Triplet-Matryoshka-V2, Arabert-all-nli-triplet-Matryoshka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Maverick: Efficient and Accurate Coreference Resolution Defying Recent\\n  Trends</td>\n",
       "      <td>Large autoregressive generative models have emerged as the cornerstone for\\nachieving the highest performance across several Natural Language Processing\\ntasks. However, the urge to attain superior results has, at times, led to the\\npremature replacement of carefully designed task-specific approaches without\\nexhaustive experimentation. The Coreference Resolution task is no exception;\\nall recent state-of-the-art solutions adopt large generative autoregressive\\nmodels that outperform encoder-based discriminative systems. In this work,we\\nchallenge this recent trend by introducing Maverick, a carefully designed - yet\\nsimple - pipeline, which enables running a state-of-the-art Coreference\\nResolution system within the constraints of an academic budget, outperforming\\nmodels with up to 13 billion parameters with as few as 500 million parameters.\\nMaverick achieves state-of-the-art performance on the CoNLL-2012 benchmark,\\ntraining with up to 0.006x the memory resources and obtaining a 170x faster\\ninference compared to previous state-of-the-art systems. We extensively\\nvalidate the robustness of the Maverick framework with an array of diverse\\nexperiments, reporting improvements over prior systems in data-scarce,\\nlong-document, and out-of-domain settings. We release our code and models for\\nresearch purposes at https://github.com/SapienzaNLP/maverick-coref.</td>\n",
       "      <td>maverick-mes-preco, maverick-mes-ontonotes, maverick-mes-litbank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>LLaVA-OneVision: Easy Visual Task Transfer</td>\n",
       "      <td>We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\\ndeveloped by consolidating our insights into data, models, and visual\\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\\ndemonstrate that LLaVA-OneVision is the first single model that can\\nsimultaneously push the performance boundaries of open LMMs in three important\\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\\nacross different modalities/scenarios, yielding new emerging capabilities. In\\nparticular, strong video understanding and cross-scenario capabilities are\\ndemonstrated through task transfer from images to videos.</td>\n",
       "      <td>llava-onevision-qwen2-7b-si, llava-onevision-qwen2-7b-ov, llava-onevision-qwen2-72b-si, llava-onevision-qwen2-0.5b-si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>The Russian-focused embedders' exploration: ruMTEB benchmark and Russian\\n  embedding model design</td>\n",
       "      <td>Embedding models play a crucial role in Natural Language Processing (NLP) by\\ncreating text embeddings used in various tasks such as information retrieval\\nand assessing semantic text similarity. This paper focuses on research related\\nto embedding models in the Russian language. It introduces a new\\nRussian-focused embedding model called ru-en-RoSBERTa and the ruMTEB benchmark,\\nthe Russian version extending the Massive Text Embedding Benchmark (MTEB). Our\\nbenchmark includes seven categories of tasks, such as semantic textual\\nsimilarity, text classification, reranking, and retrieval. The research also\\nassesses a representative set of Russian and multilingual models on the\\nproposed benchmark. The findings indicate that the new model achieves results\\nthat are on par with state-of-the-art models in Russian. We release the model\\nru-en-RoSBERTa, and the ruMTEB framework comes with open-source code,\\nintegration into the original framework and a public leaderboard.</td>\n",
       "      <td>ru-en-RoSBERTa, FRIDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Video Instruction Tuning With Synthetic Data</td>\n",
       "      <td>The development of video large multimodal models (LMMs) has been hindered by\\nthe difficulty of curating large amounts of high-quality raw data from the web.\\nTo address this, we propose an alternative approach by creating a high-quality\\nsynthetic dataset specifically for video instruction-following, namely\\nLLaVA-Video-178K. This dataset includes key tasks such as detailed captioning,\\nopen-ended question-answering (QA), and multiple-choice QA. By training on this\\ndataset, in combination with existing visual instruction tuning data, we\\nintroduce LLaVA-Video, a new video LMM. Our experiments demonstrate that\\nLLaVA-Video achieves strong performance across various video benchmarks,\\nhighlighting the effectiveness of our dataset. We plan to release the dataset,\\nits generation pipeline, and the model checkpoints.</td>\n",
       "      <td>LLaVA-Video-7B-Qwen2-Video-Only, LLaVA-Video-7B-Qwen2, LLaVA-Video-72B-Qwen2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>AuroraCap: Efficient, Performant Video Detailed Captioning and a New\\n  Benchmark</td>\n",
       "      <td>Video detailed captioning is a key task which aims to generate comprehensive\\nand coherent textual descriptions of video content, benefiting both video\\nunderstanding and generation. In this paper, we propose AuroraCap, a video\\ncaptioner based on a large multimodal model. We follow the simplest\\narchitecture design without additional parameters for temporal modeling. To\\naddress the overhead caused by lengthy video sequences, we implement the token\\nmerging strategy, reducing the number of input visual tokens. Surprisingly, we\\nfound that this strategy results in little performance loss. AuroraCap shows\\nsuperior performance on various video and image captioning benchmarks, for\\nexample, obtaining a CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and\\nGemini-1.5 Pro (82.2). However, existing video caption benchmarks only include\\nsimple descriptions, consisting of a few dozen words, which limits research in\\nthis field. Therefore, we develop VDC, a video detailed captioning benchmark\\nwith over one thousand carefully annotated structured captions. In addition, we\\npropose a new LLM-assisted metric VDCscore for bettering evaluation, which\\nadopts a divide-and-conquer strategy to transform long caption evaluation into\\nmultiple short question-answer pairs. With the help of human Elo ranking, our\\nexperiments show that this benchmark better correlates with human judgments of\\nvideo detailed captioning quality.</td>\n",
       "      <td>AuroraCap-7B-VID-xtuner, AuroraCap-7B-IMG-xtuner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Tracking Universal Features Through Fine-Tuning and Model Merging</td>\n",
       "      <td>We study how features emerge, disappear, and persist across models fine-tuned\\non different domains of text. More specifically, we start from a base one-layer\\nTransformer language model that is trained on a combination of the BabyLM\\ncorpus, and a collection of Python code from The Stack. This base model is\\nadapted to two new domains of text: TinyStories, and the Lua programming\\nlanguage, respectively; and then these two models are merged using these two\\nmodels using spherical linear interpolation. Our exploration aims to provide\\ndeeper insights into the stability and transformation of features across\\ntypical transfer-learning scenarios using small-scale models and sparse\\nauto-encoders.</td>\n",
       "      <td>baby-python-mistral-1L-tiny-lua-ft, baby-python-mistral-1L-tiny-base, baby-python-mistral-1L-tiny-TinyStories-ft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model</td>\n",
       "      <td>OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5,\\nfinetuned on over 2,000,000 Thai instruction pairs. This report provides an\\nengineering perspective on the model's development, capabilities, and\\nperformance. We discuss the model's architecture, training process, and key\\nfeatures, including multi-turn conversation support, Retrieval Augmented\\nGeneration (RAG) compatibility, and tool-calling functionality. Benchmark\\nresults demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various\\nThai language tasks, outperforming other open-source Thai language models. We\\nalso address practical considerations such as GPU memory requirements and\\ndeployment strategies.</td>\n",
       "      <td>openthaigpt1.5-7b-instruct, openthaigpt1.5-72b-instruct, openthaigpt1.5-14b-instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Tucano: Advancing Neural Text Generation for Portuguese</td>\n",
       "      <td>Significant advances have been made in natural language processing in recent\\nyears. However, our current deep learning approach to language modeling\\nrequires substantial resources in terms of data and computation. One of the\\nside effects of this data-hungry paradigm is the current schism between\\nlanguages, separating those considered high-resource, where most of the\\ndevelopment happens and resources are available, and the low-resource ones,\\nwhich struggle to attain the same level of performance and autonomy. This study\\naims to introduce a new set of resources to stimulate the future development of\\nneural text generation in Portuguese. In this work, we document the development\\nof GigaVerbo, a concatenation of deduplicated Portuguese text corpora amounting\\nto 200 billion tokens. Via this corpus, we trained a series of\\ndecoder-transformers named Tucano. Our models perform equal or superior to\\nother Portuguese and multilingual language models of similar size in several\\nPortuguese benchmarks. The evaluation of our models also reveals that model\\nperformance on many currently available benchmarks used by the Portuguese NLP\\ncommunity has little to no correlation with the scaling of token ingestion\\nduring training, highlighting the limitations of such evaluations when it comes\\nto the assessment of Portuguese generative language models. All derivatives of\\nour study are openly released on GitHub and Hugging Face. See\\nhttps://nkluge-correa.github.io/Tucano/</td>\n",
       "      <td>Tucano-630m, Tucano-160m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>CamemBERT 2.0: A Smarter French Language Model Aged to Perfection</td>\n",
       "      <td>French language models, such as CamemBERT, have been widely adopted across\\nindustries for natural language processing (NLP) tasks, with models like\\nCamemBERT seeing over 4 million downloads per month. However, these models face\\nchallenges due to temporal concept drift, where outdated training data leads to\\na decline in performance, especially when encountering new topics and\\nterminology. This issue emphasizes the need for updated models that reflect\\ncurrent linguistic trends. In this paper, we introduce two new versions of the\\nCamemBERT base model-CamemBERTav2 and CamemBERTv2-designed to address these\\nchallenges. CamemBERTav2 is based on the DeBERTaV3 architecture and makes use\\nof the Replaced Token Detection (RTD) objective for better contextual\\nunderstanding, while CamemBERTv2 is built on RoBERTa, which uses the Masked\\nLanguage Modeling (MLM) objective. Both models are trained on a significantly\\nlarger and more recent dataset with longer context length and an updated\\ntokenizer that enhances tokenization performance for French. We evaluate the\\nperformance of these models on both general-domain NLP tasks and\\ndomain-specific applications, such as medical field tasks, demonstrating their\\nversatility and effectiveness across a range of use cases. Our results show\\nthat these updated models vastly outperform their predecessors, making them\\nvaluable tools for modern NLP systems. All our new models, as well as\\nintermediate checkpoints, are made openly available on Huggingface.</td>\n",
       "      <td>camembertv2-base-xnli, camembertv2-base-pawsx, camembertv2-base-ftb-ner, camembertv2-base-fquad, camembertv2-base-cls, camembertav2-base-xnli, camembertav2-base-pawsx, camembertav2-base-ftb-ner, camembertav2-base-fquad, camembertav2-base-cls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Multimodal Autoregressive Pre-training of Large Vision Encoders</td>\n",
       "      <td>We introduce a novel method for pre-training of large-scale vision encoders.\\nBuilding on recent advancements in autoregressive pre-training of vision\\nmodels, we extend this framework to a multimodal setting, i.e., images and\\ntext. In this paper, we present AIMV2, a family of generalist vision encoders\\ncharacterized by a straightforward pre-training process, scalability, and\\nremarkable performance across a range of downstream tasks. This is achieved by\\npairing the vision encoder with a multimodal decoder that autoregressively\\ngenerates raw image patches and text tokens. Our encoders excel not only in\\nmultimodal evaluations but also in vision benchmarks such as localization,\\ngrounding, and classification. Notably, our AIMV2-3B encoder achieves 89.5%\\naccuracy on ImageNet-1k with a frozen trunk. Furthermore, AIMV2 consistently\\noutperforms state-of-the-art contrastive models (e.g., CLIP, SigLIP) in\\nmultimodal image understanding across diverse settings.</td>\n",
       "      <td>aimv2-large-patch14-448, aimv2-large-patch14-336, aimv2-large-patch14-224, aimv2-huge-patch14-448, aimv2-huge-patch14-336, aimv2-huge-patch14-224, aimv2-3B-patch14-448, aimv2-3B-patch14-336, aimv2-3B-patch14-224, aimv2-1B-patch14-448, aimv2-1B-patch14-336, aimv2-1B-patch14-224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>TLU 3: Pushing Frontiers in Open Language Model Post-Training</td>\n",
       "      <td>Language model post-training is applied to refine behaviors and unlock new\\nskills across a wide range of recent language models, but open recipes for\\napplying these techniques lag behind proprietary ones. The underlying training\\ndata and recipes for post-training are simultaneously the most important pieces\\nof the puzzle and the portion with the least transparency. To bridge this gap,\\nwe introduce T\\\"ULU 3, a family of fully-open state-of-the-art post-trained\\nmodels, alongside its data, code, and training recipes, serving as a\\ncomprehensive guide for modern post-training techniques. T\\\"ULU 3, which builds\\non Llama 3.1 base models, achieves results surpassing the instruct versions of\\nLlama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and\\nClaude 3.5-Haiku. The training algorithms for our models include supervised\\nfinetuning (SFT), Direct Preference Optimization (DPO), and a novel method we\\ncall Reinforcement Learning with Verifiable Rewards (RLVR). With T\\\"ULU 3, we\\nintroduce a multi-task evaluation scheme for post-training recipes with\\ndevelopment and unseen evaluations, standard benchmark implementations, and\\nsubstantial decontamination of existing open datasets on said benchmarks. We\\nconclude with analysis and discussion of training methods that did not reliably\\nimprove performance.\\n  In addition to the T\\\"ULU 3 model weights and demo, we release the complete\\nrecipe -- including datasets for diverse core skills, a robust toolkit for data\\ncuration and evaluation, the training code and infrastructure, and, most\\nimportantly, a detailed report for reproducing and further adapting the T\\\"ULU\\n3 approach to more domains.</td>\n",
       "      <td>Llama-3.1-Tulu-3-70B-SFT, SmolTulu-1.7b-Instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>A Supervised Machine Learning Approach for Assessing Grant Peer Review\\n  Reports</td>\n",
       "      <td>Peer review in grant evaluation informs funding decisions, but the contents\\nof peer review reports are rarely analyzed. In this work, we develop a\\nthoroughly tested pipeline to analyze the texts of grant peer review reports\\nusing methods from applied Natural Language Processing (NLP) and machine\\nlearning. We start by developing twelve categories reflecting content of grant\\npeer review reports that are of interest to research funders. This is followed\\nby multiple human annotators' iterative annotation of these categories in a\\nnovel text corpus of grant peer review reports submitted to the Swiss National\\nScience Foundation. After validating the human annotation, we use the annotated\\ntexts to fine-tune pre-trained transformer models to classify these categories\\nat scale, while conducting several robustness and validation checks. Our\\nresults show that many categories can be reliably identified by human\\nannotators and machine learning approaches. However, the choice of text\\nclassification approach considerably influences the classification performance.\\nWe also find a high correspondence between out-of-sample classification\\nperformance and human annotators' perceived difficulty in identifying\\ncategories. Our results and publicly available fine-tuned transformer models\\nwill allow researchers and research funders and anybody interested in peer\\nreview to examine and report on the contents of these reports in a structured\\nmanner. Ultimately, we hope our approach can contribute to ensuring the quality\\nand trustworthiness of grant peer review.</td>\n",
       "      <td>specter2-review-suitability, specter2-review-suggestion, specter2-review-relevance-originality-topicality, specter2-review-rationale, specter2-review-positive, specter2-review-feasibility, specter2-review-applicant-quantity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual\\n  Prompt Instruction Tuning</td>\n",
       "      <td>Large Multimodal Models (LMMs) have made significant breakthroughs with the\\nadvancement of instruction tuning. However, while existing models can\\nunderstand images and videos at a holistic level, they still struggle with\\ninstance-level understanding that requires a more nuanced comprehension and\\nalignment. Instance-level understanding is crucial, as it focuses on the\\nspecific elements that we are most interested in. Excitingly, existing works\\nfind that the state-of-the-art LMMs exhibit strong instance understanding\\ncapabilities when provided with explicit visual cues. Motivated by this, we\\nintroduce an automated annotation pipeline assisted by GPT-4o to extract\\ninstance-level information from images and videos through explicit visual\\nprompting for instance guidance. Building upon this pipeline, we proposed\\nInst-IT, a solution to enhance LMMs in Instance understanding via explicit\\nvisual prompt Instruction Tuning. Inst-IT consists of a benchmark to diagnose\\nmultimodal instance-level understanding, a large-scale instruction-tuning\\ndataset, and a continuous instruction-tuning training paradigm to effectively\\nenhance spatial-temporal instance understanding capabilities of existing LMMs.\\nExperimental results show that, with the boost of Inst-IT, our models not only\\nachieve outstanding performance on Inst-IT Bench but also demonstrate\\nsignificant improvements across various generic image and video understanding\\nbenchmarks. This highlights that our dataset not only boosts instance-level\\nunderstanding but also strengthens the overall capabilities of generic image\\nand video comprehension.</td>\n",
       "      <td>LLaVA-Next-Inst-It-Vicuna-7B, LLaVA-Next-Inst-It-Qwen2-7B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for\\n  Fast, Memory Efficient, and Long Context Finetuning and Inference</td>\n",
       "      <td>Encoder-only transformer models such as BERT offer a great performance-size\\ntradeoff for retrieval and classification tasks with respect to larger\\ndecoder-only models. Despite being the workhorse of numerous production\\npipelines, there have been limited Pareto improvements to BERT since its\\nrelease. In this paper, we introduce ModernBERT, bringing modern model\\noptimizations to encoder-only models and representing a major Pareto\\nimprovement over older encoders. Trained on 2 trillion tokens with a native\\n8192 sequence length, ModernBERT models exhibit state-of-the-art results on a\\nlarge pool of evaluations encompassing diverse classification tasks and both\\nsingle and multi-vector retrieval on different domains (including code). In\\naddition to strong downstream performance, ModernBERT is also the most speed\\nand memory efficient encoder and is designed for inference on common GPUs.</td>\n",
       "      <td>modernbert-embed-large-unsupervised, modernbert-embed-large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>GME: Improving Universal Multimodal Retrieval by Multimodal LLMs</td>\n",
       "      <td>Universal Multimodal Retrieval (UMR) aims to enable search across various\\nmodalities using a unified model, where queries and candidates can consist of\\npure text, images, or a combination of both. Previous work has attempted to\\nadopt multimodal large language models (MLLMs) to realize UMR using only text\\ndata. However, our preliminary experiments demonstrate that more diverse\\nmultimodal training data can further unlock the potential of MLLMs. Despite its\\neffectiveness, the existing multimodal training data is highly imbalanced in\\nterms of modality, which motivates us to develop a training data synthesis\\npipeline and construct a large-scale, high-quality fused-modal training\\ndataset. Based on the synthetic training data, we develop the General\\nMultimodal Embedder (GME), an MLLM-based dense retriever designed for UMR.\\nFurthermore, we construct a comprehensive UMR Benchmark (UMRB) to evaluate the\\neffectiveness of our approach. Experimental results show that our method\\nachieves state-of-the-art performance among existing UMR methods. Last, we\\nprovide in-depth analyses of model scaling, training strategies, and perform\\nablation studies on both the model and synthetic data.</td>\n",
       "      <td>gme-Qwen2-VL-7B-Instruct, gme-Qwen2-VL-2B-Instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>VideoChat-Flash: Hierarchical Compression for Long-Context Video\\n  Modeling</td>\n",
       "      <td>Long-context modeling is a critical capability for multimodal large language\\nmodels (MLLMs), enabling them to process long-form contents with implicit\\nmemorization. Despite its advances, handling extremely long videos remains\\nchallenging due to the difficulty in maintaining crucial features over extended\\nsequences. This paper introduces a Hierarchical visual token Compression (HiCo)\\nmethod designed for high-fidelity representation and a practical context\\nmodeling system VideoChat-Flash tailored for multimodal long-sequence\\nprocessing. HiCo capitalizes on the redundancy of visual information in long\\nvideos to compress long video context from the clip-level to the video-level,\\nreducing the compute significantly while preserving essential details.\\nVideoChat-Flash features a multi-stage short-to-long learning scheme, a rich\\ndataset of real-world long videos named LongVid, and an upgraded\\n\"Needle-In-A-video-Haystack\" (NIAH) for evaluating context capacities. In\\nextensive experiments, VideoChat-Flash shows the leading performance on both\\nmainstream long and short video benchmarks at the 7B model scale. It firstly\\ngets 99.1% accuracy over 10,000 frames in NIAH among open-source models.</td>\n",
       "      <td>VideoChat-Flash-Qwen2_5-7B_InternVideo2-1B, VideoChat-Flash-Qwen2_5-7B-1M_res224, VideoChat-Flash-Qwen2_5-2B_res448, VideoChat-Flash-Qwen2-7B_res448, VideoChat-Flash-Qwen2-7B_res224, InternVL_2_5_HiCo_R64, InternVL_2_5_HiCo_R16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model</td>\n",
       "      <td>As retrieval-augmented generation prevails in large language models,\\nembedding models are becoming increasingly crucial. Despite the growing number\\nof general embedding models, prior work often overlooks the critical role of\\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\\nmultilingual embedding model that leverages a large quantity of cleaner, more\\ndiverse, and domain-specific training data. Our model has been trained with key\\ntechniques proven to enhance performance: (1) persona-based synthetic data to\\ncreate diversified examples distilled from LLMs, (2) ranking consistency\\nfiltering to remove less informative samples, and (3) semi-homogeneous task\\nbatch sampling to improve training efficacy. Departing from traditional\\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\\nfacilitating the adaptation of auto-regressive language models for general\\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\\nlanguages show that our model outperforms others of comparable size, setting a\\nnew standard for multilingual embedding models with &lt;1B parameters.</td>\n",
       "      <td>KaLM-embedding-multilingual-mini-v1, KaLM-embedding-multilingual-mini-instruct-v1.5, KaLM-embedding-multilingual-mini-instruct-v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Enhancing Lexicon-Based Text Embeddings with Large Language Models</td>\n",
       "      <td>Recent large language models (LLMs) have demonstrated exceptional performance\\non general-purpose text embedding tasks. While dense embeddings have dominated\\nrelated research, we introduce the first Lexicon-based EmbeddiNgS (LENS)\\nleveraging LLMs that achieve competitive performance on these tasks. Regarding\\nthe inherent tokenization redundancy issue and unidirectional attention\\nlimitations in traditional causal LLMs, LENS consolidates the vocabulary space\\nthrough token embedding clustering, and investigates bidirectional attention\\nand various pooling strategies. Specifically, LENS simplifies lexicon matching\\nby assigning each dimension to a specific token cluster, where semantically\\nsimilar tokens are grouped together, and unlocking the full potential of LLMs\\nthrough bidirectional attention. Extensive experiments demonstrate that LENS\\noutperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB),\\ndelivering compact feature representations that match the sizes of dense\\ncounterparts. Notably, combining LENSE with dense embeddings achieves\\nstate-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).</td>\n",
       "      <td>LENS-d8000, LENS-d4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Control LLM: Controlled Evolution for Intelligence Retention in LLM</td>\n",
       "      <td>Large Language Models (LLMs) demand significant computational resources,\\nmaking it essential to enhance their capabilities without retraining from\\nscratch. A key challenge in this domain is catastrophic forgetting\\n(CF), which hampers performance during Continuous Pre-training (CPT) and\\nContinuous Supervised Fine-Tuning (CSFT). We propose Control LLM, a\\nnovel approach that leverages parallel pre-trained and expanded transformer\\nblocks, aligning their hidden-states through interpolation strategies This\\nmethod effectively preserves performance on existing tasks while seamlessly\\nintegrating new knowledge.\\n  Extensive experiments demonstrate the effectiveness of Control LLM in both\\nCPT and CSFT. On Llama3.1-8B-Instruct, it achieves significant improvements in\\nmathematical reasoning (+14.4% on Math-Hard) and coding performance (+10%\\non MBPP-PLUS). On Llama3.1-8B, it enhances multilingual capabilities (+10.6%\\non C-Eval, +6.8% on CMMLU, and +30.2% on CMMLU-0shot-CoT). It surpasses\\nexisting methods and achieves SOTA among open-source models tuned from the same\\nbase model, using substantially less data and compute. Crucially, these gains\\nare realized while preserving strong original capabilities, with minimal\\ndegradation (&lt;4.3% on MMLU) compared to &gt;35% in open-source Math\\nand Coding models. This approach has been successfully deployed in LinkedIn's\\nGenAI-powered job seeker and Ads unit products.\\n  To support further research, we release the training and evaluation code\\n(https://github.com/linkedin/ControlLLM) along with models trained on\\npublic datasets ( https://huggingface.co/ControlLLM) to the community.</td>\n",
       "      <td>Llama3.1-8B-OpenMath16-Instruct, Llama-3.1-8B-SynE-Hybrid16, Llama-3.1-8B-SynE-FPT, Llama-3.1-8B-SynE-Concat16-Lerp, Llama-3.1-8B-SynE-Concat16-Dlerp, Llama-3.1-8B-OpenCoder16-Instruct, Control-LLM-Llama3.1-8B-OpenCoder8-Instruct, Control-LLM-Llama3.1-8B-Math16-Instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>InternVideo2.5: Empowering Video MLLMs with Long and Rich Context\\n  Modeling</td>\n",
       "      <td>This paper aims to improve the performance of video multimodal large language\\nmodels (MLLM) via long and rich context (LRC) modeling. As a result, we develop\\na new version of InternVideo2.5 with a focus on enhancing the original MLLMs'\\nability to perceive fine-grained details and capture long-form temporal\\nstructure in videos. Specifically, our approach incorporates dense vision task\\nannotations into MLLMs using direct preference optimization and develops\\ncompact spatiotemporal representations through adaptive hierarchical token\\ncompression. Experimental results demonstrate this unique design of LRC greatly\\nimproves the results of video MLLM in mainstream video understanding benchmarks\\n(short &amp; long), enabling the MLLM to memorize significantly longer video inputs\\n(at least 6x longer than the original), and master specialized vision\\ncapabilities like object tracking and segmentation. Our work highlights the\\nimportance of multimodal context richness (length and fineness) in empowering\\nMLLM's innate abilites (focus and memory), providing new insights for future\\nresearch on video MLLM. Code and models are available at\\nhttps://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2.5</td>\n",
       "      <td>InternVideo2_5_Chat_8B, InternVL_2_5_HiCo_R64, InternVL_2_5_HiCo_R16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>ChunkFormer: Masked Chunking Conformer For Long-Form Speech\\n  Transcription</td>\n",
       "      <td>Deploying ASR models at an industrial scale poses significant challenges in\\nhardware resource management, especially for long-form transcription tasks\\nwhere audio may last for hours. Large Conformer models, despite their\\ncapabilities, are limited to processing only 15 minutes of audio on an 80GB\\nGPU. Furthermore, variable input lengths worsen inefficiencies, as standard\\nbatching leads to excessive padding, increasing resource consumption and\\nexecution time. To address this, we introduce ChunkFormer, an efficient ASR\\nmodel that uses chunk-wise processing with relative right context, enabling\\nlong audio transcriptions on low-memory GPUs. ChunkFormer handles up to 16\\nhours of audio on an 80GB GPU, 1.5x longer than the current state-of-the-art\\nFastConformer, while also boosting long-form transcription performance with up\\nto 7.7% absolute reduction on word error rate and maintaining accuracy on\\nshorter tasks compared to Conformer. By eliminating the need for padding in\\nstandard batching, ChunkFormer's masked batching technique reduces execution\\ntime and memory usage by more than 3x in batch processing, substantially\\nreducing costs for a wide range of ASR systems, particularly regarding GPU\\nresources for models serving in real-world applications.</td>\n",
       "      <td>chunkformer-large-vie, chunkformer-large-en-libri-960h</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_query(g, \"queries/results/q11.rq\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
