{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==0.28 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==0.28) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==0.28) (4.67.1)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from openai==0.28) (3.11.12)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.20->openai==0.28) (2025.1.31)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->openai==0.28) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->openai==0.28) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->openai==0.28) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->openai==0.28) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->openai==0.28) (1.18.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==0.28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameters scraping from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/jinaai/jina-embeddings-v3\n",
      "Hyperparameter not found for https://huggingface.co/jinaai/jina-embeddings-v3\n",
      "https://huggingface.co/openai/whisper-base.en\n",
      "Hyperparameter not found for https://huggingface.co/openai/whisper-base.en\n",
      "https://huggingface.co/jinaai/jina-embeddings-v2-small-en\n",
      "Hyperparameter not found for https://huggingface.co/jinaai/jina-embeddings-v2-small-en\n",
      "https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\n",
      "Hyperparameter found for https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\n",
      "https://huggingface.co/avsolatorio/GIST-Embedding-v0\n",
      "Hyperparameter not found for https://huggingface.co/avsolatorio/GIST-Embedding-v0\n",
      "https://huggingface.co/Snowflake/snowflake-arctic-embed-m\n",
      "Hyperparameter not found for https://huggingface.co/Snowflake/snowflake-arctic-embed-m\n",
      "https://huggingface.co/nvidia/parakeet-rnnt-0.6b\n",
      "Hyperparameter not found for https://huggingface.co/nvidia/parakeet-rnnt-0.6b\n",
      "https://huggingface.co/facebook/hubert-large-ls960-ft\n",
      "Hyperparameter not found for https://huggingface.co/facebook/hubert-large-ls960-ft\n",
      "https://huggingface.co/Snowflake/snowflake-arctic-embed-xs\n",
      "Hyperparameter not found for https://huggingface.co/Snowflake/snowflake-arctic-embed-xs\n",
      "https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion\n",
      "Hyperparameter found for https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion\n",
      "https://huggingface.co/KBLab/wav2vec2-large-voxrex-swedish\n",
      "Hyperparameter not found for https://huggingface.co/KBLab/wav2vec2-large-voxrex-swedish\n",
      "https://huggingface.co/Intel/dpt-hybrid-midas\n",
      "Hyperparameter not found for https://huggingface.co/Intel/dpt-hybrid-midas\n",
      "https://huggingface.co/BAAI/bge-multilingual-gemma2\n",
      "Hyperparameter not found for https://huggingface.co/BAAI/bge-multilingual-gemma2\n",
      "https://huggingface.co/google/pegasus-xsum\n",
      "Hyperparameter not found for https://huggingface.co/google/pegasus-xsum\n",
      "https://huggingface.co/nvidia/parakeet-tdt-1.1b\n",
      "Hyperparameter not found for https://huggingface.co/nvidia/parakeet-tdt-1.1b\n",
      "https://huggingface.co/avsolatorio/GIST-large-Embedding-v0\n",
      "Hyperparameter not found for https://huggingface.co/avsolatorio/GIST-large-Embedding-v0\n",
      "https://huggingface.co/thenlper/gte-large-zh\n",
      "Hyperparameter not found for https://huggingface.co/thenlper/gte-large-zh\n",
      "https://huggingface.co/nomic-ai/modernbert-embed-base\n",
      "Hyperparameter not found for https://huggingface.co/nomic-ai/modernbert-embed-base\n",
      "https://huggingface.co/biu-nlp/f-coref\n",
      "Hyperparameter not found for https://huggingface.co/biu-nlp/f-coref\n",
      "https://huggingface.co/Snowflake/snowflake-arctic-embed-l-v2.0\n",
      "Hyperparameter not found for https://huggingface.co/Snowflake/snowflake-arctic-embed-l-v2.0\n",
      "https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov\n",
      "Hyperparameter not found for https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-ov\n",
      "https://huggingface.co/facebook/wav2vec2-conformer-rope-large-960h-ft\n",
      "Hyperparameter not found for https://huggingface.co/facebook/wav2vec2-conformer-rope-large-960h-ft\n",
      "https://huggingface.co/lmms-lab/LLaVA-Video-7B-Qwen2\n",
      "Hyperparameter not found for https://huggingface.co/lmms-lab/LLaVA-Video-7B-Qwen2\n",
      "https://huggingface.co/NbAiLab/nb-wav2vec2-300m-nynorsk\n",
      "Hyperparameter found for https://huggingface.co/NbAiLab/nb-wav2vec2-300m-nynorsk\n",
      "https://huggingface.co/HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5\n",
      "Hyperparameter not found for https://huggingface.co/HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5\n",
      "https://huggingface.co/nvidia/parakeet-ctc-1.1b\n",
      "Hyperparameter not found for https://huggingface.co/nvidia/parakeet-ctc-1.1b\n",
      "https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\n",
      "Hyperparameter found for https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\n",
      "https://huggingface.co/thenlper/gte-base-zh\n",
      "Hyperparameter not found for https://huggingface.co/thenlper/gte-base-zh\n",
      "https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct\n",
      "Hyperparameter not found for https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-2B-Instruct\n",
      "https://huggingface.co/jinaai/jina-embeddings-v2-base-de\n",
      "Hyperparameter not found for https://huggingface.co/jinaai/jina-embeddings-v2-base-de\n",
      "https://huggingface.co/NbAiLab/nb-wav2vec2-1b-nynorsk\n",
      "Hyperparameter found for https://huggingface.co/NbAiLab/nb-wav2vec2-1b-nynorsk\n",
      "https://huggingface.co/MIT/ast-finetuned-speech-commands-v2\n",
      "Hyperparameter not found for https://huggingface.co/MIT/ast-finetuned-speech-commands-v2\n",
      "https://huggingface.co/bigscience/mt0-small\n",
      "Hyperparameter not found for https://huggingface.co/bigscience/mt0-small\n",
      "https://huggingface.co/jinaai/jina-embeddings-v2-base-es\n",
      "Hyperparameter not found for https://huggingface.co/jinaai/jina-embeddings-v2-base-es\n",
      "https://huggingface.co/Omartificial-Intelligence-Space/Arabic-Triplet-Matryoshka-V2\n",
      "Hyperparameter not found for https://huggingface.co/Omartificial-Intelligence-Space/Arabic-Triplet-Matryoshka-V2\n",
      "https://huggingface.co/Snowflake/snowflake-arctic-embed-l\n",
      "Hyperparameter not found for https://huggingface.co/Snowflake/snowflake-arctic-embed-l\n",
      "https://huggingface.co/nvidia/parakeet-rnnt-1.1b\n",
      "Hyperparameter not found for https://huggingface.co/nvidia/parakeet-rnnt-1.1b\n",
      "https://huggingface.co/biu-nlp/lingmess-coref\n",
      "Hyperparameter not found for https://huggingface.co/biu-nlp/lingmess-coref\n",
      "https://huggingface.co/HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1\n",
      "Hyperparameter not found for https://huggingface.co/HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1\n",
      "https://huggingface.co/Snowflake/snowflake-arctic-embed-s\n",
      "Hyperparameter not found for https://huggingface.co/Snowflake/snowflake-arctic-embed-s\n",
      "https://huggingface.co/Snowflake/snowflake-arctic-embed-m-long\n",
      "Hyperparameter not found for https://huggingface.co/Snowflake/snowflake-arctic-embed-m-long\n",
      "https://huggingface.co/NbAiLab/nb-wav2vec2-1b-bokmaal\n",
      "Hyperparameter found for https://huggingface.co/NbAiLab/nb-wav2vec2-1b-bokmaal\n",
      "https://huggingface.co/speechbrain/sepformer-dns4-16k-enhancement\n",
      "Hyperparameter not found for https://huggingface.co/speechbrain/sepformer-dns4-16k-enhancement\n",
      "https://huggingface.co/Lajavaness/bilingual-embedding-large\n",
      "Hyperparameter not found for https://huggingface.co/Lajavaness/bilingual-embedding-large\n",
      "https://huggingface.co/sileod/deberta-v3-base-tasksource-nli\n",
      "Hyperparameter not found for https://huggingface.co/sileod/deberta-v3-base-tasksource-nli\n",
      "https://huggingface.co/ml6team/keyphrase-extraction-distilbert-inspec\n",
      "Hyperparameter not found for https://huggingface.co/ml6team/keyphrase-extraction-distilbert-inspec\n",
      "https://huggingface.co/nvidia/canary-1b\n",
      "Hyperparameter not found for https://huggingface.co/nvidia/canary-1b\n",
      "https://huggingface.co/line-corporation/open-universe\n",
      "Hyperparameter not found for https://huggingface.co/line-corporation/open-universe\n",
      "https://huggingface.co/bhadresh-savani/bert-base-uncased-emotion\n",
      "Hyperparameter not found for https://huggingface.co/bhadresh-savani/bert-base-uncased-emotion\n",
      "https://huggingface.co/robowaifudev/megatron-gpt2-345m\n",
      "Hyperparameter not found for https://huggingface.co/robowaifudev/megatron-gpt2-345m\n",
      "https://huggingface.co/mixedbread-ai/mxbai-embed-xsmall-v1\n",
      "Hyperparameter not found for https://huggingface.co/mixedbread-ai/mxbai-embed-xsmall-v1\n",
      "https://huggingface.co/GeneZC/MiniChat-2-3B\n",
      "Hyperparameter not found for https://huggingface.co/GeneZC/MiniChat-2-3B\n",
      "https://huggingface.co/Lajavaness/bilingual-embedding-small\n",
      "Hyperparameter not found for https://huggingface.co/Lajavaness/bilingual-embedding-small\n",
      "https://huggingface.co/facebook/bart-large-xsum\n",
      "Hyperparameter not found for https://huggingface.co/facebook/bart-large-xsum\n",
      "https://huggingface.co/nvidia/parakeet-tdt_ctc-110m\n",
      "Hyperparameter not found for https://huggingface.co/nvidia/parakeet-tdt_ctc-110m\n",
      "https://huggingface.co/OpenGVLab/InternVideo2_5_Chat_8B\n",
      "Hyperparameter not found for https://huggingface.co/OpenGVLab/InternVideo2_5_Chat_8B\n",
      "https://huggingface.co/Cnam-LMSSC/EBEN_temple_vibration_pickup\n",
      "Hyperparameter not found for https://huggingface.co/Cnam-LMSSC/EBEN_temple_vibration_pickup\n",
      "https://huggingface.co/ibm-research/PowerLM-3b\n",
      "Hyperparameter not found for https://huggingface.co/ibm-research/PowerLM-3b\n",
      "https://huggingface.co/Lajavaness/sentence-flaubert-base\n",
      "Hyperparameter not found for https://huggingface.co/Lajavaness/sentence-flaubert-base\n",
      "https://huggingface.co/dangvantuan/french-document-embedding\n",
      "Hyperparameter not found for https://huggingface.co/dangvantuan/french-document-embedding\n",
      "https://huggingface.co/lealdaniel/comp-embedding-matching\n",
      "Hyperparameter found for https://huggingface.co/lealdaniel/comp-embedding-matching\n",
      "https://huggingface.co/jxm/cde-small-v2\n",
      "Hyperparameter found for https://huggingface.co/jxm/cde-small-v2\n",
      "https://huggingface.co/ai-forever/FRIDA\n",
      "Hyperparameter not found for https://huggingface.co/ai-forever/FRIDA\n",
      "https://huggingface.co/upskyy/e5-base-korean\n",
      "Hyperparameter not found for https://huggingface.co/upskyy/e5-base-korean\n",
      "https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0\n",
      "Hyperparameter not found for https://huggingface.co/kotoba-tech/kotoba-whisper-v2.0\n",
      "https://huggingface.co/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised\n",
      "Hyperparameter not found for https://huggingface.co/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-supervised\n",
      "https://huggingface.co/google/bigbird-pegasus-large-arxiv\n",
      "Hyperparameter not found for https://huggingface.co/google/bigbird-pegasus-large-arxiv\n",
      "https://huggingface.co/Intel/bert-base-uncased-mrpc\n",
      "Hyperparameter found for https://huggingface.co/Intel/bert-base-uncased-mrpc\n",
      "https://huggingface.co/lmms-lab/llava-onevision-qwen2-0.5b-si\n",
      "Hyperparameter not found for https://huggingface.co/lmms-lab/llava-onevision-qwen2-0.5b-si\n",
      "https://huggingface.co/TIGER-Lab/MAmmoTH2-7B-Plus\n",
      "Hyperparameter not found for https://huggingface.co/TIGER-Lab/MAmmoTH2-7B-Plus\n",
      "https://huggingface.co/upskyy/e5-small-korean\n",
      "Hyperparameter not found for https://huggingface.co/upskyy/e5-small-korean\n",
      "https://huggingface.co/marroyo777/bge-99GPT-v1\n",
      "Hyperparameter found for https://huggingface.co/marroyo777/bge-99GPT-v1\n",
      "https://huggingface.co/HIT-TMG/KaLM-embedding-multilingual-mini-v1\n",
      "Hyperparameter not found for https://huggingface.co/HIT-TMG/KaLM-embedding-multilingual-mini-v1\n",
      "https://huggingface.co/facebook/hubert-xlarge-ls960-ft\n",
      "Hyperparameter not found for https://huggingface.co/facebook/hubert-xlarge-ls960-ft\n",
      "https://huggingface.co/lgris/wav2vec2-large-xlsr-open-brazilian-portuguese-v2\n",
      "Hyperparameter not found for https://huggingface.co/lgris/wav2vec2-large-xlsr-open-brazilian-portuguese-v2\n",
      "https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-si\n",
      "Hyperparameter not found for https://huggingface.co/lmms-lab/llava-onevision-qwen2-7b-si\n",
      "https://huggingface.co/aspire/acge_text_embedding\n",
      "Hyperparameter not found for https://huggingface.co/aspire/acge_text_embedding\n",
      "https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral\n",
      "Hyperparameter not found for https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral\n",
      "https://huggingface.co/zake7749/gemma-2-2b-it-chinese-kyara-dpo\n",
      "Hyperparameter not found for https://huggingface.co/zake7749/gemma-2-2b-it-chinese-kyara-dpo\n",
      "https://huggingface.co/microsoft/xclip-base-patch16-kinetics-600-16-frames\n",
      "Hyperparameter not found for https://huggingface.co/microsoft/xclip-base-patch16-kinetics-600-16-frames\n",
      "https://huggingface.co/ai-forever/ru-en-RoSBERTa\n",
      "Hyperparameter not found for https://huggingface.co/ai-forever/ru-en-RoSBERTa\n",
      "https://huggingface.co/openbmb/MiniCPM-Embedding\n",
      "Hyperparameter not found for https://huggingface.co/openbmb/MiniCPM-Embedding\n",
      "https://huggingface.co/McGill-NLP/roberta-large-faithcritic\n",
      "Hyperparameter found for https://huggingface.co/McGill-NLP/roberta-large-faithcritic\n",
      "https://huggingface.co/pankajmathur/orca_mini_v3_13b\n",
      "Hyperparameter not found for https://huggingface.co/pankajmathur/orca_mini_v3_13b\n",
      "https://huggingface.co/NovaSearch/jasper_en_vision_language_v1\n",
      "Hyperparameter not found for https://huggingface.co/NovaSearch/jasper_en_vision_language_v1\n",
      "https://huggingface.co/nvidia/parakeet-ctc-0.6b\n",
      "Hyperparameter not found for https://huggingface.co/nvidia/parakeet-ctc-0.6b\n",
      "https://huggingface.co/guishe/nuner-v1_orgs\n",
      "Hyperparameter found for https://huggingface.co/guishe/nuner-v1_orgs\n",
      "https://huggingface.co/nikcheerla/amd-partial-v1\n",
      "Hyperparameter found for https://huggingface.co/nikcheerla/amd-partial-v1\n",
      "https://huggingface.co/facebook/wav2vec2-large-960h-lv60\n",
      "Hyperparameter not found for https://huggingface.co/facebook/wav2vec2-large-960h-lv60\n",
      "https://huggingface.co/nvidia/diar_sortformer_4spk-v1\n",
      "Hyperparameter not found for https://huggingface.co/nvidia/diar_sortformer_4spk-v1\n",
      "https://huggingface.co/vicgalle/Configurable-Yi-1.5-9B-Chat\n",
      "Hyperparameter not found for https://huggingface.co/vicgalle/Configurable-Yi-1.5-9B-Chat\n",
      "https://huggingface.co/nvidia/NV-Embed-v1\n",
      "Hyperparameter not found for https://huggingface.co/nvidia/NV-Embed-v1\n",
      "https://huggingface.co/Cnam-LMSSC/wav2vec2-french-phonemizer\n",
      "Hyperparameter found for https://huggingface.co/Cnam-LMSSC/wav2vec2-french-phonemizer\n",
      "https://huggingface.co/louisbrulenaudet/lemone-embed-pro\n",
      "Hyperparameter found for https://huggingface.co/louisbrulenaudet/lemone-embed-pro\n",
      "https://huggingface.co/pszemraj/led-large-book-summary\n",
      "Hyperparameter found for https://huggingface.co/pszemraj/led-large-book-summary\n",
      "https://huggingface.co/xmanii/maux-gte-persian-v2\n",
      "Hyperparameter found for https://huggingface.co/xmanii/maux-gte-persian-v2\n",
      "https://huggingface.co/nvidia/parakeet-tdt_ctc-1.1b\n",
      "Hyperparameter not found for https://huggingface.co/nvidia/parakeet-tdt_ctc-1.1b\n",
      "https://huggingface.co/abhinand/tamil-llama-7b-instruct-v0.1\n",
      "Hyperparameter not found for https://huggingface.co/abhinand/tamil-llama-7b-instruct-v0.1\n",
      "https://huggingface.co/lmms-lab/LLaVA-Video-72B-Qwen2\n",
      "Hyperparameter not found for https://huggingface.co/lmms-lab/LLaVA-Video-72B-Qwen2\n",
      "https://huggingface.co/joe32140/ModernBERT-base-msmarco\n",
      "Hyperparameter found for https://huggingface.co/joe32140/ModernBERT-base-msmarco\n",
      "https://huggingface.co/ibm-granite/granite-3b-code-instruct-128k\n",
      "Hyperparameter not found for https://huggingface.co/ibm-granite/granite-3b-code-instruct-128k\n",
      "https://huggingface.co/bigscience/mt0-large\n",
      "Hyperparameter not found for https://huggingface.co/bigscience/mt0-large\n",
      "https://huggingface.co/pankajmathur/orca_mini_v3_7b\n",
      "Hyperparameter not found for https://huggingface.co/pankajmathur/orca_mini_v3_7b\n",
      "https://huggingface.co/pankajmathur/orca_mini_v3_70b\n",
      "Hyperparameter not found for https://huggingface.co/pankajmathur/orca_mini_v3_70b\n",
      "https://huggingface.co/pankajmathur/model_007\n",
      "Hyperparameter not found for https://huggingface.co/pankajmathur/model_007\n",
      "https://huggingface.co/jonny9f/food_embeddings\n",
      "Hyperparameter found for https://huggingface.co/jonny9f/food_embeddings\n",
      "https://huggingface.co/human-centered-summarization/financial-summarization-pegasus\n",
      "Hyperparameter not found for https://huggingface.co/human-centered-summarization/financial-summarization-pegasus\n",
      "https://huggingface.co/apple/aimv2-large-patch14-224\n",
      "Hyperparameter not found for https://huggingface.co/apple/aimv2-large-patch14-224\n",
      "https://huggingface.co/bigscience/mt0-base\n",
      "Hyperparameter not found for https://huggingface.co/bigscience/mt0-base\n",
      "https://huggingface.co/khanhld/chunkformer-large-vie\n",
      "Hyperparameter not found for https://huggingface.co/khanhld/chunkformer-large-vie\n",
      "https://huggingface.co/GritLM/GritLM-8x7B\n",
      "Hyperparameter not found for https://huggingface.co/GritLM/GritLM-8x7B\n",
      "https://huggingface.co/mixedbread-ai/mxbai-embed-2d-large-v1\n",
      "Hyperparameter not found for https://huggingface.co/mixedbread-ai/mxbai-embed-2d-large-v1\n",
      "https://huggingface.co/speechbrain/asr-streaming-conformer-librispeech\n",
      "Hyperparameter found for https://huggingface.co/speechbrain/asr-streaming-conformer-librispeech\n",
      "https://huggingface.co/dmlls/all-mpnet-base-v2-negation\n",
      "Hyperparameter not found for https://huggingface.co/dmlls/all-mpnet-base-v2-negation\n",
      "https://huggingface.co/TIGER-Lab/TIGERScore-13B\n",
      "Hyperparameter not found for https://huggingface.co/TIGER-Lab/TIGERScore-13B\n",
      "https://huggingface.co/McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp-unsup-simcse\n",
      "Hyperparameter not found for https://huggingface.co/McGill-NLP/LLM2Vec-Sheared-LLaMA-mntp-unsup-simcse\n",
      "https://huggingface.co/lightonai/modernbert-embed-large\n",
      "Hyperparameter not found for https://huggingface.co/lightonai/modernbert-embed-large\n",
      "https://huggingface.co/apple/aimv2-huge-patch14-448\n",
      "Hyperparameter not found for https://huggingface.co/apple/aimv2-huge-patch14-448\n",
      "https://huggingface.co/mini1013/master_item_top_bt_flat\n",
      "Hyperparameter found for https://huggingface.co/mini1013/master_item_top_bt_flat\n",
      "https://huggingface.co/nvidia/stt_en_fastconformer_transducer_large\n",
      "Hyperparameter not found for https://huggingface.co/nvidia/stt_en_fastconformer_transducer_large\n",
      "https://huggingface.co/Harveenchadha/vakyansh-wav2vec2-tamil-tam-250\n",
      "Hyperparameter not found for https://huggingface.co/Harveenchadha/vakyansh-wav2vec2-tamil-tam-250\n",
      "https://huggingface.co/bhadresh-savani/roberta-base-emotion\n",
      "Hyperparameter found for https://huggingface.co/bhadresh-savani/roberta-base-emotion\n",
      "https://huggingface.co/Intel/ldm3d-4c\n",
      "Hyperparameter not found for https://huggingface.co/Intel/ldm3d-4c\n",
      "https://huggingface.co/tomaarsen/ModernBERT-base-gooaq\n",
      "Hyperparameter found for https://huggingface.co/tomaarsen/ModernBERT-base-gooaq\n",
      "https://huggingface.co/1aurent/vit_base_patch16_224.kaiko_ai_towards_large_pathology_fms\n",
      "Hyperparameter not found for https://huggingface.co/1aurent/vit_base_patch16_224.kaiko_ai_towards_large_pathology_fms\n",
      "https://huggingface.co/apple/aimv2-3B-patch14-448\n",
      "Hyperparameter not found for https://huggingface.co/apple/aimv2-3B-patch14-448\n",
      "https://huggingface.co/infgrad/stella-base-zh-v2\n",
      "Hyperparameter not found for https://huggingface.co/infgrad/stella-base-zh-v2\n",
      "https://huggingface.co/apple/aimv2-large-patch14-448\n",
      "Hyperparameter not found for https://huggingface.co/apple/aimv2-large-patch14-448\n",
      "https://huggingface.co/openthaigpt/openthaigpt1.5-7b-instruct\n",
      "Hyperparameter not found for https://huggingface.co/openthaigpt/openthaigpt1.5-7b-instruct\n",
      "https://huggingface.co/alvanlii/whisper-small-cantonese\n",
      "Hyperparameter found for https://huggingface.co/alvanlii/whisper-small-cantonese\n",
      "https://huggingface.co/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-unsup-simcse\n",
      "Hyperparameter not found for https://huggingface.co/McGill-NLP/LLM2Vec-Meta-Llama-3-8B-Instruct-mntp-unsup-simcse\n",
      "https://huggingface.co/nvidia/stt_en_citrinet_256_ls\n",
      "Hyperparameter not found for https://huggingface.co/nvidia/stt_en_citrinet_256_ls\n",
      "https://huggingface.co/GeneZC/MiniMA-2-3B\n",
      "Hyperparameter not found for https://huggingface.co/GeneZC/MiniMA-2-3B\n",
      "https://huggingface.co/cognitivecomputations/yayi2-30b-llama\n",
      "Hyperparameter found for https://huggingface.co/cognitivecomputations/yayi2-30b-llama\n",
      "https://huggingface.co/OpenGVLab/VideoChat-Flash-Qwen2-7B_res448\n",
      "Hyperparameter not found for https://huggingface.co/OpenGVLab/VideoChat-Flash-Qwen2-7B_res448\n",
      "https://huggingface.co/gchhablani/bert-base-cased-finetuned-qnli\n",
      "Hyperparameter found for https://huggingface.co/gchhablani/bert-base-cased-finetuned-qnli\n",
      "https://huggingface.co/pankajmathur/model_101\n",
      "Hyperparameter not found for https://huggingface.co/pankajmathur/model_101\n",
      "https://huggingface.co/fblgit/juanako-7b-UNA\n",
      "Hyperparameter found for https://huggingface.co/fblgit/juanako-7b-UNA\n",
      "https://huggingface.co/pankajmathur/Lima_Unchained_70b\n",
      "Hyperparameter not found for https://huggingface.co/pankajmathur/Lima_Unchained_70b\n",
      "https://huggingface.co/jinaai/jina-embedding-b-en-v1\n",
      "Hyperparameter not found for https://huggingface.co/jinaai/jina-embedding-b-en-v1\n",
      "https://huggingface.co/allenai/digital-socrates-13b\n",
      "Hyperparameter not found for https://huggingface.co/allenai/digital-socrates-13b\n",
      "https://huggingface.co/allenai/digital-socrates-7b\n",
      "Hyperparameter not found for https://huggingface.co/allenai/digital-socrates-7b\n",
      "https://huggingface.co/abhinand/tamil-llama-13b-base-v0.1\n",
      "Hyperparameter not found for https://huggingface.co/abhinand/tamil-llama-13b-base-v0.1\n",
      "https://huggingface.co/apple/aimv2-1B-patch14-448\n",
      "Hyperparameter not found for https://huggingface.co/apple/aimv2-1B-patch14-448\n",
      "https://huggingface.co/microsoft/xclip-large-patch14\n",
      "Hyperparameter not found for https://huggingface.co/microsoft/xclip-large-patch14\n",
      "https://huggingface.co/CallComply/Starling-LM-11B-alpha\n",
      "Hyperparameter not found for https://huggingface.co/CallComply/Starling-LM-11B-alpha\n",
      "https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-7B-Instruct\n",
      "Hyperparameter not found for https://huggingface.co/Alibaba-NLP/gme-Qwen2-VL-7B-Instruct\n",
      "https://huggingface.co/speechbrain/asr-transformer-transformerlm-librispeech\n",
      "Hyperparameter not found for https://huggingface.co/speechbrain/asr-transformer-transformerlm-librispeech\n",
      "https://huggingface.co/Zardos/Kant-Test-0.1-Mistral-7B\n",
      "Hyperparameter not found for https://huggingface.co/Zardos/Kant-Test-0.1-Mistral-7B\n",
      "https://huggingface.co/ibivibiv/strix-rufipes-70b\n",
      "Hyperparameter not found for https://huggingface.co/ibivibiv/strix-rufipes-70b\n",
      "https://huggingface.co/nomic-ai/nomic-embed-text-v1-unsupervised\n",
      "Hyperparameter not found for https://huggingface.co/nomic-ai/nomic-embed-text-v1-unsupervised\n",
      "https://huggingface.co/mini1013/master_domain\n",
      "Hyperparameter found for https://huggingface.co/mini1013/master_domain\n",
      "https://huggingface.co/snunlp/KLUE-SRoBERTa-Large-SNUExtended-klueNLI-klueSTS\n",
      "Hyperparameter found for https://huggingface.co/snunlp/KLUE-SRoBERTa-Large-SNUExtended-klueNLI-klueSTS\n",
      "https://huggingface.co/mini1013/master_cate_bc5\n",
      "Hyperparameter found for https://huggingface.co/mini1013/master_cate_bc5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nimport openai\\n\\n# Set your OpenAI API key\\nopenai.api_key = \"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\"\\n\\n# Example scraped text from your BeautifulSoup extraction\\nscraped_text = text\\n\\n\\n\\n    prompt = f\"\"\"Extract the following details from the text and output them as JSON.\\n    The JSON should have the following keys:\\n    - model_name\\n    - hyperparameters\\n\\n    Text:\\n    {scraped_text}\\n\\n    Output JSON:\"\"\"\\n\\n    # Call the GPT-3.5-turbo ChatCompletion API\\n    response = openai.ChatCompletion.create(\\n        model=\"gpt-3.5-turbo\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": \"You are an assistant that extracts structured JSON from unstructured text.\"},\\n            {\"role\": \"user\", \"content\": prompt}\\n        ],\\n        temperature=0.0,  # Lower temperature for more deterministic output\\n        max_tokens=300\\n    )\\n\\n    # Get and print the generated JSON output\\n    output = response[\\'choices\\'][0][\\'message\\'][\\'content\\']\\n    print(\"Extracted JSON:\")\\n    print(output)\\nelse:\\n    print(\"Hyperparameter not found\")'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def hyperparams_and_save(url, save_dir=\"./pages_text\"):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page for {url} (status code: {response.status_code})\")\n",
    "        return False\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    text = soup.get_text(separator=\" \", strip=True)\n",
    "    \n",
    "    keywords = [\"hyperparameter\", \"hyper-parameter\", \"hyper-parameters\"]\n",
    "    if any(keyword in text.lower() for keyword in keywords):\n",
    "        print(f\"Hyperparameter found for {url}\")\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        model_id = url.rstrip(\"/\").split(\"/\")[-1]\n",
    "        file_path = os.path.join(save_dir, f\"{model_id}.txt\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Hyperparameter not found for {url}\")\n",
    "        return False\n",
    "\n",
    "i = 0\n",
    "dataset = pd.read_csv(\"./data/merged-dataset.csv\")\n",
    "dataset = dataset.dropna(subset=[\"id_y\"])\n",
    "dataset = dataset.drop_duplicates(subset=[\"id_x\"]).sort_values(\"downloads\", ascending=False)\n",
    "dataset[\"hasHyperparams\"] = False\n",
    "\n",
    "for idx, row in dataset.iterrows():\n",
    "    url = \"https://huggingface.co/\" + row[\"id_x\"]\n",
    "    print(url)\n",
    "    if hyperparams_and_save(url):\n",
    "        dataset.loc[idx, \"hasHyperparams\"] = True\n",
    "        dataset.to_csv(\"./data/hyperparams.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using GPT to extract and format in a JSON file the hyperparameters (plus advantages/limitations if available) from selected models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: ./pages_text/selected/roberta-base-emotion.txt\n",
      "Successfully generated JSON\n",
      "JSON saved to ./output_json/roberta-base-emotion.json\n",
      "\n",
      "Processing file: ./pages_text/selected/bert-base-cased-finetuned-qnli.txt\n",
      "Successfully generated JSON\n",
      "JSON saved to ./output_json/bert-base-cased-finetuned-qnli.json\n",
      "\n",
      "Processing file: ./pages_text/selected/DeBERTa-v3-base-mnli-fever-anli.txt\n",
      "Successfully generated JSON\n",
      "JSON saved to ./output_json/DeBERTa-v3-base-mnli-fever-anli.json\n",
      "\n",
      "Processing file: ./pages_text/selected/amd-partial-v1.txt\n",
      "Successfully generated JSON\n",
      "JSON saved to ./output_json/amd-partial-v1.json\n",
      "\n",
      "Processing file: ./pages_text/selected/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7.txt\n",
      "Successfully generated JSON\n",
      "JSON saved to ./output_json/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7.json\n",
      "\n",
      "Processing file: ./pages_text/selected/nb-wav2vec2-1b-bokmaal.txt\n",
      "Successfully generated JSON\n",
      "No JSON block found in the output.\n",
      "Skipping file nb-wav2vec2-1b-bokmaal.txt due to JSON extraction error.\n",
      "\n",
      "Processing file: ./pages_text/selected/food_embeddings.txt\n",
      "Successfully generated JSON\n",
      "JSON saved to ./output_json/food_embeddings.json\n",
      "\n",
      "Processing file: ./pages_text/selected/led-large-book-summary.txt\n",
      "Successfully generated JSON\n",
      "JSON saved to ./output_json/led-large-book-summary.json\n",
      "\n",
      "Processing file: ./pages_text/selected/bert-base-uncased-mrpc.txt\n",
      "Successfully generated JSON\n",
      "JSON saved to ./output_json/bert-base-uncased-mrpc.json\n",
      "\n",
      "Processing file: ./pages_text/selected/distilbert-base-uncased-emotion.txt\n",
      "Successfully generated JSON\n",
      "JSON saved to ./output_json/distilbert-base-uncased-emotion.json\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def extract_json(text):\n",
    "    \"\"\"Extract JSON from the provided text using regex.\"\"\"\n",
    "    match = re.search(r'(\\{.*\\})', text, re.DOTALL)\n",
    "    if match:\n",
    "        json_str = match.group(1)\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Error decoding JSON:\", e)\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No JSON block found in the output.\")\n",
    "        return None\n",
    "\n",
    "input_folder = \"./pages_text/selected\"\n",
    "output_folder = \"./output_json\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    \n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "    print(f\"\\nProcessing file: {file_path}\")\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        scraped_text = f.read()\n",
    "\n",
    "    prompt = f\"\"\"Extract the following details from the text and output them as valid JSON.\n",
    "    The JSON should have the following keys:\n",
    "    - \"model_name\": Use the filename ({filename.split(\".txt\")[0]}) as the model name.\n",
    "    - \"hyperparameters\": Extract any hyperparameters mentioned.\n",
    "    - \"limitations\": Extract any limitations of the model (if available; otherwise, don't create the key in the JSON file).\n",
    "    - \"advantages\": Extract any advantages of the model (if available; otherwise, don't create the key in the JSON file).\n",
    "\n",
    "    Text:\n",
    "    {scraped_text}\n",
    "\n",
    "    Output JSON:\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistasnt that extracts structured JSON from unstructured text.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            max_tokens=300\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {e}\")\n",
    "        continue\n",
    "\n",
    "    output = response['choices'][0]['message']['content']\n",
    "    print(\"Successfully generated JSON\")\n",
    "    \n",
    "    json_output = extract_json(output)\n",
    "    if json_output is None:\n",
    "        print(f\"Skipping file {filename} due to JSON extraction error.\")\n",
    "        continue\n",
    "\n",
    "    if \"model_name\" in json_output and \"/\" in json_output[\"model_name\"]:\n",
    "        json_name = json_output[\"model_name\"].split(\"/\")[1]\n",
    "    else:\n",
    "        json_name = os.path.splitext(filename)[0]\n",
    "    \n",
    "    save_path = os.path.join(output_folder, json_name + \".json\")\n",
    "    \n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_output, f, indent=4)\n",
    "    \n",
    "    print(f\"JSON saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Graph enrichment** - Enrichment of the models' entities by adding these hyperparameters properties and eventually advantages and limitations if they get scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from rdflib import Graph, Namespace, URIRef, Literal, RDF, RDFS, OWL, Literal, URIRef\n",
    "from rdflib.namespace import XSD, RDF, SDO, RDFS\n",
    "\n",
    "KG = Namespace(\"http://kg-course/model-management/\")\n",
    "BIBO = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "SCHEMA = Namespace(\"http://schema.org/\")\n",
    "\n",
    "\n",
    "g = Graph()\n",
    "g = g.parse(\"output/graph/models-metrics-papers.ttl\", format=\"ttl\")\n",
    "# print(len(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N4afdac125c72415ea6c391a9b8d0218d (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------- Hyperparameter class -----------------\n",
    "g.add((KG.HyperParameter, RDF.type, RDFS.Class))\n",
    "g.add((KG.Model, RDFS.label, Literal(\"Hyperparameter\", lang=\"en\")))\n",
    "g.add((KG.Model, RDFS.comment, Literal(\"Hyperparameter used at training time\", lang=\"en\")))\n",
    "\n",
    "# ----------------- Hyperparameter properties --------------\n",
    "\n",
    "g.add((KG.hasHyperParameter, RDF.type, OWL.ObjectProperty))\n",
    "g.add((KG.hasHyperParameter, RDFS.label, Literal(\"has Hyperparameter\", lang=\"en\")))\n",
    "g.add((KG.hasHyperParameter, RDFS.comment, Literal(\"Links a model to one of its hyperparameters\", lang=\"en\")))\n",
    "g.add((KG.hasHyperParameter, RDFS.domain, KG.Model))\n",
    "g.add((KG.hasHyperParameter, RDFS.range, KG.HyperParameter))\n",
    "\n",
    "g.add((KG.parameterName, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((KG.parameterName, RDFS.label, Literal(\"Hyperparameter Name\", lang=\"en\")))\n",
    "g.add((KG.parameterName, RDFS.comment, Literal(\"The name of the hyperparameter\", lang=\"en\")))\n",
    "g.add((KG.parameterName, RDFS.domain, KG.HyperParameter))\n",
    "g.add((KG.parameterName, RDFS.range, RDFS.Literal))\n",
    "\n",
    "g.add((KG.parameterValue, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((KG.parameterValue, RDFS.label, Literal(\"Hyperparameter Value\", lang=\"en\")))\n",
    "g.add((KG.parameterValue, RDFS.comment, Literal(\"The value of the hyperparameter\", lang=\"en\")))\n",
    "g.add((KG.parameterValue, RDFS.domain, KG.HyperParameter))\n",
    "g.add((KG.parameterValue, RDFS.range, RDFS.Literal))\n",
    "\n",
    "g.add((KG.advantages, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((KG.advantages, RDFS.label, Literal(\"Advantages\", lang=\"en\")))\n",
    "g.add((KG.advantages, RDFS.comment, Literal(\"Advantages or improvements of the model\", lang=\"en\")))\n",
    "g.add((KG.advantages, RDFS.domain, KG.Model))\n",
    "g.add((KG.advantages, RDFS.range, RDFS.Literal))\n",
    "\n",
    "g.add((KG.limitations, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((KG.limitations, RDFS.label, Literal(\"Limitations\", lang=\"en\")))\n",
    "g.add((KG.limitations, RDFS.comment, Literal(\"Limitations of the model\", lang=\"en\")))\n",
    "g.add((KG.limitations, RDFS.domain, KG.Model))\n",
    "g.add((KG.limitations, RDFS.range, RDFS.Literal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for filename in os.listdir(\"./output_json\"):\n",
    "    file_path = os.path.join(\"./output_json\", filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        #print(f\"\\nContents of {filename}:\")\n",
    "        #print(data)\n",
    "        model_uri = URIRef(KG + data[\"model_name\"])\n",
    "        hyperparameter_uri = URIRef(KG + \"/Hyperparameter/\" + data[\"model_name\"])\n",
    "        g.add((hyperparameter_uri, RDF.type, KG.HyperParameter))\n",
    "        # Relation between model and hyperparameter\n",
    "        g.add((model_uri, KG.hasHyperParameter, hyperparameter_uri))\n",
    "\n",
    "        if data.get(\"hyperparameters\"):\n",
    "            for param_name, param_value in data[\"hyperparameters\"].items():\n",
    "                #print(f\"Hyperparameter: {param_name} - Value: {param_value}\")\n",
    "                \n",
    "                g.add((hyperparameter_uri, KG.parameterName, Literal(param_name)))\n",
    "                g.add((hyperparameter_uri, KG.parameterValue, Literal(param_value)))\n",
    "        i += 1\n",
    "        if data.get(\"advantages\") is not None:\n",
    "            if isinstance(data[\"advantages\"], list):\n",
    "                data[\"advantages\"] = \". \\n\".join(data[\"advantages\"])\n",
    "            g.add((model_uri, KG.advantages, Literal(data[\"advantages\"])))\n",
    "            if isinstance(data[\"advantages\"], dict):\n",
    "                advantages_str = \"\\n\".join([f\"{key}: {value}\" for key, value in data[\"advantages\"].items()])\n",
    "                #print(advantages_str)\n",
    "                g.add((model_uri, KG.advantages, Literal(data[\"advantages\"])))\n",
    "        if data.get(\"limitations\") is not None:\n",
    "            g.add((model_uri, KG.limitations, Literal(data[\"limitations\"])))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enrichment of models' entities about advantages and limitations from their corresponding papers since they couldn't get retrieved from HuggingFace webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction:\n",
      " Text embedding models represent documents as\n",
      "high-dimensional vectors, converting semantic re-\n",
      "lationships between documents into spatial rela-\n",
      "tionships between vectors. These models are fun-\n",
      "damental to neural information retrieval and have\n",
      "been widely adopted across various domains of\n",
      "NLP and IR research and applications. Text embed-\n",
      "dings are utilized in diverse downstream tasks such\n",
      "as classification, retrieval, and clustering. Notably,\n",
      "they have gained significant traction in building\n",
      "Retrieval-Augmented Generation (RAG) systems,\n",
      "where they serve as the primary technique in the\n",
      "retrieval step.\n",
      "A major limitation of traditional embedding\n",
      "models is that, despite being named as general-\n",
      "purpose, they often require fine-tuning for specific\n",
      "tasks [Jiao et al., 2020] and frequently struggle with\n",
      "*Equal contribution.\n",
      "common failure cases [Gao et al., 2021]. To ad-\n",
      "dress this, recent research has increasingly focused\n",
      "on leveraging large language models (LLMs) as\n",
      "the backbone for general-purpose embedding gen-\n",
      "eration, capitalizing on their ability to efficiently\n",
      "handle multiple languages and tasks [Jiang et al.,\n",
      "2024]. However, with model sizes typically reach-\n",
      "ing 7 billion parameters, deploying these models\n",
      "in real-world applications poses significant chal-\n",
      "lenges. Furthermore, the marginal improvements\n",
      "in evaluation metrics offered by LLM-based em-\n",
      "beddings, compared to encoder-only embedding\n",
      "models, render them a less practical choice for\n",
      "many use cases.\n",
      "This paper introduces jina-embeddings-v3, a\n",
      "novel text embedding model with 570 million pa-\n",
      "rameters, optimized for multilingual data, long-\n",
      "context retrieval, and high performance across\n",
      "multiple tasks. Evaluation on the MTEB bench-\n",
      "mark demonstrates that jina-embeddings-v3 not\n",
      "only significantly improves upon its predecessor,\n",
      "jina-embeddings-v2 [Günther et al., 2023] and\n",
      "its bilingual variants [Mohr et al., 2024], but\n",
      "also outperforms the latest proprietary embeddings\n",
      "from OpenAI and Cohere on English tasks, while\n",
      "surpassing multilingual-e5-large-instruct\n",
      "across all multilingual tasks.\n",
      "Additionally,\n",
      "compared to LLM-based embeddings such as\n",
      "e5-mistral-7b-instruct, which has a param-\n",
      "eter size of 7.1 billion (12x larger) and an out-\n",
      "put dimension of 4096 (4x larger) but offers\n",
      "only a 1% improvement on MTEB English tasks,\n",
      "jina-embeddings-v3 is a far more cost-efficient\n",
      "solution, making it more suitable for production\n",
      "and on-edge computing. The key contributions of\n",
      "this paper are:\n",
      "• Task-specific optimization with LoRA: We\n",
      "demonstrate that LoRA adapters [Hu et al.,\n",
      "2021] effectively generate task-specific em-\n",
      "beddings, outperforming prior instruction-\n",
      "based approaches.\n",
      "arXiv:2409.10173v3  [cs.CL]  19 Sep 2024\n",
      "• Patching retrieval failures with synthetic\n",
      "data: A qualitative analysis identified four\n",
      "common types of retrieval failures. We miti-\n",
      "gated these issues by incorporating synthetic\n",
      "training data, thereby improving model robust-\n",
      "ness on edge cases.\n",
      "• Integration of latest techniques: Our model\n",
      "incorporates several key advancements, in-\n",
      "cluding Matryoshka Representation Learn-\n",
      "ing [Kusupati et al., 2022], instruction tun-\n",
      "ing [Wei et al., 2022, Su et al., 2023], and\n",
      "long-context retrieval [Günther et al., 2023].\n",
      "Section 2 provides an overview of prior research\n",
      "relevant to the objectives of this paper. Section 3\n",
      "presents the architecture of jina-embeddings-v3\n",
      "in detail. The training procedure is described in\n",
      "Section 4. In Section 5, we conduct a thorough mul-\n",
      "tilingual evaluation, including ablation studies that\n",
      "offer insights into the impact of our architectural\n",
      "and training decisions.\n",
      "2\n",
      "\n",
      "Conclusions:\n",
      " This paper introduces jina-embeddings-v3, our\n",
      "latest text embedding model. By leveraging task-\n",
      "specific adapter tuning and failure-aware synthetic\n",
      "data augmentation on top of a robust backbone,\n",
      "jina-embeddings-v3 demonstrates competitive\n",
      "performance across a wide range of tasks. Exten-\n",
      "sive evaluations on both English and multilingual\n",
      "datasets highlight the model’s strong performance\n",
      "while maintaining a reasonable parameter size.\n",
      "We are particularly interested in evaluating\n",
      "and improving the model’s performance on low-\n",
      "resource language and further analyzing systematic\n",
      "failures caused by low data availability. We plan to\n",
      "focus on this area going forward, further strength-\n",
      "ening its capabilities in multilingual tasks where\n",
      "data availability is limited.\n"
     ]
    }
   ],
   "source": [
    "### What I do here is to extract text from the conclusions section of a paper, merge it with the abstract from paper_info.csv, \n",
    "### and then extract advantages and limitations from the merged text through GPT-3.5.\n",
    "### For simplicity I only use 3 papers, just to showcase the efficiency of the code and the method.\n",
    "\n",
    "# I have selected these papers: arxiv:2409.10173, arxiv:2212.04356, 2103.13413, 1912.08777\n",
    "import fitz\n",
    "import re\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    extracted_text = \"\"\n",
    "    for page in doc:\n",
    "        extracted_text += page.get_text()\n",
    "    return extracted_text\n",
    "\n",
    "def extract_sections(text):\n",
    "    # Extract introduction: text after \"Introduction\" until \"Conclusion\" or \"Conclusions\"\n",
    "    introduction_match = re.search(r'(?i)introduction?\\s*(.*?)\\s*(?=(Related Work|Data Processing))', text, re.DOTALL)\n",
    "    introduction = introduction_match.group(1).strip() if introduction_match else \"Not found\"\n",
    "\n",
    "    conclusions_match = re.search(r'(?i)conclusions?\\s*(.*?)\\s*(?=(references|Acknowledgments|acknowledgments|acknowledgements))', text, re.DOTALL)\n",
    "    conclusions = conclusions_match.group(1).strip() if conclusions_match else \"Not found\"\n",
    "\n",
    "    return introduction, conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation of a folder containing json files that contain model_name, advantages and disadvantages of the models. They're both retrieved from abstract, introduction and summary of the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated JSON\n",
      "JSON saved to ./output_json_papers/jinaai_jina-embeddings-v3.json\n",
      "Successfully generated JSON\n",
      "JSON saved to ./output_json_papers/kotoba-tech_kotoba-whisper-v2.0.json\n",
      "Successfully generated JSON\n",
      "JSON saved to ./output_json_papers/human-centered-summarization_financial-summarization-pegasus.json\n",
      "Successfully generated JSON\n",
      "JSON saved to ./output_json_papers/Intel_ldm3d-4c.json\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import re\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    extracted_text = \"\"\n",
    "    for page in doc:\n",
    "        extracted_text += page.get_text()\n",
    "    return extracted_text\n",
    "\n",
    "def extract_sections(text):\n",
    "    # Extract introduction: text after \"Introduction\" until \"Conclusion\" or \"Conclusions\"\n",
    "    introduction_match = re.search(r'(?i)introduction?\\s*(.*?)\\s*(?=(Related Work|Data Processing))', text, re.DOTALL)\n",
    "    introduction = introduction_match.group(1).strip() if introduction_match else \"Not found\"\n",
    "\n",
    "    conclusions_match = re.search(r'(?i)conclusions?\\s*(.*?)\\s*(?=(references|Acknowledgments|acknowledgments|acknowledgements))', text, re.DOTALL)\n",
    "    conclusions = conclusions_match.group(1).strip() if conclusions_match else \"Not found\"\n",
    "\n",
    "    return introduction, conclusions\n",
    "\n",
    "papers_info = pd.read_csv(\"./output/paper_info.csv\")\n",
    "merged = pd.read_csv(\"./data/merged-dataset.csv\")\n",
    "output_folder = \"./output_json_papers\"\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "    \n",
    "for filename in os.listdir(\"./papers/\"):\n",
    "    file_path = os.path.join(\"./papers/\", filename)\n",
    "    \n",
    "    pdf_file = file_path\n",
    "    paper_id = filename.split(\".pdf\")[0] \n",
    "    full_text = extract_text_from_pdf(pdf_file)\n",
    "    introduction, conclusions = extract_sections(full_text)\n",
    "\n",
    "    #print(\"Introduction:\\n\", introduction)\n",
    "    #print(\"\\nConclusions:\\n\", conclusions)\n",
    "\n",
    "    filtered_papers_info = papers_info[papers_info[\"id\"].astype(str) == paper_id]\n",
    "    abstract = filtered_papers_info[\"summary\"].values[0]\n",
    "    merged_text = abstract + \"\\n\" + introduction + \"\\n\" + conclusions\n",
    "    model_name = merged[merged[\"id_y\"].astype(str) == paper_id][\"id_x\"].values[0]\n",
    "    #print(merged_text)\n",
    "\n",
    "    prompt = f\"\"\"Extract structured details from the text below and output them as a valid JSON object. Do not include any extra commentary, markdown, or explanations—only output the JSON.\n",
    "\n",
    "        The JSON object must include the following keys:\n",
    "        - \"model_name\": {model_name}\n",
    "        - \"advantages\": A concise summary of the advantages or strengths of the model. Only include this key if the text provides any advantages.\n",
    "        - \"limitations\": A concise summary of the limitations or challenges of the model. Only include this key if the text mentions any limitations.\n",
    "\n",
    "        Advantages and disadvantages must be related to the model {model_name}.\n",
    "        Ignore any content that does not pertain to the model's advantages or limitations. The text provided includes detailed descriptions, evaluations, and comparisons. Focus solely on extracting the required information.\n",
    "\n",
    "        Text:\n",
    "        {merged_text}\n",
    "\n",
    "        Output JSON:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant that extracts structured JSON from unstructured text.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            max_tokens=4096\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {e}\")\n",
    "        continue\n",
    "\n",
    "    output = response['choices'][0]['message']['content']\n",
    "    print(\"Successfully generated JSON\")\n",
    "    \n",
    "    json_output = extract_json(output)\n",
    "    if json_output is None:\n",
    "        print(f\"Skipping file {filename} due to JSON extraction error.\")\n",
    "        continue\n",
    "\n",
    "    if \"model_name\" in json_output:\n",
    "        json_name = json_output[\"model_name\"].replace(\"/\", \"_\")\n",
    "    else:\n",
    "        json_name = os.path.splitext(filename)[0]\n",
    "    \n",
    "    save_path = os.path.join(output_folder, json_name + \".json\")\n",
    "    \n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_output, f, indent=4)\n",
    "    \n",
    "    print(f\"JSON saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding triples to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(\"./output_json_papers/\"):\n",
    "    file_path = os.path.join(\"./output_json_papers\", filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        model_uri = URIRef(KG + data[\"model_name\"])\n",
    "        hyperparameter_uri = URIRef(KG + \"/Hyperparameter/\" + data[\"model_name\"])\n",
    "        g.add((hyperparameter_uri, RDF.type, KG.HyperParameter))\n",
    "        # Relation between model and hyperparameter\n",
    "        g.add((model_uri, KG.hasHyperParameter, hyperparameter_uri))\n",
    "\n",
    "        if data.get(\"advantages\"):\n",
    "            g.add((model_uri, KG.advantages, Literal(data[\"advantages\"])))\n",
    "        if data.get(\"limitations\"):\n",
    "            g.add((model_uri, KG.limitations, Literal(data[\"limitations\"])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
