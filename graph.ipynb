{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import uuid\n",
    "from rdflib import Graph, Namespace, URIRef, Literal, RDF, RDFS, OWL, Literal, URIRef\n",
    "from rdflib.namespace import XSD, RDF, SDO, RDFS\n",
    "\n",
    "KG = Namespace(\"http://kg-course/model-management/\")\n",
    "BIBO = Namespace(\"http://purl.org/ontology/bibo/\")\n",
    "SCHEMA = Namespace(\"http://schema.org/\")\n",
    "\n",
    "\n",
    "g = Graph()\n",
    "g.bind(\"kg\", KG)\n",
    "g.bind(\"bibo\", BIBO)\n",
    "g.bind(\"owl\", OWL)\n",
    "g.bind(\"rdfs\", RDFS)\n",
    "g.bind(\"schema1\", SCHEMA)\n",
    "g.bind(\"xsd\", XSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models class and its properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class\n",
    "g.add((KG.Model, RDF.type, RDFS.Class))\n",
    "g.add((KG.Model, RDFS.label, Literal(\"Model\", lang=\"en\")))\n",
    "g.add((KG.Model, RDFS.comment, Literal(\"NLP, Computer Vision, Reinforcement Learning etc. models\", lang=\"en\")))\n",
    "\n",
    "# Model name\n",
    "g.add((KG.name, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((KG.name, RDFS.label, Literal(\"Model name\", lang=\"en\")))\n",
    "g.add((KG.name, RDFS.domain, KG.Model))\n",
    "g.add((KG.name, RDFS.range, RDFS.Literal))\n",
    "\n",
    "# Model creator\n",
    "g.add((KG.creator, RDF.type, OWL.ObjectProperty))\n",
    "g.add((KG.creator, RDFS.label, Literal(\"Model creator\", lang=\"en\")))\n",
    "g.add((KG.creator, RDFS.domain, KG.Model))\n",
    "g.add((KG.creator, RDFS.range, SCHEMA.Person))\n",
    "\n",
    "# Creation date\n",
    "g.add((KG.created_at, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((KG.created_at, RDFS.label, Literal(\"Creation date\", lang=\"en\")))\n",
    "g.add((KG.created_at, RDFS.domain, KG.Model))\n",
    "g.add((KG.created_at, RDFS.range, XSD.dateTime))\n",
    "\n",
    "# Downloads\n",
    "g.add((KG.downloads, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((KG.downloads, RDFS.label, Literal(\"Number of times a model has been downloaded\", lang=\"en\")))\n",
    "g.add((KG.downloads, RDFS.domain, KG.Model))\n",
    "g.add((KG.downloads, RDFS.range, XSD.integer))\n",
    "\n",
    "# Task\n",
    "g.add((KG.task, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((KG.task, RDFS.label, Literal(\"Model task\", lang=\"en\")))\n",
    "g.add((KG.task, RDFS.comment, Literal(\"The task the model is designed to perform\", lang=\"en\")))\n",
    "g.add((KG.task, RDFS.domain, KG.Model))\n",
    "g.add((KG.task, RDFS.range, RDFS.Literal))\n",
    "\n",
    "# hasPaper property\n",
    "g.add((KG.hasPaper, RDF.type, OWL.ObjectProperty))\n",
    "g.add((KG.hasPaper, RDFS.label, Literal(\"has Paper\", lang=\"en\")))\n",
    "g.add((KG.hasPaper, RDFS.comment, Literal(\"The ID of the paper that the model is based on or described in\", lang=\"en\")))\n",
    "g.add((KG.hasPaper, RDFS.domain, KG.Model))\n",
    "g.add((KG.hasPaper, RDFS.range, KG.Paper))\n",
    "\n",
    "# Base model\n",
    "g.add((KG.base_model, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((KG.base_model, RDFS.label, Literal(\"Base model\", lang=\"en\")))\n",
    "g.add((KG.base_model, RDFS.comment, Literal(\"The base model the model is built on\", lang=\"en\")))\n",
    "g.add((KG.base_model, RDFS.domain, KG.Model))\n",
    "g.add((KG.base_model, RDFS.range, RDFS.Literal))\n",
    "\n",
    "# Language\n",
    "g.add((KG.language, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((KG.language, RDFS.label, Literal(\"Supported languages\", lang=\"en\")))\n",
    "g.add((KG.language, RDFS.comment, Literal(\"The languages the model supports (e.g. IT, EN, etc.)\", lang=\"en\")))\n",
    "g.add((KG.language, RDFS.domain, KG.Model))\n",
    "g.add((KG.language, RDFS.range, RDFS.Literal))\n",
    "\n",
    "# hasEvaluationMetric property to link model to evaluation metrics\n",
    "g.add((KG.hasEvaluationMetric, RDF.type, OWL.ObjectProperty))\n",
    "g.add((KG.hasEvaluationMetric, RDFS.label, Literal(\"has Evaluation Metric\", lang=\"en\")))\n",
    "g.add((KG.hasEvaluationMetric, RDFS.domain, KG.Model))\n",
    "g.add((KG.hasEvaluationMetric, RDFS.range, KG.EvaluationMetric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics class and properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- Evaluation Metric class -----------------#\n",
    "\n",
    "g.add((KG.EvaluationMetric, RDF.type, RDFS.Class))\n",
    "g.add((KG.EvaluationMetric, RDFS.label, Literal(\"Evaluation Metric\", lang=\"en\")))\n",
    "g.add((KG.EvaluationMetric, RDFS.comment, Literal(\"The evaluation metric used to evaluate the model\", lang=\"en\")))\n",
    "\n",
    "#----------------- Evaluation Metric properties -----------------#\n",
    "\n",
    "# Task type\n",
    "g.add((KG.taskType, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((KG.taskType, RDFS.label, Literal(\"Task on which the model was evaluated\", lang=\"en\")))\n",
    "g.add((KG.taskType, RDFS.domain, KG.EvaluationMetric))\n",
    "g.add((KG.taskType, RDFS.range, RDFS.Literal))\n",
    "\n",
    "# Dataset name\n",
    "g.add((KG.datasetName, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((KG.datasetName, RDFS.label, Literal(\"Dataset Name\", lang=\"en\")))\n",
    "g.add((KG.datasetName, RDFS.comment, Literal(\"The name of the dataset used to evaluate the model, e.g. MNIST, IMDB, or custom\", lang=\"en\")))\n",
    "g.add((KG.datasetName, RDFS.domain, KG.EvaluationMetric))\n",
    "g.add((KG.datasetName, RDFS.range, RDFS.Literal))\n",
    "\n",
    "# Metric type\n",
    "g.add((KG.metricType, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((KG.metricType, RDFS.label, Literal(\"Metric Type\", lang=\"en\")))\n",
    "g.add((KG.metricType, RDFS.comment, Literal(\"The type of evaluation metric used, e.g. accuracy, F1 score, etc.\", lang=\"en\")))\n",
    "g.add((KG.metricType, RDFS.domain, KG.EvaluationMetric))\n",
    "g.add((KG.metricType, RDFS.range, RDFS.Literal))\n",
    "\n",
    "# Metric value\n",
    "g.add((KG.metricValue, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((KG.metricValue, RDFS.label, Literal(\"Metric Value\", lang=\"en\")))\n",
    "g.add((KG.metricValue, RDFS.domain, KG.EvaluationMetric))\n",
    "g.add((KG.metricValue, RDFS.range, XSD.float))\n",
    "\n",
    "# Metric error\n",
    "g.add((KG.metricError, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((KG.metricError, RDFS.label, Literal(\"Metric Error\", lang=\"en\")))\n",
    "g.add((KG.metricError, RDFS.domain, KG.EvaluationMetric))\n",
    "g.add((KG.metricError, RDFS.range, XSD.float))\n",
    "\n",
    "# Metric mean\n",
    "g.add((KG.metricMean, RDF.type, OWL.DatatypeProperty))\n",
    "g.add((KG.metricMean, RDFS.label, Literal(\"Metric Mean\", lang=\"en\")))\n",
    "g.add((KG.metricMean, RDFS.domain, KG.EvaluationMetric))\n",
    "g.add((KG.metricMean, RDFS.range, XSD.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Paper class -----------------#\n",
    "g.add((KG.Paper, RDF.type, OWL.Class))\n",
    "g.add((KG.Paper, RDFS.label, Literal(\"Scientific paper\", lang=\"en\")))\n",
    "g.add((KG.Paper, RDFS.comment, Literal(\"A scientific paper that describes a model or research work\", lang=\"en\")))\n",
    "g.add((KG.Paper, OWL.equivalentClass, BIBO.Article))\n",
    "g.add((SCHEMA.Person, RDFS.comment, Literal(\"A person, company, or organization that created the model\", lang=\"en\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Graph construction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"./data/dataset.csv\")\n",
    "\n",
    "for index, row in dataframe.iterrows():\n",
    "    model_uri = URIRef(KG + row[\"id\"])\n",
    "    g.add((model_uri, RDF.type, KG.Model))\n",
    "    g.add((model_uri, KG.name, Literal(row[\"id\"].split(\"/\")[1])))\n",
    "    g.add((model_uri, KG.creator, Literal(row[\"author\"])))\n",
    "    g.add((model_uri, KG.created_at, Literal(row[\"created_at\"], datatype=XSD.dateTime)))\n",
    "    g.add((model_uri, KG.downloads, Literal(row[\"downloads\"], datatype=XSD.integer)))\n",
    "\n",
    "    if not pd.isna(row[\"pipeline_tag\"]):\n",
    "       g.add((model_uri, KG.task, Literal(row[\"pipeline_tag\"])))\n",
    "\n",
    "    if not pd.isna(row[\"base_model\"]):\n",
    "        if \"/\" in row[\"base_model\"]:\n",
    "            row[\"base_model\"] = row[\"base_model\"].split(\"/\")[1]\n",
    "            g.add((model_uri, KG.base_model, Literal(row[\"base_model\"])))\n",
    "        else:\n",
    "            g.add((model_uri, KG.base_model, Literal(row[\"base_model\"])))\n",
    "    language_string = row[\"language\"]\n",
    "\n",
    "    if pd.isna(language_string):\n",
    "        pass\n",
    "    else:\n",
    "        language_list = [aid.strip() for aid in language_string.split(\",\") if aid.strip()] \n",
    "        if len(language_list) == 1:\n",
    "            g.add((model_uri, KG.language, Literal(language_list[0])))\n",
    "        elif len(language_list) > 1:\n",
    "            for lang in language_list:\n",
    "                g.add((model_uri, KG.language, Literal(lang)))\n",
    "\n",
    "    metrics_list = json.loads(row[\"evaluation_metrics\"])\n",
    "    for metric in metrics_list:\n",
    "        metric_uri = URIRef(KG + \"EvaluationMetric_\" + str(uuid.uuid4()))\n",
    "        g.add((metric_uri, RDF.type, KG.EvaluationMetric))\n",
    "        g.add((metric_uri, KG.taskType, Literal(metric[\"task_type\"])))\n",
    "        g.add((metric_uri, KG.datasetName, Literal(metric[\"dataset_name\"])))\n",
    "        if metric.get(\"metric_type\"):\n",
    "            g.add((metric_uri, KG.metricType, Literal(metric[\"metric_type\"])))\n",
    "        if metric.get(\"metric_value\"):\n",
    "            g.add((metric_uri, KG.metricValue, Literal(metric[\"metric_value\"], datatype=XSD.float)))\n",
    "        else:\n",
    "            if metric.get(\"metric_mean\"):\n",
    "                g.add((metric_uri, KG.metricMean, Literal(metric[\"metric_mean\"], datatype=XSD.float)))\n",
    "            if metric.get(\"metric_error\"):\n",
    "                g.add((metric_uri, KG.metricError, Literal(metric[\"metric_error\"], datatype=XSD.float)))\n",
    "        \n",
    "        g.add((model_uri, KG.hasEvaluationMetric, metric_uri))\n",
    "\n",
    "g.serialize(destination=\"./output/graph/models-metrics\", format=\"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(g))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
